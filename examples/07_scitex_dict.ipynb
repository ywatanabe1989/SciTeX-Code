{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SciTeX Dictionary Utilities Tutorial\n",
    "\n",
    "This comprehensive notebook demonstrates the SciTeX dictionary utilities module, covering advanced dictionary operations, data structures, and utilities for scientific computing.\n",
    "\n",
    "## Features Covered\n",
    "\n",
    "### Core Dictionary Classes\n",
    "* DotDict - Attribute-style access to dictionary keys\n",
    "* listed_dict - Dictionary with automatic list initialization\n",
    "\n",
    "### Dictionary Operations\n",
    "* safe_merge - Merge dictionaries with conflict detection\n",
    "* pop_keys - Remove specified keys from key lists\n",
    "* replace - String replacement using dictionaries\n",
    "* to_str - Convert dictionaries to string representations\n",
    "\n",
    "### Use Cases\n",
    "* Configuration management\n",
    "* Data aggregation and collection\n",
    "* Parameter handling for scientific experiments\n",
    "* Template processing and string manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "import scitex\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import json\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"SciTeX Dictionary Utilities Tutorial - Ready to begin!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: DotDict - Attribute-Style Dictionary Access\n",
    "\n",
    "### 1.1 Basic DotDict Usage\n",
    "\n",
    "The DotDict class allows you to access dictionary keys as attributes, making code more readable and intuitive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DotDict from a regular dictionary\n",
    "config_data = {\n",
    "    'experiment_name': 'neural_network_training',\n",
    "    'model_params': {\n",
    "        'learning_rate': 0.001,\n",
    "        'batch_size': 32,\n",
    "        'epochs': 100,\n",
    "        'hidden_layers': [128, 64, 32]\n",
    "    },\n",
    "    'data_params': {\n",
    "        'train_split': 0.8,\n",
    "        'validation_split': 0.1,\n",
    "        'test_split': 0.1\n",
    "    },\n",
    "    'output_dir': '/tmp/experiment_results',\n",
    "    'random_seed': 42\n",
    "}\n",
    "\n",
    "# Convert to DotDict\n",
    "config = scitex.dict.DotDict(config_data)\n",
    "\n",
    "# Access using dot notation\n",
    "print(f\"Experiment name: {config.experiment_name}\")\n",
    "print(f\"Learning rate: {config.model_params.learning_rate}\")\n",
    "print(f\"Hidden layers: {config.model_params.hidden_layers}\")\n",
    "print(f\"Train split: {config.data_params.train_split}\")\n",
    "\n",
    "# Also supports traditional dictionary access\n",
    "print(f\"\\nUsing bracket notation:\")\n",
    "print(f\"Output dir: {config['output_dir']}\")\n",
    "print(f\"Batch size: {config['model_params']['batch_size']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 DotDict with Complex Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DotDict with various data types\n",
    "complex_data = {\n",
    "    'metadata': {\n",
    "        'author': 'Research Team',\n",
    "        'date': '2024-01-01',\n",
    "        'version': '1.0.0'\n",
    "    },\n",
    "    'arrays': {\n",
    "        'data_matrix': np.random.randn(100, 50),\n",
    "        'labels': np.random.randint(0, 3, 100),\n",
    "        'weights': np.random.uniform(0, 1, 50)\n",
    "    },\n",
    "    'dataframes': {\n",
    "        'results': pd.DataFrame({\n",
    "            'accuracy': np.random.uniform(0.8, 0.95, 10),\n",
    "            'loss': np.random.uniform(0.1, 0.5, 10),\n",
    "            'epoch': range(1, 11)\n",
    "        })\n",
    "    },\n",
    "    'functions': {\n",
    "        'activation': 'relu',\n",
    "        'optimizer': 'adam',\n",
    "        'loss_function': 'categorical_crossentropy'\n",
    "    },\n",
    "    123: 'integer_key',  # Non-string key\n",
    "    'invalid-key': 'hyphenated_key'  # Invalid identifier\n",
    "}\n",
    "\n",
    "dot_dict = scitex.dict.DotDict(complex_data)\n",
    "\n",
    "# Access different data types\n",
    "print(f\"Data matrix shape: {dot_dict.arrays.data_matrix.shape}\")\n",
    "print(f\"Results dataframe shape: {dot_dict.dataframes.results.shape}\")\n",
    "print(f\"Activation function: {dot_dict.functions.activation}\")\n",
    "\n",
    "# Access non-string keys with bracket notation\n",
    "print(f\"Integer key: {dot_dict[123]}\")\n",
    "print(f\"Hyphenated key: {dot_dict['invalid-key']}\")\n",
    "\n",
    "# Modify values\n",
    "dot_dict.functions.activation = 'tanh'\n",
    "dot_dict.metadata.version = '1.1.0'\n",
    "\n",
    "print(f\"\\nAfter modification:\")\n",
    "print(f\"New activation: {dot_dict.functions.activation}\")\n",
    "print(f\"New version: {dot_dict.metadata.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 DotDict Methods and Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate DotDict methods\n",
    "sample_dict = scitex.dict.DotDict({\n",
    "    'a': 1,\n",
    "    'b': 2,\n",
    "    'c': {'nested': 3, 'more': 4},\n",
    "    'd': [1, 2, 3]\n",
    "})\n",
    "\n",
    "print(f\"Length: {len(sample_dict)}\")\n",
    "print(f\"Keys: {list(sample_dict.keys())}\")\n",
    "print(f\"Values: {list(sample_dict.values())}\")\n",
    "print(f\"Items: {list(sample_dict.items())}\")\n",
    "\n",
    "# Check membership\n",
    "print(f\"\\n'a' in sample_dict: {'a' in sample_dict}\")\n",
    "print(f\"'z' in sample_dict: {'z' in sample_dict}\")\n",
    "\n",
    "# Get method with default\n",
    "print(f\"Get 'a' with default: {sample_dict.get('a', 'not found')}\")\n",
    "print(f\"Get 'z' with default: {sample_dict.get('z', 'not found')}\")\n",
    "\n",
    "# Update method\n",
    "sample_dict.update({'e': 5, 'f': {'new_nested': 6}})\n",
    "print(f\"\\nAfter update: {list(sample_dict.keys())}\")\n",
    "print(f\"New nested value: {sample_dict.f.new_nested}\")\n",
    "\n",
    "# Pop method\n",
    "popped_value = sample_dict.pop('b', 'not found')\n",
    "print(f\"\\nPopped value: {popped_value}\")\n",
    "print(f\"Keys after pop: {list(sample_dict.keys())}\")\n",
    "\n",
    "# Copy method\n",
    "copied_dict = sample_dict.copy()\n",
    "copied_dict.a = 999\n",
    "print(f\"\\nOriginal 'a': {sample_dict.a}\")\n",
    "print(f\"Copy 'a': {copied_dict.a}\")\n",
    "\n",
    "# Convert back to regular dict\n",
    "regular_dict = sample_dict.to_dict()\n",
    "print(f\"\\nType of original: {type(sample_dict)}\")\n",
    "print(f\"Type of converted: {type(regular_dict)}\")\n",
    "print(f\"Type of nested in converted: {type(regular_dict['c'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: listed_dict - Dictionary with Automatic List Initialization\n",
    "\n",
    "### 2.1 Basic listed_dict Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a listed_dict without predefined keys\n",
    "data_collector = scitex.dict.listed_dict()\n",
    "\n",
    "# Simulate data collection process\n",
    "for i in range(10):\n",
    "    # Automatically creates lists for new keys\n",
    "    data_collector['experiment_A'].append(random.randint(0, 100))\n",
    "    data_collector['experiment_B'].append(random.randint(50, 150))\n",
    "    data_collector['timestamps'].append(f\"2024-01-{i+1:02d}\")\n",
    "\n",
    "print(\"Data collected:\")\n",
    "for key, values in data_collector.items():\n",
    "    print(f\"{key}: {values}\")\n",
    "\n",
    "print(f\"\\nType of data_collector: {type(data_collector)}\")\n",
    "print(f\"Length of experiment_A: {len(data_collector['experiment_A'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 listed_dict with Predefined Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create listed_dict with predefined keys\n",
    "metrics_keys = ['accuracy', 'precision', 'recall', 'f1_score']\n",
    "metrics_collector = scitex.dict.listed_dict(metrics_keys)\n",
    "\n",
    "print(\"Initial state:\")\n",
    "for key in metrics_keys:\n",
    "    print(f\"{key}: {metrics_collector[key]}\")\n",
    "\n",
    "# Simulate multiple training runs\n",
    "n_runs = 5\n",
    "for run in range(n_runs):\n",
    "    # Generate realistic metrics for each run\n",
    "    base_accuracy = 0.85 + random.uniform(-0.1, 0.1)\n",
    "    metrics_collector['accuracy'].append(base_accuracy)\n",
    "    metrics_collector['precision'].append(base_accuracy + random.uniform(-0.05, 0.05))\n",
    "    metrics_collector['recall'].append(base_accuracy + random.uniform(-0.05, 0.05))\n",
    "    \n",
    "    # F1 score as harmonic mean of precision and recall\n",
    "    p = metrics_collector['precision'][-1]\n",
    "    r = metrics_collector['recall'][-1]\n",
    "    f1 = 2 * (p * r) / (p + r) if (p + r) > 0 else 0\n",
    "    metrics_collector['f1_score'].append(f1)\n",
    "\n",
    "print(f\"\\nAfter {n_runs} runs:\")\n",
    "for key, values in metrics_collector.items():\n",
    "    avg_value = np.mean(values)\n",
    "    std_value = np.std(values)\n",
    "    print(f\"{key}: {avg_value:.3f} ± {std_value:.3f} (n={len(values)})\")\n",
    "\n",
    "# Convert to DataFrame for analysis\n",
    "metrics_df = pd.DataFrame(dict(metrics_collector))\n",
    "print(f\"\\nMetrics DataFrame:\")\n",
    "print(metrics_df.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Real-world Example: Experiment Log Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a machine learning experiment with multiple conditions\n",
    "experiment_log = scitex.dict.listed_dict()\n",
    "\n",
    "# Define experimental conditions\n",
    "learning_rates = [0.001, 0.01, 0.1]\n",
    "batch_sizes = [16, 32, 64]\n",
    "architectures = ['small', 'medium', 'large']\n",
    "\n",
    "# Run experiments\n",
    "for lr in learning_rates:\n",
    "    for batch_size in batch_sizes:\n",
    "        for arch in architectures:\n",
    "            # Simulate training results\n",
    "            final_accuracy = 0.7 + random.uniform(0, 0.25)\n",
    "            training_time = random.uniform(10, 300)  # seconds\n",
    "            \n",
    "            # Log results\n",
    "            experiment_log['learning_rate'].append(lr)\n",
    "            experiment_log['batch_size'].append(batch_size)\n",
    "            experiment_log['architecture'].append(arch)\n",
    "            experiment_log['final_accuracy'].append(final_accuracy)\n",
    "            experiment_log['training_time'].append(training_time)\n",
    "            experiment_log['experiment_id'].append(f\"lr{lr}_bs{batch_size}_{arch}\")\n",
    "\n",
    "print(f\"Total experiments: {len(experiment_log['experiment_id'])}\")\n",
    "print(f\"Unique learning rates: {set(experiment_log['learning_rate'])}\")\n",
    "print(f\"Unique batch sizes: {set(experiment_log['batch_size'])}\")\n",
    "print(f\"Unique architectures: {set(experiment_log['architecture'])}\")\n",
    "\n",
    "# Convert to DataFrame for analysis\n",
    "experiment_df = pd.DataFrame(dict(experiment_log))\n",
    "\n",
    "# Find best configurations\n",
    "best_accuracy_idx = experiment_df['final_accuracy'].idxmax()\n",
    "best_config = experiment_df.loc[best_accuracy_idx]\n",
    "\n",
    "print(f\"\\nBest configuration:\")\n",
    "print(f\"Learning rate: {best_config['learning_rate']}\")\n",
    "print(f\"Batch size: {best_config['batch_size']}\")\n",
    "print(f\"Architecture: {best_config['architecture']}\")\n",
    "print(f\"Final accuracy: {best_config['final_accuracy']:.3f}\")\n",
    "print(f\"Training time: {best_config['training_time']:.1f} seconds\")\n",
    "\n",
    "# Group by architecture and show average performance\n",
    "arch_performance = experiment_df.groupby('architecture')['final_accuracy'].agg(['mean', 'std', 'count'])\n",
    "print(f\"\\nPerformance by architecture:\")\n",
    "print(arch_performance.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Dictionary Manipulation Functions\n",
    "\n",
    "### 3.1 safe_merge - Merge Dictionaries with Conflict Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test dictionaries for merging\n",
    "config_base = {\n",
    "    'model': {\n",
    "        'type': 'neural_network',\n",
    "        'layers': 3\n",
    "    },\n",
    "    'training': {\n",
    "        'epochs': 100,\n",
    "        'batch_size': 32\n",
    "    }\n",
    "}\n",
    "\n",
    "config_experiment = {\n",
    "    'experiment': {\n",
    "        'name': 'test_run_1',\n",
    "        'date': '2024-01-01'\n",
    "    },\n",
    "    'optimization': {\n",
    "        'learning_rate': 0.001,\n",
    "        'optimizer': 'adam'\n",
    "    }\n",
    "}\n",
    "\n",
    "config_output = {\n",
    "    'output': {\n",
    "        'save_path': '/tmp/results',\n",
    "        'save_format': 'pickle'\n",
    "    },\n",
    "    'logging': {\n",
    "        'level': 'INFO',\n",
    "        'file': 'experiment.log'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Safe merge without conflicts\n",
    "try:\n",
    "    merged_config = scitex.dict.safe_merge(config_base, config_experiment, config_output)\n",
    "    print(\"Successfully merged configurations:\")\n",
    "    print(f\"Top-level keys: {list(merged_config.keys())}\")\n",
    "    print(f\"Model type: {merged_config['model']['type']}\")\n",
    "    print(f\"Experiment name: {merged_config['experiment']['name']}\")\n",
    "    print(f\"Output path: {merged_config['output']['save_path']}\")\n",
    "except ValueError as e:\n",
    "    print(f\"Merge failed: {e}\")\n",
    "\n",
    "# Test with conflicting keys\n",
    "config_conflict = {\n",
    "    'model': {  # This will conflict with config_base\n",
    "        'type': 'decision_tree',\n",
    "        'max_depth': 10\n",
    "    },\n",
    "    'validation': {\n",
    "        'split': 0.2\n",
    "    }\n",
    "}\n",
    "\n",
    "try:\n",
    "    conflicted_merge = scitex.dict.safe_merge(config_base, config_conflict)\n",
    "    print(\"\\nConflict merge succeeded (this shouldn't happen)\")\n",
    "except ValueError as e:\n",
    "    print(f\"\\nExpected conflict detected: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 pop_keys - Remove Keys from Key Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Feature selection by removing unwanted features\n",
    "all_features = [\n",
    "    'age', 'gender', 'income', 'education', 'occupation',\n",
    "    'marital_status', 'health_score', 'exercise_frequency',\n",
    "    'smoking_status', 'alcohol_consumption', 'bmi', 'blood_pressure'\n",
    "]\n",
    "\n",
    "# Remove sensitive or irrelevant features\n",
    "features_to_remove = ['gender', 'marital_status', 'income']\n",
    "\n",
    "selected_features = scitex.dict.pop_keys(all_features, features_to_remove)\n",
    "\n",
    "print(f\"Original features ({len(all_features)}): {all_features}\")\n",
    "print(f\"Features to remove: {features_to_remove}\")\n",
    "print(f\"Selected features ({len(selected_features)}): {selected_features}\")\n",
    "\n",
    "# Example: Column selection for data analysis\n",
    "dataframe_columns = [\n",
    "    'timestamp', 'user_id', 'session_id', 'action_type',\n",
    "    'page_url', 'referrer', 'user_agent', 'ip_address',\n",
    "    'duration', 'clicks', 'scrolls', 'conversions'\n",
    "]\n",
    "\n",
    "# Remove PII and technical columns for analysis\n",
    "columns_to_exclude = ['user_id', 'session_id', 'ip_address', 'user_agent']\n",
    "analysis_columns = scitex.dict.pop_keys(dataframe_columns, columns_to_exclude)\n",
    "\n",
    "print(f\"\\nOriginal columns: {dataframe_columns}\")\n",
    "print(f\"Columns to exclude: {columns_to_exclude}\")\n",
    "print(f\"Analysis columns: {analysis_columns}\")\n",
    "\n",
    "# Example: Multi-stage feature filtering\n",
    "ml_features = [\n",
    "    'feature_1', 'feature_2', 'feature_3', 'feature_4', 'feature_5',\n",
    "    'feature_6', 'feature_7', 'feature_8', 'feature_9', 'feature_10',\n",
    "    'target', 'id', 'timestamp'\n",
    "]\n",
    "\n",
    "# Remove non-feature columns\n",
    "non_features = ['target', 'id', 'timestamp']\n",
    "features_only = scitex.dict.pop_keys(ml_features, non_features)\n",
    "\n",
    "# Remove low-importance features (simulated)\n",
    "low_importance = ['feature_3', 'feature_7', 'feature_9']\n",
    "high_importance_features = scitex.dict.pop_keys(features_only, low_importance)\n",
    "\n",
    "print(f\"\\nML pipeline feature selection:\")\n",
    "print(f\"All columns: {ml_features}\")\n",
    "print(f\"Features only: {features_only}\")\n",
    "print(f\"High importance features: {high_importance_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 replace - String Replacement with Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Template processing for experiment reports\n",
    "report_template = \"\"\"\n",
    "Experiment Report\n",
    "=================\n",
    "\n",
    "Experiment Name: {EXPERIMENT_NAME}\n",
    "Date: {DATE}\n",
    "Model: {MODEL_TYPE}\n",
    "Dataset: {DATASET}\n",
    "\n",
    "Results:\n",
    "- Accuracy: {ACCURACY}\n",
    "- Precision: {PRECISION}\n",
    "- Recall: {RECALL}\n",
    "- F1 Score: {F1_SCORE}\n",
    "\n",
    "Training Time: {TRAINING_TIME}\n",
    "Status: {STATUS}\n",
    "\"\"\"\n",
    "\n",
    "# Define replacement dictionary\n",
    "experiment_values = {\n",
    "    '{EXPERIMENT_NAME}': 'Image Classification Study',\n",
    "    '{DATE}': '2024-01-15',\n",
    "    '{MODEL_TYPE}': 'Convolutional Neural Network',\n",
    "    '{DATASET}': 'CIFAR-10',\n",
    "    '{ACCURACY}': '0.923',\n",
    "    '{PRECISION}': '0.918',\n",
    "    '{RECALL}': '0.915',\n",
    "    '{F1_SCORE}': '0.916',\n",
    "    '{TRAINING_TIME}': '2.5 hours',\n",
    "    '{STATUS}': 'COMPLETED'\n",
    "}\n",
    "\n",
    "# Generate report\n",
    "final_report = scitex.dict.replace(report_template, experiment_values)\n",
    "print(final_report)\n",
    "\n",
    "# Example: Code generation with variable substitution\n",
    "code_template = \"\"\"\n",
    "def {FUNCTION_NAME}({PARAMETERS}):\n",
    "    \\\"\\\"\\\"\n",
    "    {DOCSTRING}\n",
    "    \\\"\\\"\\\"\n",
    "    {BODY}\n",
    "    return {RETURN_VALUE}\n",
    "\"\"\"\n",
    "\n",
    "function_specs = {\n",
    "    '{FUNCTION_NAME}': 'calculate_metrics',\n",
    "    '{PARAMETERS}': 'y_true, y_pred',\n",
    "    '{DOCSTRING}': 'Calculate classification metrics from predictions.',\n",
    "    '{BODY}': '''    accuracy = np.mean(y_true == y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='weighted')\n",
    "    recall = recall_score(y_true, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')''',\n",
    "    '{RETURN_VALUE}': '{'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1}'\n",
    "}\n",
    "\n",
    "generated_code = scitex.dict.replace(code_template, function_specs)\n",
    "print(\"\\nGenerated code:\")\n",
    "print(generated_code)\n",
    "\n",
    "# Example: Configuration file processing\n",
    "config_template = \"\"\"\n",
    "model_config = {\n",
    "    'architecture': '{ARCHITECTURE}',\n",
    "    'input_size': {INPUT_SIZE},\n",
    "    'hidden_layers': {HIDDEN_LAYERS},\n",
    "    'output_size': {OUTPUT_SIZE},\n",
    "    'activation': '{ACTIVATION}',\n",
    "    'dropout_rate': {DROPOUT_RATE}\n",
    "}\n",
    "\n",
    "training_config = {\n",
    "    'learning_rate': {LEARNING_RATE},\n",
    "    'batch_size': {BATCH_SIZE},\n",
    "    'epochs': {EPOCHS},\n",
    "    'optimizer': '{OPTIMIZER}'\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "config_values = {\n",
    "    '{ARCHITECTURE}': 'feedforward',\n",
    "    '{INPUT_SIZE}': '784',\n",
    "    '{HIDDEN_LAYERS}': '[128, 64, 32]',\n",
    "    '{OUTPUT_SIZE}': '10',\n",
    "    '{ACTIVATION}': 'relu',\n",
    "    '{DROPOUT_RATE}': '0.2',\n",
    "    '{LEARNING_RATE}': '0.001',\n",
    "    '{BATCH_SIZE}': '32',\n",
    "    '{EPOCHS}': '100',\n",
    "    '{OPTIMIZER}': 'adam'\n",
    "}\n",
    "\n",
    "config_code = scitex.dict.replace(config_template, config_values)\n",
    "print(\"\\nGenerated configuration:\")\n",
    "print(config_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 to_str - Convert Dictionary to String Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Creating unique experiment identifiers\n",
    "experiment_params = {\n",
    "    'lr': 0.001,\n",
    "    'bs': 32,\n",
    "    'arch': 'resnet',\n",
    "    'dropout': 0.2\n",
    "}\n",
    "\n",
    "experiment_id = scitex.dict.to_str(experiment_params)\n",
    "print(f\"Experiment ID: {experiment_id}\")\n",
    "\n",
    "# Custom delimiter\n",
    "experiment_id_custom = scitex.dict.to_str(experiment_params, delimiter='__')\n",
    "print(f\"Custom delimiter ID: {experiment_id_custom}\")\n",
    "\n",
    "# Example: Model hyperparameter tracking\n",
    "model_configs = [\n",
    "    {'model': 'cnn', 'layers': 3, 'filters': 64, 'lr': 0.01},\n",
    "    {'model': 'rnn', 'units': 128, 'dropout': 0.3, 'lr': 0.001},\n",
    "    {'model': 'transformer', 'heads': 8, 'layers': 6, 'lr': 0.0001}\n",
    "]\n",
    "\n",
    "print(\"\\nModel configuration strings:\")\n",
    "for i, config in enumerate(model_configs):\n",
    "    config_str = scitex.dict.to_str(config, delimiter='|')\n",
    "    print(f\"Config {i+1}: {config_str}\")\n",
    "\n",
    "# Example: Creating file names from parameters\n",
    "experiment_settings = {\n",
    "    'dataset': 'cifar10',\n",
    "    'model': 'vgg16',\n",
    "    'epochs': 50,\n",
    "    'augment': True\n",
    "}\n",
    "\n",
    "filename_base = scitex.dict.to_str(experiment_settings, delimiter='_')\n",
    "full_filename = f\"results_{filename_base}.pkl\"\n",
    "print(f\"\\nGenerated filename: {full_filename}\")\n",
    "\n",
    "# Example: Logging experiment parameters\n",
    "training_logs = scitex.dict.listed_dict(['timestamp', 'config_string', 'final_loss'])\n",
    "\n",
    "# Simulate multiple training runs\n",
    "configs = [\n",
    "    {'lr': 0.1, 'momentum': 0.9, 'weight_decay': 1e-4},\n",
    "    {'lr': 0.01, 'momentum': 0.95, 'weight_decay': 1e-5},\n",
    "    {'lr': 0.001, 'momentum': 0.99, 'weight_decay': 1e-3}\n",
    "]\n",
    "\n",
    "for i, config in enumerate(configs):\n",
    "    config_str = scitex.dict.to_str(config, delimiter=',')\n",
    "    final_loss = random.uniform(0.1, 0.5)\n",
    "    \n",
    "    training_logs['timestamp'].append(f\"2024-01-{i+1:02d}\")\n",
    "    training_logs['config_string'].append(config_str)\n",
    "    training_logs['final_loss'].append(final_loss)\n",
    "\n",
    "print(\"\\nTraining logs:\")\n",
    "for i in range(len(training_logs['timestamp'])):\n",
    "    print(f\"{training_logs['timestamp'][i]}: {training_logs['config_string'][i]} -> Loss: {training_logs['final_loss'][i]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Advanced Use Cases and Integration\n",
    "\n",
    "### 4.1 Scientific Configuration Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex scientific experiment configuration\n",
    "class ExperimentConfig:\n",
    "    def __init__(self):\n",
    "        self.base_config = scitex.dict.DotDict({\n",
    "            'experiment': {\n",
    "                'name': 'baseline',\n",
    "                'description': 'Baseline experiment setup',\n",
    "                'version': '1.0.0'\n",
    "            },\n",
    "            'data': {\n",
    "                'source': 'synthetic',\n",
    "                'size': 10000,\n",
    "                'features': 100,\n",
    "                'noise_level': 0.1\n",
    "            },\n",
    "            'model': {\n",
    "                'type': 'neural_network',\n",
    "                'architecture': [100, 50, 25, 10],\n",
    "                'activation': 'relu',\n",
    "                'dropout': 0.2\n",
    "            },\n",
    "            'training': {\n",
    "                'optimizer': 'adam',\n",
    "                'learning_rate': 0.001,\n",
    "                'batch_size': 32,\n",
    "                'epochs': 100,\n",
    "                'validation_split': 0.2\n",
    "            },\n",
    "            'output': {\n",
    "                'save_model': True,\n",
    "                'save_predictions': True,\n",
    "                'save_metrics': True,\n",
    "                'plot_results': True\n",
    "            }\n",
    "        })\n",
    "        \n",
    "        self.experiment_variants = []\n",
    "        self.results_log = scitex.dict.listed_dict()\n",
    "    \n",
    "    def create_variant(self, name, modifications):\n",
    "        \"\"\"Create a new experiment variant by modifying the base configuration.\"\"\"\n",
    "        variant_config = self.base_config.copy()\n",
    "        variant_config.experiment.name = name\n",
    "        \n",
    "        # Apply modifications\n",
    "        for key_path, value in modifications.items():\n",
    "            keys = key_path.split('.')\n",
    "            current = variant_config\n",
    "            for key in keys[:-1]:\n",
    "                current = current[key]\n",
    "            current[keys[-1]] = value\n",
    "        \n",
    "        # Generate unique identifier\n",
    "        config_id = scitex.dict.to_str(modifications, delimiter='_')\n",
    "        variant_config.experiment.config_id = config_id\n",
    "        \n",
    "        self.experiment_variants.append(variant_config)\n",
    "        return variant_config\n",
    "    \n",
    "    def run_experiment(self, config):\n",
    "        \"\"\"Simulate running an experiment with the given configuration.\"\"\"\n",
    "        # Simulate training\n",
    "        print(f\"Running experiment: {config.experiment.name}\")\n",
    "        print(f\"Config ID: {config.experiment.config_id}\")\n",
    "        \n",
    "        # Simulate results based on configuration\n",
    "        base_accuracy = 0.8\n",
    "        lr_factor = min(1.0, config.training.learning_rate * 1000)  # Penalize very high LR\n",
    "        dropout_factor = 1.0 - config.model.dropout * 0.5  # Slight penalty for high dropout\n",
    "        \n",
    "        final_accuracy = base_accuracy * lr_factor * dropout_factor + random.uniform(-0.1, 0.1)\n",
    "        final_accuracy = max(0.0, min(1.0, final_accuracy))  # Clamp to [0, 1]\n",
    "        \n",
    "        results = {\n",
    "            'accuracy': final_accuracy,\n",
    "            'loss': random.uniform(0.1, 0.5),\n",
    "            'training_time': random.uniform(60, 300)\n",
    "        }\n",
    "        \n",
    "        # Log results\n",
    "        self.results_log['experiment_name'].append(config.experiment.name)\n",
    "        self.results_log['config_id'].append(config.experiment.config_id)\n",
    "        self.results_log['accuracy'].append(results['accuracy'])\n",
    "        self.results_log['loss'].append(results['loss'])\n",
    "        self.results_log['training_time'].append(results['training_time'])\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get_best_config(self):\n",
    "        \"\"\"Find the best performing configuration.\"\"\"\n",
    "        if not self.results_log['accuracy']:\n",
    "            return None\n",
    "        \n",
    "        best_idx = np.argmax(self.results_log['accuracy'])\n",
    "        return {\n",
    "            'name': self.results_log['experiment_name'][best_idx],\n",
    "            'config_id': self.results_log['config_id'][best_idx],\n",
    "            'accuracy': self.results_log['accuracy'][best_idx],\n",
    "            'loss': self.results_log['loss'][best_idx],\n",
    "            'training_time': self.results_log['training_time'][best_idx]\n",
    "        }\n",
    "\n",
    "# Create experiment manager\n",
    "exp_manager = ExperimentConfig()\n",
    "\n",
    "# Create different experiment variants\n",
    "variants = [\n",
    "    ('high_lr', {'training.learning_rate': 0.01}),\n",
    "    ('low_lr', {'training.learning_rate': 0.0001}),\n",
    "    ('high_dropout', {'model.dropout': 0.5}),\n",
    "    ('large_batch', {'training.batch_size': 128}),\n",
    "    ('small_batch', {'training.batch_size': 8}),\n",
    "    ('deep_model', {'model.architecture': [100, 80, 60, 40, 20, 10]})\n",
    "]\n",
    "\n",
    "# Create and run experiments\n",
    "for variant_name, modifications in variants:\n",
    "    config = exp_manager.create_variant(variant_name, modifications)\n",
    "    results = exp_manager.run_experiment(config)\n",
    "    print(f\"Results: Accuracy={results['accuracy']:.3f}, Loss={results['loss']:.3f}\")\n",
    "    print()\n",
    "\n",
    "# Find best configuration\n",
    "best_config = exp_manager.get_best_config()\n",
    "print(f\"Best configuration:\")\n",
    "print(f\"Name: {best_config['name']}\")\n",
    "print(f\"Config ID: {best_config['config_id']}\")\n",
    "print(f\"Accuracy: {best_config['accuracy']:.3f}\")\n",
    "print(f\"Loss: {best_config['loss']:.3f}\")\n",
    "print(f\"Training time: {best_config['training_time']:.1f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Data Processing Pipeline with Dictionary Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data processing pipeline using dictionary utilities\n",
    "class DataProcessingPipeline:\n",
    "    def __init__(self):\n",
    "        self.pipeline_config = scitex.dict.DotDict({\n",
    "            'input': {\n",
    "                'source': 'database',\n",
    "                'format': 'csv',\n",
    "                'encoding': 'utf-8'\n",
    "            },\n",
    "            'preprocessing': {\n",
    "                'remove_duplicates': True,\n",
    "                'handle_missing': 'impute',\n",
    "                'normalize': True,\n",
    "                'feature_selection': True\n",
    "            },\n",
    "            'feature_engineering': {\n",
    "                'create_interactions': False,\n",
    "                'polynomial_features': False,\n",
    "                'text_vectorization': 'tfidf'\n",
    "            },\n",
    "            'output': {\n",
    "                'format': 'parquet',\n",
    "                'compression': 'snappy',\n",
    "                'save_metadata': True\n",
    "            }\n",
    "        })\n",
    "        \n",
    "        self.processing_log = scitex.dict.listed_dict()\n",
    "        self.feature_metadata = scitex.dict.listed_dict()\n",
    "    \n",
    "    def process_dataset(self, dataset_info):\n",
    "        \"\"\"Process a dataset according to the pipeline configuration.\"\"\"\n",
    "        dataset_name = dataset_info['name']\n",
    "        original_features = dataset_info['features']\n",
    "        \n",
    "        print(f\"Processing dataset: {dataset_name}\")\n",
    "        print(f\"Original features: {len(original_features)}\")\n",
    "        \n",
    "        # Step 1: Preprocessing\n",
    "        processed_features = original_features.copy()\n",
    "        \n",
    "        if self.pipeline_config.preprocessing.remove_duplicates:\n",
    "            # Simulate removing duplicate features\n",
    "            duplicate_features = ['feature_1_copy', 'feature_2_duplicate']\n",
    "            processed_features = scitex.dict.pop_keys(processed_features, duplicate_features)\n",
    "            print(f\"Removed duplicates: {len(original_features) - len(processed_features)} features\")\n",
    "        \n",
    "        if self.pipeline_config.preprocessing.feature_selection:\n",
    "            # Simulate feature selection\n",
    "            low_variance_features = [f for f in processed_features if 'low_var' in f]\n",
    "            processed_features = scitex.dict.pop_keys(processed_features, low_variance_features)\n",
    "            print(f\"Feature selection: {len(processed_features)} features remaining\")\n",
    "        \n",
    "        # Step 2: Feature Engineering\n",
    "        if self.pipeline_config.feature_engineering.create_interactions:\n",
    "            # Simulate creating interaction features\n",
    "            interaction_features = [f\"{f1}_x_{f2}\" for f1 in processed_features[:3] for f2 in processed_features[3:6]]\n",
    "            processed_features.extend(interaction_features)\n",
    "            print(f\"Created interactions: {len(interaction_features)} new features\")\n",
    "        \n",
    "        # Log processing results\n",
    "        processing_summary = {\n",
    "            'dataset': dataset_name,\n",
    "            'original_features': len(original_features),\n",
    "            'final_features': len(processed_features),\n",
    "            'reduction_ratio': len(processed_features) / len(original_features),\n",
    "            'config': scitex.dict.to_str(self.pipeline_config.preprocessing.to_dict(), delimiter='|')\n",
    "        }\n",
    "        \n",
    "        for key, value in processing_summary.items():\n",
    "            self.processing_log[key].append(value)\n",
    "        \n",
    "        # Store feature metadata\n",
    "        self.feature_metadata['dataset_name'].append(dataset_name)\n",
    "        self.feature_metadata['final_features'].append(processed_features)\n",
    "        self.feature_metadata['feature_count'].append(len(processed_features))\n",
    "        \n",
    "        return processed_features\n",
    "    \n",
    "    def get_processing_summary(self):\n",
    "        \"\"\"Get a summary of all processing operations.\"\"\"\n",
    "        if not self.processing_log['dataset']:\n",
    "            return \"No datasets processed yet.\"\n",
    "        \n",
    "        summary_df = pd.DataFrame(dict(self.processing_log))\n",
    "        return summary_df\n",
    "    \n",
    "    def merge_feature_sets(self, *feature_sets):\n",
    "        \"\"\"Safely merge multiple feature sets.\"\"\"\n",
    "        try:\n",
    "            # Convert lists to dictionaries for merging\n",
    "            feature_dicts = []\n",
    "            for i, features in enumerate(feature_sets):\n",
    "                feature_dict = {f\"set_{i}_{j}\": feature for j, feature in enumerate(features)}\n",
    "                feature_dicts.append(feature_dict)\n",
    "            \n",
    "            merged_dict = scitex.dict.safe_merge(*feature_dicts)\n",
    "            merged_features = list(merged_dict.values())\n",
    "            \n",
    "            print(f\"Successfully merged {len(feature_sets)} feature sets\")\n",
    "            print(f\"Total features: {len(merged_features)}\")\n",
    "            \n",
    "            return merged_features\n",
    "        except ValueError as e:\n",
    "            print(f\"Feature merge failed: {e}\")\n",
    "            return None\n",
    "\n",
    "# Create processing pipeline\n",
    "pipeline = DataProcessingPipeline()\n",
    "\n",
    "# Define test datasets\n",
    "test_datasets = [\n",
    "    {\n",
    "        'name': 'customer_data',\n",
    "        'features': ['age', 'income', 'feature_1_copy', 'education', 'low_var_1', 'spending', 'low_var_2', 'location']\n",
    "    },\n",
    "    {\n",
    "        'name': 'product_data',\n",
    "        'features': ['price', 'category', 'rating', 'feature_2_duplicate', 'reviews', 'low_var_3', 'availability']\n",
    "    },\n",
    "    {\n",
    "        'name': 'transaction_data',\n",
    "        'features': ['amount', 'timestamp', 'payment_method', 'low_var_4', 'merchant', 'low_var_5']\n",
    "    }\n",
    "]\n",
    "\n",
    "# Process all datasets\n",
    "processed_feature_sets = []\n",
    "for dataset in test_datasets:\n",
    "    processed_features = pipeline.process_dataset(dataset)\n",
    "    processed_feature_sets.append(processed_features)\n",
    "    print(f\"Final features for {dataset['name']}: {processed_features}\")\n",
    "    print()\n",
    "\n",
    "# Get processing summary\n",
    "summary = pipeline.get_processing_summary()\n",
    "print(\"Processing Summary:\")\n",
    "print(summary)\n",
    "print()\n",
    "\n",
    "# Merge feature sets (this should work as features are from different datasets)\n",
    "merged_features = pipeline.merge_feature_sets(*processed_feature_sets)\n",
    "if merged_features:\n",
    "    print(f\"\\nMerged features: {merged_features}\")\n",
    "    print(f\"Total merged features: {len(merged_features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Best Practices\n",
    "\n",
    "This tutorial demonstrated the comprehensive dictionary utilities available in the SciTeX library:\n",
    "\n",
    "### Key Components Covered:\n",
    "\n",
    "1. **DotDict**: Enables attribute-style access to dictionary keys\n",
    "   - Supports nested dictionaries\n",
    "   - Handles various data types (arrays, DataFrames, functions)\n",
    "   - Provides standard dictionary methods\n",
    "\n",
    "2. **listed_dict**: Automatically initializes lists for new keys\n",
    "   - Perfect for data collection and aggregation\n",
    "   - Supports predefined keys\n",
    "   - Integrates well with pandas DataFrames\n",
    "\n",
    "3. **safe_merge**: Merges dictionaries with conflict detection\n",
    "   - Prevents accidental overwrites\n",
    "   - Useful for configuration management\n",
    "   - Supports multiple dictionaries\n",
    "\n",
    "4. **pop_keys**: Removes specified keys from key lists\n",
    "   - Feature selection and filtering\n",
    "   - Data privacy and security\n",
    "   - Multi-stage processing pipelines\n",
    "\n",
    "5. **replace**: String replacement using dictionaries\n",
    "   - Template processing\n",
    "   - Code generation\n",
    "   - Report generation\n",
    "\n",
    "6. **to_str**: Converts dictionaries to string representations\n",
    "   - Experiment identification\n",
    "   - File naming\n",
    "   - Configuration tracking\n",
    "\n",
    "### Best Practices:\n",
    "\n",
    "- Use **DotDict** for configuration objects and nested data structures\n",
    "- Use **listed_dict** for collecting experimental data and metrics\n",
    "- Use **safe_merge** when combining configurations from multiple sources\n",
    "- Use **pop_keys** for feature selection and data filtering\n",
    "- Use **replace** for template processing and code generation\n",
    "- Use **to_str** for creating unique identifiers and file names\n",
    "- Combine utilities for complex data processing pipelines\n",
    "- Always validate merged configurations in critical applications\n",
    "- Use meaningful delimiters in **to_str** for better readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"SciTeX Dictionary Utilities Tutorial Complete!\")\n",
    "print(\"\\nKey takeaways:\")\n",
    "print(\"1. DotDict provides intuitive attribute-style access to dictionaries\")\n",
    "print(\"2. listed_dict simplifies data collection and aggregation\")\n",
    "print(\"3. safe_merge prevents configuration conflicts\")\n",
    "print(\"4. pop_keys enables flexible feature selection\")\n",
    "print(\"5. replace supports powerful template processing\")\n",
    "print(\"6. to_str creates unique identifiers from parameters\")\n",
    "print(\"7. All utilities work together for complex scientific computing workflows\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}