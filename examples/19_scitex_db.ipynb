{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprehensive SciTeX Database Module Examples\n",
    "\n",
    "This notebook demonstrates the complete functionality of the `scitex.db` module, which provides database operations and utilities for scientific data management.\n",
    "\n",
    "## Module Overview\n",
    "\n",
    "The `scitex.db` module includes:\n",
    "- SQLite3 database management with comprehensive mixins\n",
    "- PostgreSQL database operations\n",
    "- Database inspection and analysis tools\n",
    "- Duplicate data detection and removal\n",
    "\n",
    "## Import Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tempfile\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Import scitex db module\n",
    "import scitex.db as sdb\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Available classes/functions in scitex.db:\")\n",
    "db_attrs = [attr for attr in dir(sdb) if not attr.startswith('_')]\n",
    "for i, attr in enumerate(db_attrs):\n",
    "    print(f\"{i+1:2d}. {attr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Database Inspection Tools\n",
    "\n",
    "### Creating Sample Database\n",
    "\n",
    "Let's start by creating a sample database for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Create sample database for demonstration\n",
    "# Create temporary database file\n",
    "temp_db = tempfile.NamedTemporaryFile(suffix='.db', delete=False)\n",
    "db_path = temp_db.name\n",
    "temp_db.close()\n",
    "\n",
    "print(f\"Creating sample database at: {db_path}\")\n",
    "\n",
    "# Create sample data\n",
    "np.random.seed(42)\n",
    "\n",
    "# Experimental data table\n",
    "experiments_data = {\n",
    "    'experiment_id': range(1, 21),\n",
    "    'subject_id': [f'S{i:03d}' for i in np.random.randint(1, 11, 20)],\n",
    "    'condition': np.random.choice(['control', 'treatment_A', 'treatment_B'], 20),\n",
    "    'measurement': np.random.normal(100, 15, 20),\n",
    "    'timestamp': pd.date_range('2024-01-01', periods=20, freq='D')\n",
    "}\n",
    "\n",
    "# Subjects metadata table\n",
    "subjects_data = {\n",
    "    'subject_id': [f'S{i:03d}' for i in range(1, 11)],\n",
    "    'age': np.random.randint(18, 65, 10),\n",
    "    'gender': np.random.choice(['M', 'F'], 10),\n",
    "    'group': np.random.choice(['A', 'B'], 10)\n",
    "}\n",
    "\n",
    "# Time series data table\n",
    "time_series_data = []\n",
    "for exp_id in range(1, 6):  # First 5 experiments have time series\n",
    "    n_points = 100\n",
    "    time_points = np.linspace(0, 10, n_points)\n",
    "    signal = np.sin(2 * np.pi * time_points) + 0.1 * np.random.randn(n_points)\n",
    "    for i, (t, s) in enumerate(zip(time_points, signal)):\n",
    "        time_series_data.append({\n",
    "            'experiment_id': exp_id,\n",
    "            'time_point': i,\n",
    "            'time_value': t,\n",
    "            'signal_value': s\n",
    "        })\n",
    "\n",
    "# Create database and tables\n",
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "# Experiments table\n",
    "experiments_df = pd.DataFrame(experiments_data)\n",
    "experiments_df.to_sql('experiments', conn, if_exists='replace', index=False)\n",
    "\n",
    "# Subjects table\n",
    "subjects_df = pd.DataFrame(subjects_data)\n",
    "subjects_df.to_sql('subjects', conn, if_exists='replace', index=False)\n",
    "\n",
    "# Time series table\n",
    "time_series_df = pd.DataFrame(time_series_data)\n",
    "time_series_df.to_sql('time_series', conn, if_exists='replace', index=False)\n",
    "\n",
    "conn.close()\n",
    "\n",
    "print(f\"Sample database created with {len(experiments_df)} experiments\")\n",
    "print(f\"Number of subjects: {len(subjects_df)}\")\n",
    "print(f\"Number of time series points: {len(time_series_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Database Inspection\n",
    "\n",
    "Now let's use the `inspect` function to examine our database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Database inspection\n",
    "print(\"Database Inspection:\")\n",
    "print(\"=\" * 20)\n",
    "\n",
    "try:\n",
    "    # Inspect all tables\n",
    "    inspection_results = sdb.inspect(db_path, verbose=True)\n",
    "    \n",
    "    print(f\"\\nNumber of tables inspected: {len(inspection_results)}\")\n",
    "    \n",
    "    # Show structure of each inspection result\n",
    "    for i, result in enumerate(inspection_results):\n",
    "        print(f\"\\nTable {i+1} structure:\")\n",
    "        print(f\"Index: {result.index.names}\")\n",
    "        print(f\"Columns: {list(result.columns)}\")\n",
    "        print(f\"Shape: {result.shape}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error during inspection: {e}\")\n",
    "    print(\"This might require additional dependencies or configuration.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Inspect specific tables\n",
    "print(\"Inspecting specific tables:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "try:\n",
    "    # Inspect only experiments table\n",
    "    experiments_inspection = sdb.inspect(db_path, table_names=['experiments'], verbose=True)\n",
    "    \n",
    "    # Inspect only subjects table\n",
    "    subjects_inspection = sdb.inspect(db_path, table_names=['subjects'], verbose=True)\n",
    "    \n",
    "    print(\"\\nSpecific table inspection completed.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error during specific table inspection: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. SQLite3 Database Management\n",
    "\n",
    "### Basic SQLite3 Operations\n",
    "\n",
    "Let's demonstrate the comprehensive SQLite3 class functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 4: SQLite3 database management\n",
    "print(\"SQLite3 Database Management:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "try:\n",
    "    # Initialize SQLite3 database manager\n",
    "    db_manager = sdb.SQLite3(db_path)\n",
    "    \n",
    "    print(f\"Database manager initialized for: {db_path}\")\n",
    "    print(f\"SQLite3 class available methods:\")\n",
    "    \n",
    "    # Show available methods\n",
    "    methods = [method for method in dir(db_manager) if not method.startswith('_')]\n",
    "    for i, method in enumerate(methods[:10]):  # Show first 10 methods\n",
    "        print(f\"  {i+1:2d}. {method}\")\n",
    "    if len(methods) > 10:\n",
    "        print(f\"  ... and {len(methods) - 10} more methods\")\n",
    "    \n",
    "    # Call the database summary\n",
    "    print(\"\\nDatabase Summary:\")\n",
    "    summary = db_manager(print_summary=True, verbose=True)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error with SQLite3 manager: {e}\")\n",
    "    print(\"This might require additional dependencies or configuration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Database Querying\n",
    "\n",
    "Let's demonstrate database querying capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 5: Database querying\n",
    "print(\"Database Querying Examples:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Direct SQL queries for demonstration\n",
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "try:\n",
    "    # Query 1: Basic select\n",
    "    print(\"1. Basic SELECT query:\")\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"SELECT * FROM experiments LIMIT 5\")\n",
    "    results = cursor.fetchall()\n",
    "    \n",
    "    # Get column names\n",
    "    cursor.execute(\"PRAGMA table_info(experiments)\")\n",
    "    columns = [col[1] for col in cursor.fetchall()]\n",
    "    \n",
    "    print(f\"Columns: {columns}\")\n",
    "    print(\"First 5 experiments:\")\n",
    "    for row in results:\n",
    "        print(f\"  {dict(zip(columns, row))}\")\n",
    "    \n",
    "    # Query 2: Aggregation\n",
    "    print(\"\\n2. Aggregation query:\")\n",
    "    cursor.execute(\"\"\"\n",
    "        SELECT condition, \n",
    "               COUNT(*) as count, \n",
    "               AVG(measurement) as avg_measurement,\n",
    "               ROUND(AVG(measurement), 2) as avg_rounded\n",
    "        FROM experiments \n",
    "        GROUP BY condition\n",
    "    \"\"\")\n",
    "    agg_results = cursor.fetchall()\n",
    "    \n",
    "    print(\"Results by condition:\")\n",
    "    for row in agg_results:\n",
    "        print(f\"  Condition: {row[0]}, Count: {row[1]}, Avg: {row[2]:.2f}\")\n",
    "    \n",
    "    # Query 3: Join query\n",
    "    print(\"\\n3. JOIN query:\")\n",
    "    cursor.execute(\"\"\"\n",
    "        SELECT e.experiment_id, e.condition, e.measurement, s.age, s.gender\n",
    "        FROM experiments e\n",
    "        JOIN subjects s ON e.subject_id = s.subject_id\n",
    "        LIMIT 5\n",
    "    \"\"\")\n",
    "    join_results = cursor.fetchall()\n",
    "    \n",
    "    print(\"Experiment data with subject info:\")\n",
    "    for row in join_results:\n",
    "        print(f\"  Exp {row[0]}: {row[1]}, measurement={row[2]:.1f}, subject: age={row[3]}, gender={row[4]}\")\n",
    "    \n",
    "    # Query 4: Statistical analysis\n",
    "    print(\"\\n4. Statistical analysis:\")\n",
    "    cursor.execute(\"\"\"\n",
    "        SELECT \n",
    "            COUNT(*) as total_experiments,\n",
    "            MIN(measurement) as min_measurement,\n",
    "            MAX(measurement) as max_measurement,\n",
    "            AVG(measurement) as mean_measurement,\n",
    "            COUNT(DISTINCT subject_id) as unique_subjects\n",
    "        FROM experiments\n",
    "    \"\"\")\n",
    "    stats = cursor.fetchone()\n",
    "    \n",
    "    print(f\"Total experiments: {stats[0]}\")\n",
    "    print(f\"Measurement range: [{stats[1]:.2f}, {stats[2]:.2f}]\")\n",
    "    print(f\"Mean measurement: {stats[3]:.2f}\")\n",
    "    print(f\"Unique subjects: {stats[4]}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error during querying: {e}\")\n",
    "    \n",
    "finally:\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Duplicate Detection and Removal\n",
    "\n",
    "### Creating Data with Duplicates\n",
    "\n",
    "Let's create a database with duplicate entries to demonstrate the duplicate removal functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 6: Create database with duplicates\n",
    "print(\"Creating database with duplicate entries:\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Create temporary database with duplicates\n",
    "temp_dup_db = tempfile.NamedTemporaryFile(suffix='.db', delete=False)\n",
    "dup_db_path = temp_dup_db.name\n",
    "temp_dup_db.close()\n",
    "\n",
    "# Create sample data with intentional duplicates\n",
    "np.random.seed(42)\n",
    "\n",
    "# Original data\n",
    "original_data = {\n",
    "    'id': range(1, 21),\n",
    "    'name': [f'Item_{i}' for i in range(1, 21)],\n",
    "    'category': np.random.choice(['A', 'B', 'C'], 20),\n",
    "    'value': np.random.randint(1, 100, 20),\n",
    "    'date': pd.date_range('2024-01-01', periods=20, freq='D')\n",
    "}\n",
    "\n",
    "original_df = pd.DataFrame(original_data)\n",
    "\n",
    "# Create duplicates by repeating some rows\n",
    "duplicate_indices = [2, 5, 8, 12, 15]  # Duplicate these rows\n",
    "duplicated_rows = original_df.iloc[duplicate_indices].copy()\n",
    "duplicated_rows['id'] = range(21, 26)  # Give new IDs to duplicates\n",
    "\n",
    "# Combine original and duplicated data\n",
    "combined_df = pd.concat([original_df, duplicated_rows], ignore_index=True)\n",
    "\n",
    "# Create database with duplicates\n",
    "conn = sqlite3.connect(dup_db_path)\n",
    "combined_df.to_sql('test_data', conn, if_exists='replace', index=False)\n",
    "conn.close()\n",
    "\n",
    "print(f\"Created database with duplicates at: {dup_db_path}\")\n",
    "print(f\"Original entries: {len(original_df)}\")\n",
    "print(f\"Duplicate entries added: {len(duplicated_rows)}\")\n",
    "print(f\"Total entries: {len(combined_df)}\")\n",
    "\n",
    "# Show the data structure\n",
    "print(\"\\nFirst few entries:\")\n",
    "print(combined_df.head())\n",
    "\n",
    "print(\"\\nDuplicated entries (different IDs, same other data):\")\n",
    "for idx in duplicate_indices:\n",
    "    original_row = original_df.iloc[idx]\n",
    "    duplicate_row = duplicated_rows[duplicated_rows.index == duplicate_indices.index(idx)].iloc[0]\n",
    "    print(f\"Original: ID={original_row['id']}, Name={original_row['name']}, Category={original_row['category']}\")\n",
    "    print(f\"Duplicate: ID={duplicate_row['id']}, Name={duplicate_row['name']}, Category={duplicate_row['category']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duplicate Detection and Removal\n",
    "\n",
    "Now let's use the `delete_duplicates` function to identify and remove duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 7: Duplicate detection and removal (dry run)\n",
    "print(\"Duplicate Detection and Removal:\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "try:\n",
    "    # First, do a dry run to see what would be removed\n",
    "    print(\"1. DRY RUN - Detecting duplicates:\")\n",
    "    print(\"-\" * 35)\n",
    "    \n",
    "    # Exclude 'id' column from duplicate detection (since IDs are unique)\n",
    "    columns_to_check = ['name', 'category', 'value', 'date']\n",
    "    \n",
    "    dry_run_result = sdb.delete_duplicates(\n",
    "        dup_db_path,\n",
    "        'test_data',\n",
    "        columns=columns_to_check,\n",
    "        dry_run=True\n",
    "    )\n",
    "    \n",
    "    if dry_run_result:\n",
    "        total_processed, total_duplicates = dry_run_result\n",
    "        print(f\"\\nDry run results:\")\n",
    "        print(f\"Total rows that would be processed: {total_processed}\")\n",
    "        print(f\"Total duplicates that would be removed: {total_duplicates}\")\n",
    "    \n",
    "    # Now do the actual removal\n",
    "    print(\"\\n2. ACTUAL REMOVAL - Removing duplicates:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    actual_result = sdb.delete_duplicates(\n",
    "        dup_db_path,\n",
    "        'test_data',\n",
    "        columns=columns_to_check,\n",
    "        dry_run=False\n",
    "    )\n",
    "    \n",
    "    if actual_result:\n",
    "        total_processed, total_duplicates = actual_result\n",
    "        print(f\"\\nActual removal results:\")\n",
    "        print(f\"Total rows processed: {total_processed}\")\n",
    "        print(f\"Total duplicates removed: {total_duplicates}\")\n",
    "    \n",
    "    # Verify the results\n",
    "    print(\"\\n3. VERIFICATION - Checking results:\")\n",
    "    print(\"-\" * 35)\n",
    "    \n",
    "    conn = sqlite3.connect(dup_db_path)\n",
    "    remaining_df = pd.read_sql_query(\"SELECT * FROM test_data\", conn)\n",
    "    conn.close()\n",
    "    \n",
    "    print(f\"Remaining entries after duplicate removal: {len(remaining_df)}\")\n",
    "    print(f\"Expected unique entries: {len(original_df)}\")\n",
    "    \n",
    "    # Check if there are still duplicates\n",
    "    duplicate_check = remaining_df[columns_to_check].duplicated().sum()\n",
    "    print(f\"Remaining duplicates: {duplicate_check}\")\n",
    "    \n",
    "    if duplicate_check == 0:\n",
    "        print(\"✓ All duplicates successfully removed!\")\n",
    "    else:\n",
    "        print(\"⚠ Some duplicates may still remain.\")\n",
    "    \n",
    "    # Show final data\n",
    "    print(\"\\nFinal data sample:\")\n",
    "    print(remaining_df.head())\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error during duplicate removal: {e}\")\n",
    "    print(\"This might require additional dependencies or configuration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. PostgreSQL Database Operations\n",
    "\n",
    "### PostgreSQL Class Demonstration\n",
    "\n",
    "Note: PostgreSQL operations require a running PostgreSQL server and proper credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 8: PostgreSQL operations (conceptual demonstration)\n",
    "print(\"PostgreSQL Database Operations:\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "try:\n",
    "    # Note: This will likely fail without a running PostgreSQL server\n",
    "    # This is just to show the interface\n",
    "    \n",
    "    print(\"Attempting to create PostgreSQL connection...\")\n",
    "    print(\"(This will likely fail without a running PostgreSQL server)\")\n",
    "    \n",
    "    # Show available PostgreSQL class\n",
    "    print(f\"\\nPostgreSQL class available: {hasattr(sdb, 'PostgreSQL')}\")\n",
    "    \n",
    "    if hasattr(sdb, 'PostgreSQL'):\n",
    "        print(\"PostgreSQL class methods:\")\n",
    "        postgres_methods = [method for method in dir(sdb.PostgreSQL) if not method.startswith('_')]\n",
    "        for i, method in enumerate(postgres_methods[:10]):\n",
    "            print(f\"  {i+1:2d}. {method}\")\n",
    "        if len(postgres_methods) > 10:\n",
    "            print(f\"  ... and {len(postgres_methods) - 10} more methods\")\n",
    "    \n",
    "    # Conceptual usage (would require actual PostgreSQL server)\n",
    "    print(\"\\nConceptual usage:\")\n",
    "    print(\"# pg_db = sdb.PostgreSQL(\")\n",
    "    print(\"#     dbname='scientific_data',\")\n",
    "    print(\"#     user='researcher',\")\n",
    "    print(\"#     password='password',\")\n",
    "    print(\"#     host='localhost',\")\n",
    "    print(\"#     port=5432\")\n",
    "    print(\"# )\")\n",
    "    print(\"# pg_db.connect()\")\n",
    "    print(\"# results = pg_db.query('SELECT * FROM experiments')\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"PostgreSQL operations not available: {e}\")\n",
    "    print(\"This requires psycopg2 and a running PostgreSQL server.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Practical Applications\n",
    "\n",
    "### Scientific Data Management Workflow\n",
    "\n",
    "Let's demonstrate a complete scientific data management workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 9: Complete scientific data management workflow\n",
    "print(\"Scientific Data Management Workflow:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Create a more complex scientific database\n",
    "workflow_db = tempfile.NamedTemporaryFile(suffix='.db', delete=False)\n",
    "workflow_db_path = workflow_db.name\n",
    "workflow_db.close()\n",
    "\n",
    "print(f\"Creating scientific database at: {workflow_db_path}\")\n",
    "\n",
    "# Simulate experimental setup\n",
    "np.random.seed(42)\n",
    "\n",
    "# 1. Experimental conditions\n",
    "conditions = {\n",
    "    'condition_id': range(1, 5),\n",
    "    'condition_name': ['baseline', 'low_dose', 'medium_dose', 'high_dose'],\n",
    "    'dose_mg': [0, 10, 50, 100],\n",
    "    'description': [\n",
    "        'Control condition',\n",
    "        'Low dose treatment',\n",
    "        'Medium dose treatment', \n",
    "        'High dose treatment'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# 2. Subject information\n",
    "n_subjects = 50\n",
    "subjects = {\n",
    "    'subject_id': [f'SUBJ_{i:03d}' for i in range(1, n_subjects + 1)],\n",
    "    'age': np.random.randint(18, 80, n_subjects),\n",
    "    'gender': np.random.choice(['M', 'F'], n_subjects),\n",
    "    'weight_kg': np.random.normal(70, 15, n_subjects),\n",
    "    'group': np.random.choice(['experimental', 'control'], n_subjects),\n",
    "    'enrollment_date': pd.date_range('2024-01-01', periods=n_subjects, freq='D')\n",
    "}\n",
    "\n",
    "# 3. Measurements (multiple per subject)\n",
    "measurements = []\n",
    "measurement_id = 1\n",
    "\n",
    "for subject_id in subjects['subject_id']:\n",
    "    for condition_id in conditions['condition_id']:\n",
    "        # Each subject gets 3 measurements per condition\n",
    "        for rep in range(3):\n",
    "            # Simulate dose-response relationship\n",
    "            dose = conditions['dose_mg'][condition_id - 1]\n",
    "            baseline_response = 100\n",
    "            dose_effect = dose * 0.5 + np.random.normal(0, 10)\n",
    "            response = baseline_response + dose_effect + np.random.normal(0, 5)\n",
    "            \n",
    "            measurements.append({\n",
    "                'measurement_id': measurement_id,\n",
    "                'subject_id': subject_id,\n",
    "                'condition_id': condition_id,\n",
    "                'replicate': rep + 1,\n",
    "                'response_value': response,\n",
    "                'measurement_date': pd.Timestamp('2024-01-01') + pd.Timedelta(days=measurement_id),\n",
    "                'quality_score': np.random.uniform(0.7, 1.0)\n",
    "            })\n",
    "            measurement_id += 1\n",
    "\n",
    "# 4. Create database\n",
    "conn = sqlite3.connect(workflow_db_path)\n",
    "\n",
    "# Create tables\n",
    "conditions_df = pd.DataFrame(conditions)\n",
    "subjects_df = pd.DataFrame(subjects)\n",
    "measurements_df = pd.DataFrame(measurements)\n",
    "\n",
    "conditions_df.to_sql('conditions', conn, if_exists='replace', index=False)\n",
    "subjects_df.to_sql('subjects', conn, if_exists='replace', index=False)\n",
    "measurements_df.to_sql('measurements', conn, if_exists='replace', index=False)\n",
    "\n",
    "conn.close()\n",
    "\n",
    "print(f\"Database created with:\")\n",
    "print(f\"  - {len(conditions_df)} experimental conditions\")\n",
    "print(f\"  - {len(subjects_df)} subjects\")\n",
    "print(f\"  - {len(measurements_df)} measurements\")\n",
    "\n",
    "# 5. Inspect the database\n",
    "print(\"\\nInspecting the scientific database:\")\n",
    "try:\n",
    "    inspection_results = sdb.inspect(workflow_db_path, verbose=False)\n",
    "    print(f\"Successfully inspected {len(inspection_results)} tables\")\n",
    "except Exception as e:\n",
    "    print(f\"Inspection error: {e}\")\n",
    "\n",
    "# 6. Perform scientific analysis queries\n",
    "print(\"\\nScientific Analysis Queries:\")\n",
    "conn = sqlite3.connect(workflow_db_path)\n",
    "\n",
    "try:\n",
    "    # Analysis 1: Dose-response relationship\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"\"\"\n",
    "        SELECT c.condition_name, c.dose_mg, \n",
    "               COUNT(m.measurement_id) as n_measurements,\n",
    "               AVG(m.response_value) as mean_response,\n",
    "               ROUND(AVG(m.response_value), 2) as mean_rounded,\n",
    "               MIN(m.response_value) as min_response,\n",
    "               MAX(m.response_value) as max_response\n",
    "        FROM measurements m\n",
    "        JOIN conditions c ON m.condition_id = c.condition_id\n",
    "        GROUP BY c.condition_id, c.condition_name, c.dose_mg\n",
    "        ORDER BY c.dose_mg\n",
    "    \"\"\")\n",
    "    \n",
    "    dose_response = cursor.fetchall()\n",
    "    print(\"\\n1. Dose-Response Analysis:\")\n",
    "    print(\"Condition\\t\\tDose (mg)\\tN\\tMean Response\\tRange\")\n",
    "    print(\"-\" * 65)\n",
    "    for row in dose_response:\n",
    "        print(f\"{row[0]:15s}\\t{row[1]:8.0f}\\t{row[2]:3d}\\t{row[4]:11.2f}\\t[{row[5]:.1f}, {row[6]:.1f}]\")\n",
    "    \n",
    "    # Analysis 2: Subject demographics\n",
    "    cursor.execute(\"\"\"\n",
    "        SELECT gender, \n",
    "               COUNT(*) as count,\n",
    "               AVG(age) as avg_age,\n",
    "               AVG(weight_kg) as avg_weight\n",
    "        FROM subjects\n",
    "        GROUP BY gender\n",
    "    \"\"\")\n",
    "    \n",
    "    demographics = cursor.fetchall()\n",
    "    print(\"\\n2. Subject Demographics:\")\n",
    "    print(\"Gender\\tCount\\tAvg Age\\tAvg Weight (kg)\")\n",
    "    print(\"-\" * 40)\n",
    "    for row in demographics:\n",
    "        print(f\"{row[0]}\\t{row[1]}\\t{row[2]:.1f}\\t{row[3]:.1f}\")\n",
    "    \n",
    "    # Analysis 3: Data quality assessment\n",
    "    cursor.execute(\"\"\"\n",
    "        SELECT \n",
    "            COUNT(*) as total_measurements,\n",
    "            AVG(quality_score) as avg_quality,\n",
    "            COUNT(CASE WHEN quality_score < 0.8 THEN 1 END) as low_quality_count,\n",
    "            ROUND(COUNT(CASE WHEN quality_score < 0.8 THEN 1 END) * 100.0 / COUNT(*), 2) as low_quality_percent\n",
    "        FROM measurements\n",
    "    \"\"\")\n",
    "    \n",
    "    quality = cursor.fetchone()\n",
    "    print(\"\\n3. Data Quality Assessment:\")\n",
    "    print(f\"Total measurements: {quality[0]}\")\n",
    "    print(f\"Average quality score: {quality[1]:.3f}\")\n",
    "    print(f\"Low quality measurements (<0.8): {quality[2]} ({quality[3]}%)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Analysis error: {e}\")\n",
    "    \n",
    "finally:\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Database Maintenance and Optimization\n",
    "\n",
    "Let's demonstrate database maintenance operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 10: Database maintenance and optimization\n",
    "print(\"Database Maintenance and Optimization:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Check database file size before maintenance\n",
    "db_size_before = os.path.getsize(workflow_db_path)\n",
    "print(f\"Database size before maintenance: {db_size_before / 1024:.2f} KB\")\n",
    "\n",
    "# Perform maintenance operations\n",
    "conn = sqlite3.connect(workflow_db_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "try:\n",
    "    # 1. Analyze database structure\n",
    "    print(\"\\n1. Database Structure Analysis:\")\n",
    "    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\n",
    "    tables = cursor.fetchall()\n",
    "    print(f\"Number of tables: {len(tables)}\")\n",
    "    \n",
    "    for table in tables:\n",
    "        table_name = table[0]\n",
    "        cursor.execute(f\"SELECT COUNT(*) FROM {table_name}\")\n",
    "        row_count = cursor.fetchone()[0]\n",
    "        print(f\"  {table_name}: {row_count} rows\")\n",
    "    \n",
    "    # 2. Check for indexes\n",
    "    print(\"\\n2. Index Analysis:\")\n",
    "    cursor.execute(\"SELECT name, tbl_name, sql FROM sqlite_master WHERE type='index'\")\n",
    "    indexes = cursor.fetchall()\n",
    "    print(f\"Number of indexes: {len(indexes)}\")\n",
    "    \n",
    "    # 3. Create useful indexes for scientific queries\n",
    "    print(\"\\n3. Creating Indexes for Performance:\")\n",
    "    \n",
    "    # Index on measurements for faster joins\n",
    "    try:\n",
    "        cursor.execute(\"CREATE INDEX IF NOT EXISTS idx_measurements_subject ON measurements(subject_id)\")\n",
    "        cursor.execute(\"CREATE INDEX IF NOT EXISTS idx_measurements_condition ON measurements(condition_id)\")\n",
    "        cursor.execute(\"CREATE INDEX IF NOT EXISTS idx_measurements_date ON measurements(measurement_date)\")\n",
    "        print(\"  ✓ Created indexes on measurements table\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Error creating indexes: {e}\")\n",
    "    \n",
    "    # 4. Database statistics\n",
    "    print(\"\\n4. Database Statistics:\")\n",
    "    cursor.execute(\"PRAGMA database_list\")\n",
    "    db_info = cursor.fetchall()\n",
    "    print(f\"Database info: {db_info[0]}\")\n",
    "    \n",
    "    # Check page count and size\n",
    "    cursor.execute(\"PRAGMA page_count\")\n",
    "    page_count = cursor.fetchone()[0]\n",
    "    cursor.execute(\"PRAGMA page_size\")\n",
    "    page_size = cursor.fetchone()[0]\n",
    "    \n",
    "    print(f\"Pages: {page_count}, Page size: {page_size} bytes\")\n",
    "    print(f\"Calculated DB size: {page_count * page_size / 1024:.2f} KB\")\n",
    "    \n",
    "    # 5. Vacuum database to reclaim space\n",
    "    print(\"\\n5. Database Vacuum Operation:\")\n",
    "    cursor.execute(\"VACUUM\")\n",
    "    conn.commit()\n",
    "    print(\"  ✓ Database vacuumed successfully\")\n",
    "    \n",
    "    # 6. Update statistics\n",
    "    cursor.execute(\"ANALYZE\")\n",
    "    conn.commit()\n",
    "    print(\"  ✓ Database statistics updated\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Maintenance error: {e}\")\n",
    "    \n",
    "finally:\n",
    "    conn.close()\n",
    "\n",
    "# Check database size after maintenance\n",
    "db_size_after = os.path.getsize(workflow_db_path)\n",
    "print(f\"\\nDatabase size after maintenance: {db_size_after / 1024:.2f} KB\")\n",
    "size_change = db_size_after - db_size_before\n",
    "print(f\"Size change: {size_change / 1024:+.2f} KB\")\n",
    "\n",
    "if size_change < 0:\n",
    "    print(f\"Space saved: {abs(size_change) / 1024:.2f} KB\")\n",
    "elif size_change > 0:\n",
    "    print(f\"Space used (indexes): {size_change / 1024:.2f} KB\")\n",
    "else:\n",
    "    print(\"No size change\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Cleanup\n",
    "\n",
    "Let's clean up the temporary databases created during this demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 11: Cleanup temporary databases\n",
    "print(\"Cleaning up temporary databases:\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "temp_databases = [\n",
    "    (db_path, \"Sample database\"),\n",
    "    (dup_db_path, \"Duplicate test database\"),\n",
    "    (workflow_db_path, \"Scientific workflow database\")\n",
    "]\n",
    "\n",
    "for db_file, description in temp_databases:\n",
    "    try:\n",
    "        if os.path.exists(db_file):\n",
    "            size = os.path.getsize(db_file)\n",
    "            os.unlink(db_file)\n",
    "            print(f\"✓ Removed {description} ({size / 1024:.2f} KB)\")\n",
    "        else:\n",
    "            print(f\"✗ {description} not found\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error removing {description}: {e}\")\n",
    "\n",
    "print(\"\\nCleanup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook has demonstrated the comprehensive functionality of the `scitex.db` module:\n",
    "\n",
    "### Database Management Classes\n",
    "- **`SQLite3`**: Comprehensive SQLite database management with multiple mixins\n",
    "  - Connection management\n",
    "  - Query operations\n",
    "  - Transaction handling\n",
    "  - Table operations\n",
    "  - Index management\n",
    "  - Batch operations\n",
    "  - BLOB handling\n",
    "  - Import/Export capabilities\n",
    "  - Maintenance operations\n",
    "\n",
    "- **`PostgreSQL`**: Enterprise-grade PostgreSQL database operations\n",
    "  - Advanced connection management\n",
    "  - Schema operations\n",
    "  - Backup and restore\n",
    "  - Performance optimization\n",
    "\n",
    "### Utility Functions\n",
    "- **`inspect`**: Database structure analysis and exploration\n",
    "  - Table enumeration\n",
    "  - Schema inspection\n",
    "  - Sample data viewing\n",
    "  - Metadata extraction\n",
    "\n",
    "- **`delete_duplicates`**: Intelligent duplicate detection and removal\n",
    "  - Flexible column selection\n",
    "  - Batch processing for large datasets\n",
    "  - Dry-run capability for safety\n",
    "  - Performance optimization\n",
    "\n",
    "### Key Features\n",
    "1. **Scientific Focus**: Designed for research data management\n",
    "2. **Robustness**: Comprehensive error handling and validation\n",
    "3. **Performance**: Optimized for large scientific datasets\n",
    "4. **Flexibility**: Support for various database operations\n",
    "5. **Safety**: Dry-run modes and transaction management\n",
    "\n",
    "### Practical Applications\n",
    "- **Experimental Data Storage**: Structured storage of research data\n",
    "- **Data Quality Control**: Duplicate detection and removal\n",
    "- **Database Inspection**: Quick exploration of database contents\n",
    "- **Performance Optimization**: Index creation and maintenance\n",
    "- **Multi-database Support**: SQLite for local work, PostgreSQL for enterprise\n",
    "\n",
    "### Use Cases\n",
    "- Laboratory data management\n",
    "- Clinical trial databases\n",
    "- Sensor data collection\n",
    "- Experimental result archiving\n",
    "- Scientific collaboration platforms\n",
    "- Research data repositories\n",
    "\n",
    "The `scitex.db` module provides a complete toolkit for scientific database management, from simple data storage to complex multi-table research databases with advanced querying and maintenance capabilities."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}