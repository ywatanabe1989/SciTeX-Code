{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SciTeX Scholar - Complete Tutorial with Impact Factor Integration\n",
    "\n",
    "This notebook demonstrates the new simplified `Scholar` class for scientific literature management with comprehensive impact factor support.\n",
    "\n",
    "## Key Improvements\n",
    "\n",
    "- **Single entry point**: One `Scholar` class for all functionality\n",
    "- **Default enrichment**: Papers are enriched with journal metrics by default\n",
    "- **No async complexity**: Simple synchronous API that works in notebooks\n",
    "- **Chainable methods**: Fluent interface for common workflows\n",
    "- **Smart defaults**: Works out-of-the-box with reasonable settings\n",
    "- **📊 Impact Factor Integration**: Automatic journal impact factor lookup using the `impact_factor` package\n",
    "- **🏆 Journal Rankings**: Quartile and ranking information for comprehensive evaluation\n",
    "\n",
    "## Installation & Setup\n",
    "\n",
    "Make sure you have scitex and impact_factor installed:\n",
    "```bash\n",
    "pip install -e ~/proj/scitex_repo\n",
    "# impact_factor package should be automatically available\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the new simplified interface\n",
    "from scitex.scholar import Scholar\n",
    "\n",
    "# Import impact factor tools for direct database access\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import impact_factor\n",
    "\n",
    "# Optional: Set up API keys for enhanced features\n",
    "import os\n",
    "# os.environ['SEMANTIC_SCHOLAR_API_KEY'] = 'your_key_here'\n",
    "# os.environ['OPENAI_API_KEY'] = 'your_key_here'\n",
    "\n",
    "print(\"✅ Scholar and Impact Factor tools loaded successfully!\")\n",
    "print(f\"📊 Impact Factor database: {impact_factor.DEFAULT_DB}\")\n",
    "print(f\"📈 Database size: {os.path.getsize(impact_factor.DEFAULT_DB) / 1024 / 1024:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Quick Start - Simple Search\n",
    "\n",
    "The fastest way to get started:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick search using Scholar class directly with impact factor enrichment\n",
    "scholar = Scholar(enrich_by_default=True)  # Ensure enrichment is enabled\n",
    "papers = scholar.search(\"deep learning neuroscience\", limit=5)\n",
    "\n",
    "# Display results with comprehensive impact factor information\n",
    "print(f\"📚 Found {len(papers)} papers (automatically enriched with impact factors):\\n\")\n",
    "for i, paper in enumerate(papers, 1):\n",
    "    print(f\"{i}. {paper.title}\")\n",
    "    print(f\"   Authors: {', '.join(paper.authors[:2]) if paper.authors else 'Unknown'}...\")\n",
    "    print(f\"   Journal: {paper.journal if paper.journal else 'Unknown'}\")\n",
    "    print(f\"   Year: {paper.year}, Citations: {paper.citation_count}\")\n",
    "    \n",
    "    # Show enriched impact factor data if available\n",
    "    if hasattr(paper, 'impact_factor') and paper.impact_factor:\n",
    "        print(f\"   📊 Journal Impact Factor: {paper.impact_factor}\")\n",
    "    if hasattr(paper, 'journal_quartile') and paper.journal_quartile:\n",
    "        print(f\"   🏆 Journal Quartile: {paper.journal_quartile}\")\n",
    "    if hasattr(paper, 'journal_ranking') and paper.journal_ranking:\n",
    "        print(f\"   📈 Journal Ranking: {paper.journal_ranking}\")\n",
    "    \n",
    "    # Additional impact factor lookup if not automatically enriched\n",
    "    if paper.journal and (not hasattr(paper, 'impact_factor') or not paper.impact_factor):\n",
    "        # Direct lookup from impact_factor database\n",
    "        try:\n",
    "            conn = sqlite3.connect(impact_factor.DEFAULT_DB)\n",
    "            query = \"SELECT factor, jcr FROM factor WHERE journal LIKE ? ORDER BY factor DESC LIMIT 1\"\n",
    "            result = pd.read_sql_query(query, conn, params=[f'%{paper.journal}%'])\n",
    "            conn.close()\n",
    "            \n",
    "            if len(result) > 0:\n",
    "                print(f\"   📊 Impact Factor (manual lookup): {result.iloc[0]['factor']:.3f}\")\n",
    "                print(f\"   🏆 Quartile (manual lookup): {result.iloc[0]['jcr']}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ⚠️ Could not lookup impact factor: {e}\")\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Using the Scholar Class\n",
    "\n",
    "For more control and advanced features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Scholar with custom settings\n",
    "scholar = Scholar(\n",
    "    email=\"researcher@university.edu\",  # For PubMed access\n",
    "    enrich_by_default=True,              # Default enrichment (can be turned off)\n",
    "    workspace_dir=\"./scholar_workspace\"  # Custom workspace\n",
    ")\n",
    "\n",
    "# Get workspace info\n",
    "info = scholar.get_workspace_info()\n",
    "print(\"Scholar Workspace Info:\")\n",
    "for key, value in info.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Enhanced Search with Filtering\n",
    "\n",
    "Search and filter papers using the fluent interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search with filtering and sorting\n",
    "recent_papers = scholar.search(\"neural networks\", limit=20) \\\n",
    "                      .filter(year_min=2020, min_citations=10) \\\n",
    "                      .sort_by(\"citations\")\n",
    "\n",
    "print(f\"Found {len(recent_papers)} recent high-impact papers:\\n\")\n",
    "\n",
    "for i, paper in enumerate(recent_papers[:5], 1):\n",
    "    print(f\"{i}. {paper.title[:80]}...\")\n",
    "    print(f\"   Year: {paper.year}, Citations: {paper.citation_count}\")\n",
    "    if hasattr(paper, 'impact_factor') and paper.impact_factor:\n",
    "        print(f\"   Journal IF: {paper.impact_factor}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Multiple Topic Search\n",
    "\n",
    "Search multiple topics and combine results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search multiple related topics\n",
    "topics = [\n",
    "    \"transformer neural networks\",\n",
    "    \"attention mechanisms deep learning\",\n",
    "    \"BERT language models\"\n",
    "]\n",
    "\n",
    "all_papers = scholar.search_multiple(\n",
    "    queries=topics,\n",
    "    papers_per_query=5,\n",
    "    combine_results=True  # Automatically removes duplicates\n",
    ")\n",
    "\n",
    "print(f\"Combined search found {len(all_papers)} unique papers\")\n",
    "\n",
    "# Filter for high-impact recent work\n",
    "high_impact = all_papers.filter(year_min=2019, min_citations=50)\n",
    "print(f\"High-impact recent papers: {len(high_impact)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Bibliography Generation\n",
    "\n",
    "Generate enriched bibliographies with automatic formatting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for papers on a specific topic with impact factor filtering\n",
    "ml_papers = scholar.search(\"machine learning interpretability\", limit=15)\n",
    "\n",
    "# Filter for quality papers with impact factor consideration\n",
    "quality_papers = ml_papers.filter(year_min=2018, min_citations=20)\n",
    "\n",
    "print(f\"📊 Generating enriched bibliography for {len(quality_papers)} papers...\")\n",
    "\n",
    "# Enrich papers with impact factor information if not already enriched\n",
    "def enrich_paper_with_impact_factor(paper):\n",
    "    \"\"\"Add impact factor information to a paper if available.\"\"\"\n",
    "    if not paper.journal:\n",
    "        return paper\n",
    "    \n",
    "    try:\n",
    "        conn = sqlite3.connect(impact_factor.DEFAULT_DB)\n",
    "        query = \"\"\"\n",
    "        SELECT factor, jcr, journal_abbr \n",
    "        FROM factor \n",
    "        WHERE journal LIKE ? \n",
    "        ORDER BY factor DESC \n",
    "        LIMIT 1\n",
    "        \"\"\"\n",
    "        result = pd.read_sql_query(query, conn, params=[f'%{paper.journal}%'])\n",
    "        conn.close()\n",
    "        \n",
    "        if len(result) > 0:\n",
    "            paper.impact_factor = result.iloc[0]['factor']\n",
    "            paper.journal_quartile = result.iloc[0]['jcr']\n",
    "            paper.journal_abbr = result.iloc[0]['journal_abbr']\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Could not enrich {paper.journal}: {e}\")\n",
    "    \n",
    "    return paper\n",
    "\n",
    "# Enrich all papers\n",
    "enriched_papers = [enrich_paper_with_impact_factor(paper) for paper in quality_papers]\n",
    "\n",
    "# Show impact factor distribution\n",
    "impact_factors = [p.impact_factor for p in enriched_papers if hasattr(p, 'impact_factor') and p.impact_factor]\n",
    "if impact_factors:\n",
    "    print(f\"📈 Impact Factor Statistics:\")\n",
    "    print(f\"   Papers with IF data: {len(impact_factors)}/{len(enriched_papers)}\")\n",
    "    print(f\"   Average IF: {sum(impact_factors)/len(impact_factors):.3f}\")\n",
    "    print(f\"   Highest IF: {max(impact_factors):.3f}\")\n",
    "    print(f\"   Lowest IF: {min(impact_factors):.3f}\")\n",
    "\n",
    "# Save as enriched BibTeX (includes impact factors)\n",
    "bib_file = \"ml_interpretability_enriched.bib\"\n",
    "print(f\"\\n💾 Saving enriched bibliography to: {bib_file}\")\n",
    "\n",
    "try:\n",
    "    # Generate enriched BibTeX entries\n",
    "    with open(bib_file, 'w', encoding='utf-8') as f:\n",
    "        for paper in enriched_papers:\n",
    "            bibtex = paper.to_bibtex(include_enriched=True)\n",
    "            f.write(bibtex + \"\\n\\n\")\n",
    "    \n",
    "    print(f\"✅ Successfully saved {len(enriched_papers)} enriched entries\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error saving bibliography: {e}\")\n",
    "\n",
    "# Preview first enriched entry\n",
    "if enriched_papers:\n",
    "    print(\"\\n📄 Sample enriched BibTeX entry:\")\n",
    "    print(\"=\" * 60)\n",
    "    try:\n",
    "        sample_bibtex = enriched_papers[0].to_bibtex(include_enriched=True)\n",
    "        # Show first 500 characters\n",
    "        preview = sample_bibtex[:500] + \"...\" if len(sample_bibtex) > 500 else sample_bibtex\n",
    "        print(preview)\n",
    "    except Exception as e:\n",
    "        print(f\"Could not generate sample BibTeX: {e}\")\n",
    "\n",
    "# Show papers by impact factor quartile\n",
    "quartile_distribution = {}\n",
    "for paper in enriched_papers:\n",
    "    if hasattr(paper, 'journal_quartile') and paper.journal_quartile:\n",
    "        quartile_distribution[paper.journal_quartile] = quartile_distribution.get(paper.journal_quartile, 0) + 1\n",
    "\n",
    "if quartile_distribution:\n",
    "    print(f\"\\n🏆 Journal Quartile Distribution:\")\n",
    "    for quartile, count in sorted(quartile_distribution.items()):\n",
    "        print(f\"   {quartile}: {count} papers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. PDF Downloads\n",
    "\n",
    "Download PDFs for open-access papers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for open-access papers\n",
    "oa_papers = scholar.search(\"computer vision\", limit=5)\n",
    "\n",
    "# Filter for potentially open-access papers\n",
    "recent_papers = oa_papers.filter(year_min=2020)\n",
    "\n",
    "print(f\"Attempting to download PDFs for {len(recent_papers)} papers...\")\n",
    "\n",
    "try:\n",
    "    # Download PDFs (max 3 to avoid overwhelming servers)\n",
    "    downloaded = scholar.download_pdfs(recent_papers, max_downloads=3)\n",
    "    \n",
    "    print(f\"\\nSuccessfully downloaded {len(downloaded)} PDFs:\")\n",
    "    for title, path in downloaded.items():\n",
    "        print(f\"  - {title[:60]}... → {path.name}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"PDF download failed: {e}\")\n",
    "    print(\"Note: PDF downloads require papers to be open-access\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Local PDF Indexing and Search\n",
    "\n",
    "Build searchable index from your local PDF collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have PDFs downloaded, build a local index\n",
    "pdf_dir = \"./scholar_workspace/pdfs\"\n",
    "\n",
    "try:\n",
    "    import os\n",
    "    if os.path.exists(pdf_dir) and os.listdir(pdf_dir):\n",
    "        print(f\"Building search index from {pdf_dir}...\")\n",
    "        index_path = scholar.build_local_index(pdf_dir)\n",
    "        print(f\"Index created: {index_path}\")\n",
    "        \n",
    "        # Search your local collection\n",
    "        local_results = scholar.search_local(\"neural networks\")\n",
    "        print(f\"\\nFound {len(local_results)} papers in local collection\")\n",
    "        \n",
    "    else:\n",
    "        print(\"No PDFs found for indexing. Download some PDFs first.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Local indexing failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Advanced Features\n",
    "\n",
    "### Paper Collection Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a larger collection for analysis\n",
    "ai_papers = scholar.search(\"artificial intelligence\", limit=30)\n",
    "\n",
    "# Analyze the collection\n",
    "print(\"Collection Analysis:\")\n",
    "print(f\"Total papers: {len(ai_papers)}\")\n",
    "\n",
    "# Year distribution\n",
    "years = [p.year for p in ai_papers if p.year]\n",
    "if years:\n",
    "    print(f\"Year range: {min(years)} - {max(years)}\")\n",
    "    from collections import Counter\n",
    "    year_counts = Counter(years)\n",
    "    print(\"Papers by year:\")\n",
    "    for year, count in sorted(year_counts.items(), reverse=True)[:5]:\n",
    "        print(f\"  {year}: {count} papers\")\n",
    "\n",
    "# Citation analysis\n",
    "citations = [p.citation_count for p in ai_papers if p.citation_count]\n",
    "if citations:\n",
    "    print(f\"\\nCitation statistics:\")\n",
    "    print(f\"  Average citations: {sum(citations)/len(citations):.1f}\")\n",
    "    print(f\"  Max citations: {max(citations)}\")\n",
    "    print(f\"  Highly cited (>100): {sum(1 for c in citations if c > 100)}\")\n",
    "\n",
    "# Export to different formats\n",
    "data_export = ai_papers.to_dict()\n",
    "print(f\"\\nExported {len(data_export)} papers to dictionary format\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison with Old API\n",
    "\n",
    "Here's how the new API compares to the old approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== OLD API (Complex) ===\")\n",
    "print(\"\"\"\n",
    "# Old way - multiple imports and manual enrichment\n",
    "from scitex.scholar import search_papers, PaperEnrichmentService, generate_enriched_bibliography\n",
    "import asyncio\n",
    "\n",
    "# Async search\n",
    "papers = await search_papers(\"deep learning\", limit=10)\n",
    "\n",
    "# Manual enrichment\n",
    "enricher = PaperEnrichmentService()\n",
    "enriched_papers = enricher.enrich_papers(papers)\n",
    "\n",
    "# Manual bibliography generation\n",
    "generate_enriched_bibliography(enriched_papers, \"output.bib\", enrich=False)\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n=== NEW API (Simple) ===\")\n",
    "print(\"\"\"\n",
    "# New way - one class, automatic enrichment\n",
    "from scitex.scholar import Scholar\n",
    "\n",
    "# Simple search with automatic enrichment\n",
    "scholar = Scholar()\n",
    "papers = scholar.search(\"deep learning\", limit=10)\n",
    "\n",
    "# One-liner bibliography with enrichment\n",
    "papers.save_bibliography(\"output.bib\")\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n✅ The new API is much simpler and more intuitive!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Best Practices\n",
    "\n",
    "### Performance Tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Reuse Scholar instance for multiple searches\n",
    "scholar = Scholar(enrich_by_default=True)  # Initialize once\n",
    "\n",
    "# Multiple searches reuse the same components\n",
    "papers1 = scholar.search(\"topic 1\", limit=5)\n",
    "papers2 = scholar.search(\"topic 2\", limit=5)\n",
    "\n",
    "# 2. Use appropriate limits\n",
    "# For exploration: limit=10-20\n",
    "# For comprehensive reviews: limit=50-100\n",
    "# For quick checks: limit=5\n",
    "\n",
    "# 3. Filter early to reduce processing\n",
    "recent_quality = scholar.search(\"machine learning\", limit=50) \\\n",
    "                       .filter(year_min=2020, min_citations=10) \\\n",
    "                       .sort_by(\"impact_factor\")\n",
    "\n",
    "print(f\"Filtered to {len(recent_quality)} high-quality recent papers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Scholar class handles errors gracefully\n",
    "try:\n",
    "    # Even if some components fail, basic search should work\n",
    "    papers = scholar.search(\"test query\", limit=3)\n",
    "    print(f\"Search successful: found {len(papers)} papers\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Search failed: {e}\")\n",
    "    # Fallback to basic Scholar search with minimal features\n",
    "    scholar_basic = Scholar(enrich_by_default=False)\n",
    "    papers = scholar_basic.search(\"test query\", limit=3)\n",
    "    print(f\"Fallback search: found {len(papers)} papers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The new `Scholar` class provides:\n",
    "\n",
    "✅ **Single entry point** - No need to import multiple classes  \n",
    "✅ **Default enrichment** - Papers automatically get journal metrics  \n",
    "✅ **Simple sync API** - No async/await complexity  \n",
    "✅ **Chainable methods** - Fluent interface for workflows  \n",
    "✅ **Smart defaults** - Works out-of-the-box  \n",
    "✅ **Progress feedback** - See what's happening during long operations  \n",
    "✅ **Error resilience** - Graceful fallbacks when components fail  \n",
    "\n",
    "### Quick Reference\n",
    "\n",
    "```python\n",
    "# Basic usage\n",
    "from scitex.scholar import Scholar\n",
    "scholar = Scholar()\n",
    "papers = scholar.search(\"your topic\", limit=20)\n",
    "papers.save_bibliography(\"papers.bib\")\n",
    "\n",
    "# Advanced workflow\n",
    "papers = scholar.search(\"topic\", limit=50) \\\n",
    "               .filter(year_min=2020, min_citations=10) \\\n",
    "               .sort_by(\"impact_factor\")\n",
    "scholar.download_pdfs(papers, max_downloads=5)\n",
    "```\n",
    "\n",
    "The Scholar class maintains backward compatibility with all existing components while providing a much simpler interface for new users."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
