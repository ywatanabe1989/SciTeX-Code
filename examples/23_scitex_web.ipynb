{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SciTeX Web Operations Tutorial\n",
    "\n",
    "This comprehensive notebook demonstrates the SciTeX web utilities module, covering scientific literature search, web scraping, content extraction, and URL processing for research workflows.\n",
    "\n",
    "## Features Covered\n",
    "\n",
    "### Academic Literature Search\n",
    "* PubMed search functionality\n",
    "* Article metadata retrieval\n",
    "* BibTeX citation generation\n",
    "* CrossRef metrics integration\n",
    "\n",
    "### Web Content Processing\n",
    "* URL content summarization\n",
    "* Main content extraction\n",
    "* Web crawling capabilities\n",
    "* JSON conversion utilities\n",
    "\n",
    "### Research Applications\n",
    "* Literature review automation\n",
    "* Reference management\n",
    "* Web-based data collection\n",
    "* Content analysis pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "import scitex\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.parse\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create output directory for web examples\n",
    "web_output = Path('./web_examples')\n",
    "web_output.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"SciTeX Web Operations Tutorial - Ready to begin!\")\n",
    "print(\"Note: This tutorial requires internet connection for demonstration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Academic Literature Search with PubMed\n",
    "\n",
    "### 1.1 Basic PubMed Search\n",
    "\n",
    "The SciTeX web module provides powerful tools for searching academic literature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for recent papers on machine learning in neuroscience\n",
    "try:\n",
    "    print(\"Searching PubMed for machine learning in neuroscience...\")\n",
    "    \n",
    "    # Perform search with limited results for demonstration\n",
    "    search_query = \"machine learning neuroscience 2023[PDAT]\"\n",
    "    \n",
    "    # Note: In practice, you would use scitex.web.search_pubmed()\n",
    "    # For demonstration, we'll simulate the search results\n",
    "    \n",
    "    # Simulated search results structure\n",
    "    search_results = {\n",
    "        'query': search_query,\n",
    "        'total_results': 1245,\n",
    "        'retrieved': 10,\n",
    "        'articles': [\n",
    "            {\n",
    "                'pmid': '37123456',\n",
    "                'title': 'Deep Learning Approaches for Epilepsy Prediction',\n",
    "                'authors': ['Smith, J.', 'Johnson, A.', 'Williams, B.'],\n",
    "                'journal': 'Nature Neuroscience',\n",
    "                'year': '2023',\n",
    "                'doi': '10.1038/s41593-023-01234-5',\n",
    "                'abstract': 'This study presents novel deep learning methods for predicting epileptic seizures...',\n",
    "                'keywords': ['epilepsy', 'deep learning', 'EEG', 'prediction']\n",
    "            },\n",
    "            {\n",
    "                'pmid': '37123457',\n",
    "                'title': 'Neural Networks for fMRI Data Analysis',\n",
    "                'authors': ['Brown, C.', 'Davis, E.', 'Miller, F.'],\n",
    "                'journal': 'NeuroImage',\n",
    "                'year': '2023',\n",
    "                'doi': '10.1016/j.neuroimage.2023.123456',\n",
    "                'abstract': 'We demonstrate the application of convolutional neural networks to fMRI analysis...',\n",
    "                'keywords': ['fMRI', 'neural networks', 'brain imaging', 'connectivity']\n",
    "            },\n",
    "            {\n",
    "                'pmid': '37123458',\n",
    "                'title': 'Machine Learning in Parkinson\\'s Disease Research',\n",
    "                'authors': ['Garcia, H.', 'Lopez, I.', 'Martinez, J.'],\n",
    "                'journal': 'Brain',\n",
    "                'year': '2023',\n",
    "                'doi': '10.1093/brain/awad123',\n",
    "                'abstract': 'This review examines machine learning applications in Parkinson\\'s disease...',\n",
    "                'keywords': ['Parkinson\\'s disease', 'machine learning', 'biomarkers', 'diagnosis']\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    print(f\"Search completed successfully!\")\n",
    "    print(f\"Query: {search_results['query']}\")\n",
    "    print(f\"Total results available: {search_results['total_results']}\")\n",
    "    print(f\"Retrieved: {search_results['retrieved']} articles\")\n",
    "    \n",
    "    # Display first few results\n",
    "    print(\"\\nFirst 3 results:\")\n",
    "    for i, article in enumerate(search_results['articles'][:3]):\n",
    "        print(f\"\\n{i+1}. {article['title']}\")\n",
    "        print(f\"   Authors: {', '.join(article['authors'])}\")\n",
    "        print(f\"   Journal: {article['journal']} ({article['year']})\")\n",
    "        print(f\"   PMID: {article['pmid']}\")\n",
    "        print(f\"   DOI: {article['doi']}\")\n",
    "        print(f\"   Keywords: {', '.join(article['keywords'])}\")\n",
    "        print(f\"   Abstract: {article['abstract'][:100]}...\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Search simulation: {e}\")\n",
    "    print(\"Note: In practice, this would connect to PubMed API\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 BibTeX Citation Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate BibTeX citations from search results\n",
    "def generate_bibtex_entry(article):\n",
    "    \"\"\"Generate a BibTeX entry for an article.\"\"\"\n",
    "    # Clean title and create citation key\n",
    "    first_author_last = article['authors'][0].split(',')[0].replace(' ', '')\n",
    "    year = article['year']\n",
    "    title_words = article['title'].split()[:3]\n",
    "    key_words = [w.lower().replace(',', '').replace('.', '') for w in title_words]\n",
    "    citation_key = f\"{first_author_last}{year}{''.join(key_words)}\"\n",
    "    \n",
    "    # Format authors for BibTeX\n",
    "    authors_bibtex = ' and '.join(article['authors'])\n",
    "    \n",
    "    # Create BibTeX entry\n",
    "    bibtex_entry = f\"\"\"@article{{{citation_key},\n",
    "    title = {{{article['title']}}},\n",
    "    author = {{{authors_bibtex}}},\n",
    "    journal = {{{article['journal']}}},\n",
    "    year = {{{article['year']}}},\n",
    "    doi = {{{article['doi']}}},\n",
    "    pmid = {{{article['pmid']}}},\n",
    "    keywords = {{{', '.join(article['keywords'])}}}\n",
    "}}\"\"\"\n",
    "    return bibtex_entry\n",
    "\n",
    "# Generate BibTeX for all articles\n",
    "bibtex_entries = []\n",
    "for article in search_results['articles']:\n",
    "    bibtex_entry = generate_bibtex_entry(article)\n",
    "    bibtex_entries.append(bibtex_entry)\n",
    "\n",
    "# Save to file\n",
    "bibtex_file = web_output / 'literature_search.bib'\n",
    "with open(bibtex_file, 'w') as f:\n",
    "    f.write('\\n\\n'.join(bibtex_entries))\n",
    "\n",
    "print(f\"Generated BibTeX file: {bibtex_file}\")\n",
    "print(f\"Number of entries: {len(bibtex_entries)}\")\n",
    "\n",
    "# Display first entry\n",
    "print(\"\\nFirst BibTeX entry:\")\n",
    "print(bibtex_entries[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Literature Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze search results\n",
    "articles_df = pd.DataFrame(search_results['articles'])\n",
    "\n",
    "print(\"Literature search analysis:\")\n",
    "print(f\"Total articles: {len(articles_df)}\")\n",
    "print(f\"Unique journals: {articles_df['journal'].nunique()}\")\n",
    "print(f\"Year range: {articles_df['year'].min()} - {articles_df['year'].max()}\")\n",
    "\n",
    "# Journal distribution\n",
    "journal_counts = articles_df['journal'].value_counts()\n",
    "print(\"\\nJournal distribution:\")\n",
    "for journal, count in journal_counts.items():\n",
    "    print(f\"  {journal}: {count} articles\")\n",
    "\n",
    "# Keyword analysis\n",
    "all_keywords = []\n",
    "for keywords in articles_df['keywords']:\n",
    "    all_keywords.extend(keywords)\n",
    "\n",
    "keyword_counts = pd.Series(all_keywords).value_counts()\n",
    "print(f\"\\nTop keywords:\")\n",
    "for keyword, count in keyword_counts.head(10).items():\n",
    "    print(f\"  {keyword}: {count} occurrences\")\n",
    "\n",
    "# Create visualizations\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Journal distribution\n",
    "journal_counts.plot(kind='bar', ax=axes[0])\n",
    "axes[0].set_title('Articles by Journal')\n",
    "axes[0].set_xlabel('Journal')\n",
    "axes[0].set_ylabel('Number of Articles')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Top keywords\n",
    "keyword_counts.head(8).plot(kind='bar', ax=axes[1])\n",
    "axes[1].set_title('Top Keywords')\n",
    "axes[1].set_xlabel('Keywords')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save analysis results\n",
    "analysis_results = {\n",
    "    'search_query': search_results['query'],\n",
    "    'total_articles': len(articles_df),\n",
    "    'unique_journals': articles_df['journal'].nunique(),\n",
    "    'journal_distribution': journal_counts.to_dict(),\n",
    "    'top_keywords': keyword_counts.head(10).to_dict(),\n",
    "    'articles_summary': articles_df[['title', 'journal', 'year', 'pmid']].to_dict('records')\n",
    "}\n",
    "\n",
    "analysis_file = web_output / 'literature_analysis.json'\n",
    "with open(analysis_file, 'w') as f:\n",
    "    json.dump(analysis_results, f, indent=2)\n",
    "\n",
    "print(f\"\\nAnalysis saved to: {analysis_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Web Content Processing and Extraction\n",
    "\n",
    "### 2.1 URL Content Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate URL content extraction and summarization\n",
    "def extract_main_content(html_content):\n",
    "    \"\"\"Extract main content from HTML using BeautifulSoup.\"\"\"\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Remove script and style elements\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.decompose()\n",
    "    \n",
    "    # Get text\n",
    "    text = soup.get_text()\n",
    "    \n",
    "    # Break into lines and remove leading/trailing space\n",
    "    lines = (line.strip() for line in text.splitlines())\n",
    "    \n",
    "    # Break multi-headlines into a line each\n",
    "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "    \n",
    "    # Drop blank lines\n",
    "    text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def summarize_content(content, max_length=500):\n",
    "    \"\"\"Create a summary of content by taking first and key sentences.\"\"\"\n",
    "    sentences = content.split('. ')\n",
    "    \n",
    "    if len(content) <= max_length:\n",
    "        return content\n",
    "    \n",
    "    # Take first few sentences and look for key information\n",
    "    summary_sentences = sentences[:3]\n",
    "    \n",
    "    # Look for sentences with key scientific terms\n",
    "    key_terms = ['research', 'study', 'analysis', 'results', 'conclusion', 'method', 'data']\n",
    "    \n",
    "    for sentence in sentences[3:]:\n",
    "        if any(term in sentence.lower() for term in key_terms):\n",
    "            summary_sentences.append(sentence)\n",
    "            if len('. '.join(summary_sentences)) > max_length:\n",
    "                break\n",
    "    \n",
    "    summary = '. '.join(summary_sentences)\n",
    "    if len(summary) > max_length:\n",
    "        summary = summary[:max_length] + '...'\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Simulate processing different types of web content\n",
    "sample_urls = [\n",
    "    {\n",
    "        'url': 'https://example-research-site.com/article1',\n",
    "        'title': 'Machine Learning in Healthcare',\n",
    "        'content': '''Machine Learning in Healthcare: A Comprehensive Review\n",
    "        \n",
    "        This research article examines the current state of machine learning applications in healthcare. \n",
    "        The study analyzes over 500 recent papers to identify trends and opportunities. \n",
    "        Our analysis reveals significant growth in deep learning applications for medical imaging. \n",
    "        Results show that convolutional neural networks achieve 95% accuracy in radiological diagnosis. \n",
    "        The conclusion emphasizes the need for better data standardization and ethical frameworks. \n",
    "        Future research should focus on interpretability and clinical validation of ML models.'''\n",
    "    },\n",
    "    {\n",
    "        'url': 'https://university.edu/neuroscience-lab',\n",
    "        'title': 'Neuroscience Laboratory Research',\n",
    "        'content': '''Welcome to the Computational Neuroscience Laboratory\n",
    "        \n",
    "        Our laboratory focuses on understanding brain function through computational modeling. \n",
    "        We use advanced data analysis techniques to study neural networks and brain connectivity. \n",
    "        Current research projects include epilepsy prediction using EEG signals. \n",
    "        Our method combines signal processing with machine learning for real-time seizure detection. \n",
    "        The research team has published over 50 papers in top-tier journals. \n",
    "        We collaborate with hospitals to translate our findings into clinical applications.'''\n",
    "    },\n",
    "    {\n",
    "        'url': 'https://conference.org/proceedings',\n",
    "        'title': 'AI Conference Proceedings',\n",
    "        'content': '''International Conference on Artificial Intelligence in Medicine 2024\n",
    "        \n",
    "        This conference brings together researchers from around the world to discuss AI in medicine. \n",
    "        Over 200 presentations cover topics from natural language processing to computer vision. \n",
    "        Key findings include improved diagnostic accuracy using transformer models. \n",
    "        The study demonstrates that large language models can assist in medical documentation. \n",
    "        Results indicate 40% reduction in documentation time for physicians. \n",
    "        Future work will focus on multimodal AI systems for comprehensive patient care.'''\n",
    "    }\n",
    "]\n",
    "\n",
    "# Process and summarize content\n",
    "processed_content = []\n",
    "\n",
    "for item in sample_urls:\n",
    "    # Extract main content (simulated)\n",
    "    main_content = extract_main_content(f\"<html><body>{item['content']}</body></html>\")\n",
    "    \n",
    "    # Create summary\n",
    "    summary = summarize_content(main_content, max_length=300)\n",
    "    \n",
    "    processed_item = {\n",
    "        'url': item['url'],\n",
    "        'title': item['title'],\n",
    "        'original_length': len(item['content']),\n",
    "        'summary_length': len(summary),\n",
    "        'compression_ratio': len(summary) / len(item['content']),\n",
    "        'summary': summary\n",
    "    }\n",
    "    \n",
    "    processed_content.append(processed_item)\n",
    "\n",
    "# Display results\n",
    "print(\"Web Content Processing Results:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i, item in enumerate(processed_content, 1):\n",
    "    print(f\"\\n{i}. {item['title']}\")\n",
    "    print(f\"   URL: {item['url']}\")\n",
    "    print(f\"   Original length: {item['original_length']} characters\")\n",
    "    print(f\"   Summary length: {item['summary_length']} characters\")\n",
    "    print(f\"   Compression ratio: {item['compression_ratio']:.2f}\")\n",
    "    print(f\"   Summary: {item['summary']}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Save processed content\n",
    "content_file = web_output / 'processed_web_content.json'\n",
    "with open(content_file, 'w') as f:\n",
    "    json.dump(processed_content, f, indent=2)\n",
    "\n",
    "print(f\"\\nProcessed content saved to: {content_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Web Crawling and Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate web crawling functionality\n",
    "def simulate_crawl_url(base_url, max_depth=2):\n",
    "    \"\"\"Simulate crawling a website to collect content.\"\"\"\n",
    "    \n",
    "    # Simulate a research website structure\n",
    "    simulated_site = {\n",
    "        'https://research-institute.edu/': {\n",
    "            'title': 'Research Institute Home',\n",
    "            'content': 'Welcome to our research institute. We conduct cutting-edge research in AI and neuroscience.',\n",
    "            'links': [\n",
    "                'https://research-institute.edu/publications',\n",
    "                'https://research-institute.edu/projects',\n",
    "                'https://research-institute.edu/team'\n",
    "            ]\n",
    "        },\n",
    "        'https://research-institute.edu/publications': {\n",
    "            'title': 'Publications',\n",
    "            'content': 'Our recent publications include work on deep learning for medical diagnosis and brain-computer interfaces.',\n",
    "            'links': [\n",
    "                'https://research-institute.edu/publications/2024',\n",
    "                'https://research-institute.edu/publications/2023'\n",
    "            ]\n",
    "        },\n",
    "        'https://research-institute.edu/projects': {\n",
    "            'title': 'Research Projects',\n",
    "            'content': 'Current projects focus on epilepsy prediction, cancer diagnosis, and neural prosthetics.',\n",
    "            'links': [\n",
    "                'https://research-institute.edu/projects/epilepsy',\n",
    "                'https://research-institute.edu/projects/cancer'\n",
    "            ]\n",
    "        },\n",
    "        'https://research-institute.edu/publications/2024': {\n",
    "            'title': '2024 Publications',\n",
    "            'content': 'This year we published 15 papers in top-tier journals including Nature and Science.',\n",
    "            'links': []\n",
    "        },\n",
    "        'https://research-institute.edu/projects/epilepsy': {\n",
    "            'title': 'Epilepsy Prediction Project',\n",
    "            'content': 'Using machine learning to predict epileptic seizures from EEG data with 85% accuracy.',\n",
    "            'links': []\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    visited = set()\n",
    "    to_visit = [(base_url, 0)]\n",
    "    crawl_results = []\n",
    "    \n",
    "    while to_visit:\n",
    "        current_url, depth = to_visit.pop(0)\n",
    "        \n",
    "        if current_url in visited or depth > max_depth:\n",
    "            continue\n",
    "            \n",
    "        if current_url in simulated_site:\n",
    "            visited.add(current_url)\n",
    "            page_data = simulated_site[current_url]\n",
    "            \n",
    "            crawl_results.append({\n",
    "                'url': current_url,\n",
    "                'title': page_data['title'],\n",
    "                'content': page_data['content'],\n",
    "                'depth': depth,\n",
    "                'links_found': len(page_data['links']),\n",
    "                'word_count': len(page_data['content'].split())\n",
    "            })\n",
    "            \n",
    "            # Add linked pages to visit queue\n",
    "            for link in page_data['links']:\n",
    "                if link not in visited:\n",
    "                    to_visit.append((link, depth + 1))\n",
    "    \n",
    "    return crawl_results\n",
    "\n",
    "# Perform simulated crawl\n",
    "print(\"Simulating web crawl...\")\n",
    "crawl_results = simulate_crawl_url('https://research-institute.edu/', max_depth=2)\n",
    "\n",
    "print(f\"\\nCrawl completed!\")\n",
    "print(f\"Pages crawled: {len(crawl_results)}\")\n",
    "\n",
    "# Analyze crawl results\n",
    "crawl_df = pd.DataFrame(crawl_results)\n",
    "\n",
    "print(\"\\nCrawl Analysis:\")\n",
    "print(f\"Total pages: {len(crawl_df)}\")\n",
    "print(f\"Max depth reached: {crawl_df['depth'].max()}\")\n",
    "print(f\"Total words collected: {crawl_df['word_count'].sum()}\")\n",
    "print(f\"Average words per page: {crawl_df['word_count'].mean():.1f}\")\n",
    "\n",
    "# Display crawl results\n",
    "print(\"\\nCrawled Pages:\")\n",
    "for i, page in enumerate(crawl_results, 1):\n",
    "    print(f\"\\n{i}. {page['title']}\")\n",
    "    print(f\"   URL: {page['url']}\")\n",
    "    print(f\"   Depth: {page['depth']}\")\n",
    "    print(f\"   Links found: {page['links_found']}\")\n",
    "    print(f\"   Word count: {page['word_count']}\")\n",
    "    print(f\"   Content preview: {page['content'][:100]}...\")\n",
    "\n",
    "# Save crawl results\n",
    "crawl_file = web_output / 'crawl_results.json'\n",
    "with open(crawl_file, 'w') as f:\n",
    "    json.dump(crawl_results, f, indent=2)\n",
    "\n",
    "print(f\"\\nCrawl results saved to: {crawl_file}\")\n",
    "\n",
    "# Visualize crawl statistics\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Pages by depth\n",
    "depth_counts = crawl_df['depth'].value_counts().sort_index()\n",
    "depth_counts.plot(kind='bar', ax=axes[0])\n",
    "axes[0].set_title('Pages by Crawl Depth')\n",
    "axes[0].set_xlabel('Depth Level')\n",
    "axes[0].set_ylabel('Number of Pages')\n",
    "\n",
    "# Word count distribution\n",
    "crawl_df['word_count'].hist(bins=5, ax=axes[1])\n",
    "axes[1].set_title('Word Count Distribution')\n",
    "axes[1].set_xlabel('Words per Page')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Content Analysis and Information Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze collected web content for research insights\n",
    "def extract_research_terms(content):\n",
    "    \"\"\"Extract research-related terms from content.\"\"\"\n",
    "    research_keywords = [\n",
    "        'machine learning', 'deep learning', 'neural network', 'artificial intelligence',\n",
    "        'data analysis', 'statistical analysis', 'research', 'study', 'experiment',\n",
    "        'clinical trial', 'publication', 'journal', 'conference', 'peer review',\n",
    "        'neuroscience', 'brain', 'EEG', 'fMRI', 'epilepsy', 'seizure',\n",
    "        'diagnosis', 'prediction', 'accuracy', 'model', 'algorithm'\n",
    "    ]\n",
    "    \n",
    "    content_lower = content.lower()\n",
    "    found_terms = []\n",
    "    \n",
    "    for term in research_keywords:\n",
    "        if term in content_lower:\n",
    "            count = content_lower.count(term)\n",
    "            found_terms.append((term, count))\n",
    "    \n",
    "    return found_terms\n",
    "\n",
    "def categorize_content(content, title):\n",
    "    \"\"\"Categorize content based on research area.\"\"\"\n",
    "    categories = {\n",
    "        'Machine Learning': ['machine learning', 'deep learning', 'neural network', 'AI', 'algorithm'],\n",
    "        'Neuroscience': ['neuroscience', 'brain', 'EEG', 'fMRI', 'neural', 'seizure', 'epilepsy'],\n",
    "        'Medical': ['medical', 'clinical', 'diagnosis', 'patient', 'treatment', 'healthcare'],\n",
    "        'Research': ['research', 'study', 'experiment', 'analysis', 'publication', 'journal'],\n",
    "        'Data Science': ['data', 'statistics', 'analysis', 'model', 'prediction', 'accuracy']\n",
    "    }\n",
    "    \n",
    "    content_and_title = (content + ' ' + title).lower()\n",
    "    category_scores = {}\n",
    "    \n",
    "    for category, keywords in categories.items():\n",
    "        score = sum(content_and_title.count(keyword) for keyword in keywords)\n",
    "        category_scores[category] = score\n",
    "    \n",
    "    # Return category with highest score\n",
    "    if max(category_scores.values()) > 0:\n",
    "        return max(category_scores, key=category_scores.get)\n",
    "    else:\n",
    "        return 'General'\n",
    "\n",
    "# Analyze all collected content\n",
    "content_analysis = []\n",
    "\n",
    "# Combine crawl results and processed content\n",
    "all_content = crawl_results + processed_content\n",
    "\n",
    "for item in all_content:\n",
    "    content = item.get('content', item.get('summary', ''))\n",
    "    title = item['title']\n",
    "    \n",
    "    # Extract research terms\n",
    "    research_terms = extract_research_terms(content)\n",
    "    \n",
    "    # Categorize content\n",
    "    category = categorize_content(content, title)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    analysis_item = {\n",
    "        'title': title,\n",
    "        'url': item.get('url', 'N/A'),\n",
    "        'category': category,\n",
    "        'word_count': len(content.split()),\n",
    "        'research_terms_count': len(research_terms),\n",
    "        'research_terms': research_terms,\n",
    "        'top_terms': sorted(research_terms, key=lambda x: x[1], reverse=True)[:5]\n",
    "    }\n",
    "    \n",
    "    content_analysis.append(analysis_item)\n",
    "\n",
    "# Create analysis DataFrame\n",
    "analysis_df = pd.DataFrame(content_analysis)\n",
    "\n",
    "print(\"Content Analysis Results:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total content items analyzed: {len(analysis_df)}\")\n",
    "\n",
    "# Category distribution\n",
    "category_counts = analysis_df['category'].value_counts()\n",
    "print(f\"\\nContent categories:\")\n",
    "for category, count in category_counts.items():\n",
    "    print(f\"  {category}: {count} items\")\n",
    "\n",
    "# Research terms analysis\n",
    "all_research_terms = []\n",
    "for terms_list in analysis_df['research_terms']:\n",
    "    all_research_terms.extend(terms_list)\n",
    "\n",
    "# Count term frequencies\n",
    "term_frequencies = {}\n",
    "for term, count in all_research_terms:\n",
    "    if term in term_frequencies:\n",
    "        term_frequencies[term] += count\n",
    "    else:\n",
    "        term_frequencies[term] = count\n",
    "\n",
    "# Sort by frequency\n",
    "sorted_terms = sorted(term_frequencies.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(f\"\\nTop research terms across all content:\")\n",
    "for term, freq in sorted_terms[:10]:\n",
    "    print(f\"  {term}: {freq} occurrences\")\n",
    "\n",
    "# Display detailed analysis for each item\n",
    "print(f\"\\nDetailed Analysis:\")\n",
    "for i, item in enumerate(content_analysis, 1):\n",
    "    print(f\"\\n{i}. {item['title']}\")\n",
    "    print(f\"   Category: {item['category']}\")\n",
    "    print(f\"   Word count: {item['word_count']}\")\n",
    "    print(f\"   Research terms found: {item['research_terms_count']}\")\n",
    "    if item['top_terms']:\n",
    "        top_terms_str = ', '.join([f\"{term} ({count})\" for term, count in item['top_terms']])\n",
    "        print(f\"   Top terms: {top_terms_str}\")\n",
    "\n",
    "# Save analysis results\n",
    "analysis_summary = {\n",
    "    'total_items': len(analysis_df),\n",
    "    'category_distribution': category_counts.to_dict(),\n",
    "    'top_research_terms': dict(sorted_terms[:15]),\n",
    "    'detailed_analysis': content_analysis\n",
    "}\n",
    "\n",
    "analysis_file = web_output / 'content_analysis.json'\n",
    "with open(analysis_file, 'w') as f:\n",
    "    json.dump(analysis_summary, f, indent=2)\n",
    "\n",
    "print(f\"\\nContent analysis saved to: {analysis_file}\")\n",
    "\n",
    "# Visualize analysis results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Category distribution\n",
    "category_counts.plot(kind='pie', ax=axes[0], autopct='%1.1f%%')\n",
    "axes[0].set_title('Content Categories')\n",
    "axes[0].set_ylabel('')\n",
    "\n",
    "# Top terms frequency\n",
    "top_terms_df = pd.Series(dict(sorted_terms[:8]))\n",
    "top_terms_df.plot(kind='bar', ax=axes[1])\n",
    "axes[1].set_title('Top Research Terms Frequency')\n",
    "axes[1].set_xlabel('Terms')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Research Workflow Integration\n",
    "\n",
    "### 3.1 Automated Literature Review Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an automated literature review pipeline\n",
    "class LiteratureReviewPipeline:\n",
    "    def __init__(self, output_dir):\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.output_dir.mkdir(exist_ok=True)\n",
    "        self.search_results = []\n",
    "        self.processed_articles = []\n",
    "        self.analysis_results = {}\n",
    "    \n",
    "    def search_literature(self, queries, max_results_per_query=20):\n",
    "        \"\"\"Search for literature using multiple queries.\"\"\"\n",
    "        print(\"Searching literature...\")\n",
    "        \n",
    "        # Simulate searches for different queries\n",
    "        simulated_results = {\n",
    "            'machine learning epilepsy': [\n",
    "                {'title': 'Deep Learning for Epilepsy Detection', 'journal': 'Nature Medicine', 'year': '2024', 'relevance': 0.95},\n",
    "                {'title': 'ML-based Seizure Prediction Systems', 'journal': 'Brain', 'year': '2023', 'relevance': 0.89},\n",
    "                {'title': 'Neural Networks in EEG Analysis', 'journal': 'IEEE TBME', 'year': '2024', 'relevance': 0.82}\n",
    "            ],\n",
    "            'AI medical diagnosis': [\n",
    "                {'title': 'AI in Radiology: Current Status', 'journal': 'Radiology', 'year': '2024', 'relevance': 0.91},\n",
    "                {'title': 'Computer Vision for Medical Imaging', 'journal': 'Medical Image Analysis', 'year': '2023', 'relevance': 0.87},\n",
    "                {'title': 'Deep Learning in Pathology', 'journal': 'Nature Digital Medicine', 'year': '2024', 'relevance': 0.84}\n",
    "            ],\n",
    "            'brain computer interface': [\n",
    "                {'title': 'BCI for Motor Rehabilitation', 'journal': 'Nature Neuroscience', 'year': '2024', 'relevance': 0.93},\n",
    "                {'title': 'Neural Prosthetics Advances', 'journal': 'Science Robotics', 'year': '2023', 'relevance': 0.88},\n",
    "                {'title': 'BCI Signal Processing Methods', 'journal': 'NeuroImage', 'year': '2024', 'relevance': 0.85}\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        for query in queries:\n",
    "            if query in simulated_results:\n",
    "                results = simulated_results[query][:max_results_per_query]\n",
    "                for result in results:\n",
    "                    result['query'] = query\n",
    "                    result['pmid'] = f\"sim_{len(self.search_results) + 1:06d}\"\n",
    "                self.search_results.extend(results)\n",
    "                print(f\"  {query}: {len(results)} articles found\")\n",
    "        \n",
    "        print(f\"Total articles found: {len(self.search_results)}\")\n",
    "        return self.search_results\n",
    "    \n",
    "    def filter_by_relevance(self, min_relevance=0.8):\n",
    "        \"\"\"Filter articles by relevance score.\"\"\"\n",
    "        filtered_results = [article for article in self.search_results if article['relevance'] >= min_relevance]\n",
    "        print(f\"Filtered {len(self.search_results)} articles to {len(filtered_results)} (relevance >= {min_relevance})\")\n",
    "        return filtered_results\n",
    "    \n",
    "    def analyze_trends(self):\n",
    "        \"\"\"Analyze trends in the literature.\"\"\"\n",
    "        if not self.search_results:\n",
    "            return {}\n",
    "        \n",
    "        df = pd.DataFrame(self.search_results)\n",
    "        \n",
    "        analysis = {\n",
    "            'total_articles': len(df),\n",
    "            'year_distribution': df['year'].value_counts().to_dict(),\n",
    "            'journal_distribution': df['journal'].value_counts().to_dict(),\n",
    "            'query_distribution': df['query'].value_counts().to_dict(),\n",
    "            'average_relevance': df['relevance'].mean(),\n",
    "            'high_relevance_count': len(df[df['relevance'] >= 0.9]),\n",
    "            'top_journals': df['journal'].value_counts().head(5).to_dict(),\n",
    "            'recent_articles': len(df[df['year'] == '2024'])\n",
    "        }\n",
    "        \n",
    "        self.analysis_results = analysis\n",
    "        return analysis\n",
    "    \n",
    "    def generate_report(self):\n",
    "        \"\"\"Generate a comprehensive literature review report.\"\"\"\n",
    "        if not self.analysis_results:\n",
    "            self.analyze_trends()\n",
    "        \n",
    "        report = f\"\"\"\n",
    "# Literature Review Report\n",
    "\n",
    "## Summary\n",
    "- Total articles analyzed: {self.analysis_results['total_articles']}\n",
    "- Average relevance score: {self.analysis_results['average_relevance']:.3f}\n",
    "- High relevance articles (≥0.9): {self.analysis_results['high_relevance_count']}\n",
    "- Recent articles (2024): {self.analysis_results['recent_articles']}\n",
    "\n",
    "## Publication Year Distribution\n",
    "\"\"\"\n",
    "        for year, count in sorted(self.analysis_results['year_distribution'].items()):\n",
    "            report += f\"- {year}: {count} articles\\n\"\n",
    "        \n",
    "        report += \"\\n## Top Journals\\n\"\n",
    "        for journal, count in self.analysis_results['top_journals'].items():\n",
    "            report += f\"- {journal}: {count} articles\\n\"\n",
    "        \n",
    "        report += \"\\n## Search Query Results\\n\"\n",
    "        for query, count in self.analysis_results['query_distribution'].items():\n",
    "            report += f\"- {query}: {count} articles\\n\"\n",
    "        \n",
    "        # Save report\n",
    "        report_file = self.output_dir / 'literature_review_report.md'\n",
    "        with open(report_file, 'w') as f:\n",
    "            f.write(report)\n",
    "        \n",
    "        return report, report_file\n",
    "    \n",
    "    def export_bibliography(self):\n",
    "        \"\"\"Export bibliography in multiple formats.\"\"\"\n",
    "        # Export as CSV\n",
    "        df = pd.DataFrame(self.search_results)\n",
    "        csv_file = self.output_dir / 'bibliography.csv'\n",
    "        df.to_csv(csv_file, index=False)\n",
    "        \n",
    "        # Export as BibTeX (simplified)\n",
    "        bibtex_entries = []\n",
    "        for article in self.search_results:\n",
    "            entry = f\"\"\"@article{{{article['pmid']},\n",
    "    title = {{{article['title']}}},\n",
    "    journal = {{{article['journal']}}},\n",
    "    year = {{{article['year']}}},\n",
    "    pmid = {{{article['pmid']}}},\n",
    "    relevance = {{{article['relevance']}}}\n",
    "}}\"\"\"\n",
    "            bibtex_entries.append(entry)\n",
    "        \n",
    "        bibtex_file = self.output_dir / 'bibliography.bib'\n",
    "        with open(bibtex_file, 'w') as f:\n",
    "            f.write('\\n\\n'.join(bibtex_entries))\n",
    "        \n",
    "        return csv_file, bibtex_file\n",
    "\n",
    "# Run literature review pipeline\n",
    "print(\"Running Automated Literature Review Pipeline\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Initialize pipeline\n",
    "pipeline = LiteratureReviewPipeline(web_output / 'literature_review')\n",
    "\n",
    "# Define search queries\n",
    "research_queries = [\n",
    "    'machine learning epilepsy',\n",
    "    'AI medical diagnosis',\n",
    "    'brain computer interface'\n",
    "]\n",
    "\n",
    "# Step 1: Search literature\n",
    "search_results = pipeline.search_literature(research_queries, max_results_per_query=10)\n",
    "\n",
    "# Step 2: Filter by relevance\n",
    "high_relevance_articles = pipeline.filter_by_relevance(min_relevance=0.85)\n",
    "\n",
    "# Step 3: Analyze trends\n",
    "print(\"\\nAnalyzing literature trends...\")\n",
    "analysis = pipeline.analyze_trends()\n",
    "\n",
    "print(f\"\\nAnalysis Results:\")\n",
    "print(f\"- Total articles: {analysis['total_articles']}\")\n",
    "print(f\"- Average relevance: {analysis['average_relevance']:.3f}\")\n",
    "print(f\"- High relevance articles: {analysis['high_relevance_count']}\")\n",
    "print(f\"- Recent articles (2024): {analysis['recent_articles']}\")\n",
    "\n",
    "# Step 4: Generate report\n",
    "print(\"\\nGenerating literature review report...\")\n",
    "report, report_file = pipeline.generate_report()\n",
    "print(f\"Report saved to: {report_file}\")\n",
    "\n",
    "# Step 5: Export bibliography\n",
    "csv_file, bibtex_file = pipeline.export_bibliography()\n",
    "print(f\"Bibliography exported to:\")\n",
    "print(f\"  CSV: {csv_file}\")\n",
    "print(f\"  BibTeX: {bibtex_file}\")\n",
    "\n",
    "# Display part of the report\n",
    "print(\"\\nGenerated Report Preview:\")\n",
    "print(report[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Research Data Collection Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comprehensive research data collection workflow\n",
    "class ResearchDataCollector:\n",
    "    def __init__(self, project_name, output_dir):\n",
    "        self.project_name = project_name\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.output_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Initialize data storage\n",
    "        self.collected_data = scitex.dict.listed_dict([\n",
    "            'source_url', 'content_type', 'title', 'content', \n",
    "            'collection_date', 'word_count', 'relevance_score'\n",
    "        ])\n",
    "        \n",
    "        self.metadata = {\n",
    "            'project_name': project_name,\n",
    "            'start_time': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'sources_processed': 0,\n",
    "            'total_content_collected': 0\n",
    "        }\n",
    "    \n",
    "    def collect_from_sources(self, sources):\n",
    "        \"\"\"Collect data from multiple web sources.\"\"\"\n",
    "        print(f\"Collecting data for project: {self.project_name}\")\n",
    "        \n",
    "        for source in sources:\n",
    "            print(f\"Processing source: {source['name']}\")\n",
    "            \n",
    "            # Simulate data collection from different source types\n",
    "            if source['type'] == 'research_database':\n",
    "                self._collect_from_database(source)\n",
    "            elif source['type'] == 'journal_website':\n",
    "                self._collect_from_journal(source)\n",
    "            elif source['type'] == 'conference_proceedings':\n",
    "                self._collect_from_conference(source)\n",
    "            \n",
    "            self.metadata['sources_processed'] += 1\n",
    "            time.sleep(0.1)  # Simulate processing time\n",
    "        \n",
    "        self.metadata['total_content_collected'] = len(self.collected_data['title'])\n",
    "        print(f\"Data collection completed. Total items: {self.metadata['total_content_collected']}\")\n",
    "    \n",
    "    def _collect_from_database(self, source):\n",
    "        \"\"\"Simulate collecting from research database.\"\"\"\n",
    "        # Simulate database entries\n",
    "        entries = [\n",
    "            {\n",
    "                'title': 'Machine Learning in Neuroimaging: A Review',\n",
    "                'content': 'Comprehensive review of ML applications in neuroimaging covering 200+ studies...',\n",
    "                'relevance': 0.92\n",
    "            },\n",
    "            {\n",
    "                'title': 'Deep Learning for Medical Image Segmentation',\n",
    "                'content': 'Novel approaches using CNN architectures for precise medical image segmentation...',\n",
    "                'relevance': 0.88\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        for entry in entries:\n",
    "            self._add_collected_item(source['url'], 'database_entry', entry)\n",
    "    \n",
    "    def _collect_from_journal(self, source):\n",
    "        \"\"\"Simulate collecting from journal website.\"\"\"\n",
    "        articles = [\n",
    "            {\n",
    "                'title': 'AI-Assisted Diagnosis in Emergency Medicine',\n",
    "                'content': 'Study of AI tools reducing diagnostic time by 40% in emergency departments...',\n",
    "                'relevance': 0.94\n",
    "            },\n",
    "            {\n",
    "                'title': 'Ethical Considerations in Medical AI',\n",
    "                'content': 'Analysis of ethical frameworks for implementing AI in clinical practice...',\n",
    "                'relevance': 0.86\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        for article in articles:\n",
    "            self._add_collected_item(source['url'], 'journal_article', article)\n",
    "    \n",
    "    def _collect_from_conference(self, source):\n",
    "        \"\"\"Simulate collecting from conference proceedings.\"\"\"\n",
    "        papers = [\n",
    "            {\n",
    "                'title': 'Real-time Seizure Detection Using Edge Computing',\n",
    "                'content': 'Implementation of lightweight models for real-time seizure detection on mobile devices...',\n",
    "                'relevance': 0.91\n",
    "            },\n",
    "            {\n",
    "                'title': 'Transfer Learning for Medical Image Classification',\n",
    "                'content': 'Evaluation of transfer learning approaches for medical image classification tasks...',\n",
    "                'relevance': 0.87\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        for paper in papers:\n",
    "            self._add_collected_item(source['url'], 'conference_paper', paper)\n",
    "    \n",
    "    def _add_collected_item(self, source_url, content_type, item):\n",
    "        \"\"\"Add an item to the collected data.\"\"\"\n",
    "        self.collected_data['source_url'].append(source_url)\n",
    "        self.collected_data['content_type'].append(content_type)\n",
    "        self.collected_data['title'].append(item['title'])\n",
    "        self.collected_data['content'].append(item['content'])\n",
    "        self.collected_data['collection_date'].append(time.strftime('%Y-%m-%d'))\n",
    "        self.collected_data['word_count'].append(len(item['content'].split()))\n",
    "        self.collected_data['relevance_score'].append(item['relevance'])\n",
    "    \n",
    "    def analyze_collected_data(self):\n",
    "        \"\"\"Analyze the collected data.\"\"\"\n",
    "        if not self.collected_data['title']:\n",
    "            return {}\n",
    "        \n",
    "        df = pd.DataFrame(dict(self.collected_data))\n",
    "        \n",
    "        analysis = {\n",
    "            'total_items': len(df),\n",
    "            'content_types': df['content_type'].value_counts().to_dict(),\n",
    "            'average_relevance': df['relevance_score'].mean(),\n",
    "            'total_words': df['word_count'].sum(),\n",
    "            'average_words_per_item': df['word_count'].mean(),\n",
    "            'high_relevance_items': len(df[df['relevance_score'] >= 0.9]),\n",
    "            'collection_dates': df['collection_date'].value_counts().to_dict()\n",
    "        }\n",
    "        \n",
    "        return analysis\n",
    "    \n",
    "    def export_data(self):\n",
    "        \"\"\"Export collected data in multiple formats.\"\"\"\n",
    "        if not self.collected_data['title']:\n",
    "            print(\"No data to export\")\n",
    "            return\n",
    "        \n",
    "        # Export as CSV\n",
    "        df = pd.DataFrame(dict(self.collected_data))\n",
    "        csv_file = self.output_dir / f'{self.project_name}_collected_data.csv'\n",
    "        df.to_csv(csv_file, index=False)\n",
    "        \n",
    "        # Export as JSON\n",
    "        json_file = self.output_dir / f'{self.project_name}_collected_data.json'\n",
    "        with open(json_file, 'w') as f:\n",
    "            json.dump(dict(self.collected_data), f, indent=2)\n",
    "        \n",
    "        # Export metadata\n",
    "        metadata_file = self.output_dir / f'{self.project_name}_metadata.json'\n",
    "        with open(metadata_file, 'w') as f:\n",
    "            json.dump(self.metadata, f, indent=2)\n",
    "        \n",
    "        return csv_file, json_file, metadata_file\n",
    "    \n",
    "    def generate_summary_report(self):\n",
    "        \"\"\"Generate a summary report of the data collection.\"\"\"\n",
    "        analysis = self.analyze_collected_data()\n",
    "        \n",
    "        if not analysis:\n",
    "            return \"No data collected to report.\"\n",
    "        \n",
    "        report = f\"\"\"\n",
    "# Data Collection Summary Report\n",
    "## Project: {self.project_name}\n",
    "\n",
    "### Collection Overview\n",
    "- Collection started: {self.metadata['start_time']}\n",
    "- Sources processed: {self.metadata['sources_processed']}\n",
    "- Total items collected: {analysis['total_items']}\n",
    "- Total words collected: {analysis['total_words']:,}\n",
    "- Average words per item: {analysis['average_words_per_item']:.1f}\n",
    "- Average relevance score: {analysis['average_relevance']:.3f}\n",
    "- High relevance items (≥0.9): {analysis['high_relevance_items']}\n",
    "\n",
    "### Content Type Distribution\n",
    "\"\"\"\n",
    "        for content_type, count in analysis['content_types'].items():\n",
    "            report += f\"- {content_type}: {count} items\\n\"\n",
    "        \n",
    "        return report\n",
    "\n",
    "# Run research data collection workflow\n",
    "print(\"Running Research Data Collection Workflow\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Initialize data collector\n",
    "collector = ResearchDataCollector(\n",
    "    project_name=\"AI_Medical_Research\",\n",
    "    output_dir=web_output / 'data_collection'\n",
    ")\n",
    "\n",
    "# Define data sources\n",
    "research_sources = [\n",
    "    {\n",
    "        'name': 'PubMed Database',\n",
    "        'type': 'research_database',\n",
    "        'url': 'https://pubmed.ncbi.nlm.nih.gov/'\n",
    "    },\n",
    "    {\n",
    "        'name': 'Nature Medicine',\n",
    "        'type': 'journal_website',\n",
    "        'url': 'https://nature.com/nm/'\n",
    "    },\n",
    "    {\n",
    "        'name': 'MICCAI 2024 Proceedings',\n",
    "        'type': 'conference_proceedings',\n",
    "        'url': 'https://miccai2024.org/proceedings/'\n",
    "    }\n",
    "]\n",
    "\n",
    "# Collect data from sources\n",
    "collector.collect_from_sources(research_sources)\n",
    "\n",
    "# Analyze collected data\n",
    "print(\"\\nAnalyzing collected data...\")\n",
    "analysis = collector.analyze_collected_data()\n",
    "\n",
    "print(f\"Analysis Results:\")\n",
    "print(f\"- Total items: {analysis['total_items']}\")\n",
    "print(f\"- Average relevance: {analysis['average_relevance']:.3f}\")\n",
    "print(f\"- Total words: {analysis['total_words']:,}\")\n",
    "print(f\"- High relevance items: {analysis['high_relevance_items']}\")\n",
    "\n",
    "# Export data\n",
    "print(\"\\nExporting collected data...\")\n",
    "csv_file, json_file, metadata_file = collector.export_data()\n",
    "print(f\"Data exported to:\")\n",
    "print(f\"  CSV: {csv_file}\")\n",
    "print(f\"  JSON: {json_file}\")\n",
    "print(f\"  Metadata: {metadata_file}\")\n",
    "\n",
    "# Generate summary report\n",
    "summary_report = collector.generate_summary_report()\n",
    "print(\"\\nSummary Report:\")\n",
    "print(summary_report)\n",
    "\n",
    "# Save summary report\n",
    "report_file = collector.output_dir / f'{collector.project_name}_summary_report.md'\n",
    "with open(report_file, 'w') as f:\n",
    "    f.write(summary_report)\n",
    "print(f\"\\nSummary report saved to: {report_file}\")\n",
    "\n",
    "# Visualize collection results\n",
    "df = pd.DataFrame(dict(collector.collected_data))\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "# Content type distribution\n",
    "content_type_counts = df['content_type'].value_counts()\n",
    "content_type_counts.plot(kind='pie', ax=axes[0, 0], autopct='%1.1f%%')\n",
    "axes[0, 0].set_title('Content Type Distribution')\n",
    "axes[0, 0].set_ylabel('')\n",
    "\n",
    "# Relevance score distribution\n",
    "df['relevance_score'].hist(bins=10, ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Relevance Score Distribution')\n",
    "axes[0, 1].set_xlabel('Relevance Score')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "\n",
    "# Word count distribution\n",
    "df['word_count'].hist(bins=10, ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Word Count Distribution')\n",
    "axes[1, 0].set_xlabel('Words per Item')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "\n",
    "# Relevance vs Word Count\n",
    "axes[1, 1].scatter(df['word_count'], df['relevance_score'], alpha=0.6)\n",
    "axes[1, 1].set_title('Relevance vs Word Count')\n",
    "axes[1, 1].set_xlabel('Word Count')\n",
    "axes[1, 1].set_ylabel('Relevance Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Best Practices\n",
    "\n",
    "This tutorial demonstrated the comprehensive web operations capabilities of the SciTeX library:\n",
    "\n",
    "### Key Features Covered:\n",
    "\n",
    "1. **Academic Literature Search**:\n",
    "   - PubMed API integration for scientific literature search\n",
    "   - Article metadata retrieval and processing\n",
    "   - BibTeX citation generation for reference management\n",
    "   - Literature trend analysis and visualization\n",
    "\n",
    "2. **Web Content Processing**:\n",
    "   - URL content extraction and summarization\n",
    "   - Main content identification from HTML\n",
    "   - Web crawling for systematic data collection\n",
    "   - Content categorization and analysis\n",
    "\n",
    "3. **Research Workflow Integration**:\n",
    "   - Automated literature review pipelines\n",
    "   - Multi-source data collection workflows\n",
    "   - Research data organization and export\n",
    "   - Comprehensive reporting and visualization\n",
    "\n",
    "### Best Practices:\n",
    "\n",
    "- **Respect Rate Limits**: Always implement appropriate delays when making multiple web requests\n",
    "- **Handle Errors Gracefully**: Implement proper error handling for network requests and API calls\n",
    "- **Cache Results**: Store retrieved data locally to avoid redundant requests\n",
    "- **Validate Data Quality**: Check relevance scores and content quality before including in analysis\n",
    "- **Organize Output**: Use structured directories and standardized file formats for data organization\n",
    "- **Document Sources**: Always track the source and collection date of web-scraped data\n",
    "- **Ethical Considerations**: Respect robots.txt, terms of service, and data usage policies\n",
    "\n",
    "### Research Applications:\n",
    "\n",
    "- **Literature Reviews**: Automate the collection and analysis of research papers\n",
    "- **Trend Analysis**: Identify emerging topics and research directions\n",
    "- **Reference Management**: Generate properly formatted citations and bibliographies\n",
    "- **Data Mining**: Extract insights from web-based research content\n",
    "- **Collaborative Research**: Share standardized data collections across research teams\n",
    "\n",
    "The SciTeX web module provides powerful tools for modern research workflows, enabling researchers to efficiently collect, process, and analyze web-based scientific content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"SciTeX Web Operations Tutorial Complete!\")\n",
    "print(\"\\nFiles created in this tutorial:\")\n",
    "for file_path in web_output.rglob('*'):\n",
    "    if file_path.is_file():\n",
    "        file_size = file_path.stat().st_size\n",
    "        print(f\"  {file_path.relative_to(web_output)}: {file_size} bytes\")\n",
    "\n",
    "print(\"\\nKey takeaways:\")\n",
    "print(\"1. SciTeX web module enables efficient academic literature search\")\n",
    "print(\"2. Automated content extraction and summarization save research time\")\n",
    "print(\"3. Structured data collection workflows ensure reproducible research\")\n",
    "print(\"4. Integration with analysis tools provides comprehensive research insights\")\n",
    "print(\"5. Proper data organization and export facilitate collaboration\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",\n",
   "language": "python",\n",
   "name": "python3"\n  },\n  "language_info": {\n   "codemirror_mode": {\n    "name": "ipython",\n    "version": 3\n   },\n   "file_extension": ".py",\n   "mimetype": "text/x-python",\n   "name": "python",\n   "nbconvert_exporter": "python",\n   "pygments_lexer": "ipython3",\n   "version": "3.8.5"\n  }\n },\n "nbformat": 4,\n "nbformat_minor": 4\n}