{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SciTeX Pandas Utilities (pd) Module\n",
    "\n",
    "This notebook demonstrates the powerful pandas utilities provided by the SciTeX `pd` module. These utilities extend pandas functionality with scientific computing-focused operations:\n",
    "\n",
    "- **Data Transformation**: Reshape and restructure DataFrames\n",
    "- **Statistical Operations**: Find p-values, indicators, and statistical measures\n",
    "- **Data Cleaning**: Handle missing values and data types\n",
    "- **Column Management**: Reorder, merge, and manipulate columns\n",
    "- **Scientific Workflows**: Convert between different data formats\n",
    "\n",
    "## Installation and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scitex as stx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# Configure SciTeX for this notebook\n",
    "stx.repro.fix_seeds(42)\n",
    "print(\"SciTeX Pandas Utilities (pd) Module Demonstration\")\n",
    "print(f\"SciTeX version: {stx.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Creating Sample Scientific Data\n",
    "\n",
    "Let's create realistic scientific datasets to demonstrate the pd utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample experimental data\n",
    "np.random.seed(42)\n",
    "n_subjects = 100\n",
    "n_conditions = 3\n",
    "\n",
    "# Create experimental dataset\n",
    "experimental_data = pd.DataFrame({\n",
    "    'subject_id': range(1, n_subjects + 1),\n",
    "    'age': np.random.normal(30, 8, n_subjects).astype(int),\n",
    "    'gender': np.random.choice(['M', 'F'], n_subjects),\n",
    "    'condition_A': np.random.normal(50, 10, n_subjects),\n",
    "    'condition_B': np.random.normal(55, 12, n_subjects), \n",
    "    'condition_C': np.random.normal(48, 9, n_subjects),\n",
    "    'p_value_A': np.random.uniform(0, 0.1, n_subjects),\n",
    "    'p_value_B': np.random.uniform(0, 0.08, n_subjects),\n",
    "    'significance_A': np.random.choice([True, False], n_subjects, p=[0.3, 0.7]),\n",
    "    'significance_B': np.random.choice([True, False], n_subjects, p=[0.25, 0.75])\n",
    "})\n",
    "\n",
    "# Add some missing values for demonstration\n",
    "experimental_data.loc[np.random.choice(n_subjects, 10, replace=False), 'condition_C'] = np.nan\n",
    "\n",
    "print(\"Sample Experimental Dataset:\")\n",
    "print(experimental_data.head(10))\n",
    "print(f\"\\nDataset shape: {experimental_data.shape}\")\n",
    "print(f\"Missing values: {experimental_data.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Statistical Finding Utilities\n",
    "\n",
    "Find statistical indicators and p-values in your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find statistical indicators in the data\n",
    "print(\"=== Finding Statistical Indicators ===\")\n",
    "\n",
    "# Find rows with significant results\n",
    "significant_A = stx.pd.find_indi(experimental_data, 'significance_A', True)\n",
    "significant_B = stx.pd.find_indi(experimental_data, 'significance_B', True)\n",
    "\n",
    "print(f\"Subjects with significant condition A: {len(significant_A)}\")\n",
    "print(f\"Subjects with significant condition B: {len(significant_B)}\")\n",
    "\n",
    "# Find subjects with both conditions significant\n",
    "both_significant = stx.pd.find_indi(\n",
    "    experimental_data, \n",
    "    ['significance_A', 'significance_B'], \n",
    "    [True, True]\n",
    ")\n",
    "print(f\"Subjects with both conditions significant: {len(both_significant)}\")\n",
    "\n",
    "# Display some significant subjects\n",
    "print(\"\\nFirst 5 subjects with significant condition A:\")\n",
    "print(experimental_data.iloc[significant_A[:5]][['subject_id', 'condition_A', 'p_value_A', 'significance_A']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find p-values and analyze statistical significance\n",
    "print(\"=== P-value Analysis ===\")\n",
    "\n",
    "# Find subjects with p-values below specific thresholds\n",
    "low_pval_A = stx.pd.find_pval(experimental_data, 'p_value_A', threshold=0.05)\n",
    "very_low_pval_A = stx.pd.find_pval(experimental_data, 'p_value_A', threshold=0.01)\n",
    "\n",
    "print(f\"Subjects with p_value_A < 0.05: {len(low_pval_A)}\")\n",
    "print(f\"Subjects with p_value_A < 0.01: {len(very_low_pval_A)}\")\n",
    "\n",
    "# Analyze p-value distribution\n",
    "p_value_stats = {\n",
    "    'p_value_A': {\n",
    "        'mean': experimental_data['p_value_A'].mean(),\n",
    "        'median': experimental_data['p_value_A'].median(),\n",
    "        'below_0.05': (experimental_data['p_value_A'] < 0.05).sum(),\n",
    "        'below_0.01': (experimental_data['p_value_A'] < 0.01).sum()\n",
    "    },\n",
    "    'p_value_B': {\n",
    "        'mean': experimental_data['p_value_B'].mean(),\n",
    "        'median': experimental_data['p_value_B'].median(),\n",
    "        'below_0.05': (experimental_data['p_value_B'] < 0.05).sum(),\n",
    "        'below_0.01': (experimental_data['p_value_B'] < 0.01).sum()\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"\\nP-value Statistics:\")\n",
    "for col, stats in p_value_stats.items():\n",
    "    print(f\"\\n{col}:\")\n",
    "    for stat, value in stats.items():\n",
    "        print(f\"  {stat}: {value:.4f}\" if isinstance(value, float) else f\"  {stat}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Type and Format Conversion\n",
    "\n",
    "Convert data types and ensure proper DataFrame formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate data type conversion and DataFrame forcing\n",
    "print(\"=== Data Type Conversion ===\")\n",
    "\n",
    "# Create mixed data for demonstration\n",
    "mixed_data = {\n",
    "    'numeric_strings': ['1.5', '2.7', '3.9', 'invalid', '5.1'],\n",
    "    'integer_strings': ['10', '20', '30', 'bad', '50'],\n",
    "    'mixed_values': [1, '2.5', 3.7, 'text', 5]\n",
    "}\n",
    "mixed_df = pd.DataFrame(mixed_data)\n",
    "\n",
    "print(\"Original mixed data:\")\n",
    "print(mixed_df)\n",
    "print(f\"\\nData types:\\n{mixed_df.dtypes}\")\n",
    "\n",
    "# Convert to numeric with error handling\n",
    "numeric_df = mixed_df.copy()\n",
    "numeric_df['numeric_strings'] = stx.pd.to_numeric(numeric_df['numeric_strings'], errors='coerce')\n",
    "numeric_df['integer_strings'] = stx.pd.to_numeric(numeric_df['integer_strings'], errors='coerce')\n",
    "numeric_df['mixed_values'] = stx.pd.to_numeric(numeric_df['mixed_values'], errors='coerce')\n",
    "\n",
    "print(\"\\nAfter numeric conversion:\")\n",
    "print(numeric_df)\n",
    "print(f\"\\nNew data types:\\n{numeric_df.dtypes}\")\n",
    "\n",
    "# Force various objects to DataFrame format\n",
    "list_data = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n",
    "array_data = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "dict_data = {'A': [1, 2, 3], 'B': [4, 5, 6]}\n",
    "\n",
    "print(\"\\n=== DataFrame Forcing ===\")\n",
    "print(f\"List to DataFrame: {stx.pd.force_df(list_data).shape}\")\n",
    "print(f\"Array to DataFrame: {stx.pd.force_df(array_data).shape}\")\n",
    "print(f\"Dict to DataFrame: {stx.pd.force_df(dict_data).shape}\")\n",
    "\n",
    "# Show forced DataFrame from array\n",
    "print(\"\\nArray converted to DataFrame:\")\n",
    "print(stx.pd.force_df(array_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Column Management and Manipulation\n",
    "\n",
    "Reorder, merge, and manage DataFrame columns effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate column management utilities\n",
    "print(\"=== Column Management ===\")\n",
    "\n",
    "# Start with our experimental data\n",
    "df_work = experimental_data.copy()\n",
    "print(f\"Original columns: {list(df_work.columns)}\")\n",
    "\n",
    "# Move important columns to the front\n",
    "df_work = stx.pd.mv_to_first(df_work, ['subject_id', 'age', 'gender'])\n",
    "print(f\"\\nAfter moving key columns to front: {list(df_work.columns[:6])}...\")\n",
    "\n",
    "# Move statistical columns to the end\n",
    "df_work = stx.pd.mv_to_last(df_work, ['p_value_A', 'p_value_B', 'significance_A', 'significance_B'])\n",
    "print(f\"After moving stats to end: ...{list(df_work.columns[-4:])}\")\n",
    "\n",
    "# Move specific column to specific position\n",
    "df_work = stx.pd.mv(df_work, 'condition_A', 3)  # Move to position 3\n",
    "print(f\"\\nAfter moving condition_A to position 3: {list(df_work.columns[:6])}...\")\n",
    "\n",
    "print(\"\\nFinal column order:\")\n",
    "for i, col in enumerate(df_work.columns):\n",
    "    print(f\"  {i}: {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate column merging and melting\n",
    "print(\"=== Column Merging and Melting ===\")\n",
    "\n",
    "# Merge condition columns with a separator\n",
    "condition_cols = ['condition_A', 'condition_B', 'condition_C']\n",
    "df_merged = stx.pd.merge_columns(\n",
    "    experimental_data[['subject_id'] + condition_cols], \n",
    "    condition_cols, \n",
    "    'all_conditions',\n",
    "    separator='|'\n",
    ")\n",
    "\n",
    "print(\"Merged condition columns:\")\n",
    "print(df_merged[['subject_id', 'all_conditions']].head())\n",
    "\n",
    "# Melt columns for long-format analysis\n",
    "melted_df = stx.pd.melt_cols(\n",
    "    experimental_data[['subject_id', 'age'] + condition_cols],\n",
    "    id_vars=['subject_id', 'age'],\n",
    "    value_vars=condition_cols,\n",
    "    var_name='condition',\n",
    "    value_name='score'\n",
    ")\n",
    "\n",
    "print(\"\\nMelted conditions to long format:\")\n",
    "print(melted_df.head(10))\n",
    "print(f\"\\nMelted DataFrame shape: {melted_df.shape}\")\n",
    "print(f\"Unique conditions: {melted_df['condition'].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. XYZ Data Format Conversion\n",
    "\n",
    "Convert between different data representations commonly used in scientific analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate XYZ format conversions\n",
    "print(\"=== XYZ Format Conversions ===\")\n",
    "\n",
    "# Create sample data suitable for XYZ conversion\n",
    "# This represents measurements at different time points and conditions\n",
    "measurements_df = pd.DataFrame({\n",
    "    'time': np.tile([0, 1, 2, 3, 4], 20),  # 5 time points, 20 subjects\n",
    "    'subject': np.repeat(range(1, 21), 5),  # 20 subjects\n",
    "    'measurement': np.random.normal(50, 10, 100) + np.tile([0, 2, 4, 3, 1], 20)\n",
    "})\n",
    "\n",
    "print(\"Original measurements (long format):\")\n",
    "print(measurements_df.head(15))\n",
    "print(f\"Shape: {measurements_df.shape}\")\n",
    "\n",
    "# Convert to XYZ format (wide format with pivot)\n",
    "xyz_format = stx.pd.to_xyz(\n",
    "    measurements_df,\n",
    "    x_col='time',\n",
    "    y_col='subject', \n",
    "    z_col='measurement'\n",
    ")\n",
    "\n",
    "print(\"\\nConverted to XYZ format (pivot table):\")\n",
    "print(xyz_format.head())\n",
    "print(f\"XYZ shape: {xyz_format.shape}\")\n",
    "\n",
    "# Convert back from XYZ to long format\n",
    "back_to_long = stx.pd.from_xyz(\n",
    "    xyz_format,\n",
    "    x_name='time',\n",
    "    y_name='subject',\n",
    "    z_name='measurement'\n",
    ")\n",
    "\n",
    "print(\"\\nConverted back to long format:\")\n",
    "print(back_to_long.head(15))\n",
    "print(f\"Back to long shape: {back_to_long.shape}\")\n",
    "\n",
    "# Verify conversion accuracy\n",
    "original_sorted = measurements_df.sort_values(['subject', 'time']).reset_index(drop=True)\n",
    "converted_sorted = back_to_long.sort_values(['subject', 'time']).reset_index(drop=True)\n",
    "\n",
    "conversion_accurate = np.allclose(\n",
    "    original_sorted['measurement'].dropna(), \n",
    "    converted_sorted['measurement'].dropna(),\n",
    "    rtol=1e-10\n",
    ")\n",
    "print(f\"\\nConversion accuracy: {'✓ Perfect' if conversion_accurate else '✗ Error'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Cleaning and Processing\n",
    "\n",
    "Clean and process data with advanced utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate data cleaning utilities\n",
    "print(\"=== Data Cleaning and Processing ===\")\n",
    "\n",
    "# Create messy data for cleaning demonstration\n",
    "messy_data = pd.DataFrame({\n",
    "    'id': [1, 2, 3, 4, 5],\n",
    "    'values': [1.234567, 2.345678, 3.456789, 4.567890, 5.678901],\n",
    "    'text_data': ['  SAMPLE TEXT  ', 'Another  Text', '  THIRD item  ', 'fourth_ITEM', '  FINAL  '],\n",
    "    'categories': ['Group_A', 'group_b', 'GROUP_C', 'Group_A', 'group_b']\n",
    "})\n",
    "\n",
    "print(\"Original messy data:\")\n",
    "print(messy_data)\n",
    "\n",
    "# Round numerical values\n",
    "rounded_data = messy_data.copy()\n",
    "rounded_data['values'] = stx.pd.round(rounded_data['values'], decimals=2)\n",
    "\n",
    "print(\"\\nAfter rounding values to 2 decimal places:\")\n",
    "print(rounded_data)\n",
    "\n",
    "# Replace and standardize categorical data\n",
    "standardized_data = rounded_data.copy()\n",
    "standardized_data['categories'] = stx.pd.replace(\n",
    "    standardized_data['categories'],\n",
    "    {'Group_A': 'GroupA', 'group_b': 'GroupB', 'GROUP_C': 'GroupC'}\n",
    ")\n",
    "\n",
    "print(\"\\nAfter standardizing categories:\")\n",
    "print(standardized_data)\n",
    "\n",
    "# Sort data by multiple columns\n",
    "sorted_data = stx.pd.sort(\n",
    "    standardized_data, \n",
    "    by=['categories', 'values'], \n",
    "    ascending=[True, False]\n",
    ")\n",
    "\n",
    "print(\"\\nAfter sorting by categories (asc) and values (desc):\")\n",
    "print(sorted_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Advanced DataFrame Slicing\n",
    "\n",
    "Perform sophisticated DataFrame slicing operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate advanced slicing capabilities\n",
    "print(\"=== Advanced DataFrame Slicing ===\")\n",
    "\n",
    "# Use our experimental data for slicing demonstrations\n",
    "df_slice = experimental_data.copy()\n",
    "\n",
    "print(f\"Original data shape: {df_slice.shape}\")\n",
    "print(f\"Columns: {list(df_slice.columns)}\")\n",
    "\n",
    "# Slice by multiple conditions\n",
    "young_significant = stx.pd.slice(\n",
    "    df_slice,\n",
    "    conditions=[\n",
    "        ('age', '<', 35),\n",
    "        ('significance_A', '==', True),\n",
    "        ('p_value_A', '<', 0.05)\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"\\nYoung subjects with significant condition A: {len(young_significant)} subjects\")\n",
    "print(young_significant[['subject_id', 'age', 'condition_A', 'p_value_A', 'significance_A']].head())\n",
    "\n",
    "# Slice by percentile ranges\n",
    "top_performers = stx.pd.slice(\n",
    "    df_slice,\n",
    "    conditions=[\n",
    "        ('condition_A', '>=', df_slice['condition_A'].quantile(0.75)),\n",
    "        ('condition_B', '>=', df_slice['condition_B'].quantile(0.75))\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"\\nTop 25% performers in both conditions: {len(top_performers)} subjects\")\n",
    "print(f\"Average condition A score: {top_performers['condition_A'].mean():.2f}\")\n",
    "print(f\"Average condition B score: {top_performers['condition_B'].mean():.2f}\")\n",
    "\n",
    "# Complex slicing with gender and age groups\n",
    "female_middle_aged = stx.pd.slice(\n",
    "    df_slice,\n",
    "    conditions=[\n",
    "        ('gender', '==', 'F'),\n",
    "        ('age', '>=', 25),\n",
    "        ('age', '<=', 40)\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"\\nFemale subjects aged 25-40: {len(female_middle_aged)} subjects\")\n",
    "age_stats = female_middle_aged['age'].describe()\n",
    "print(f\"Age statistics: mean={age_stats['mean']:.1f}, std={age_stats['std']:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Handling Pandas Warnings\n",
    "\n",
    "Manage pandas warnings in scientific workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate warning handling\n",
    "print(\"=== Pandas Warning Management ===\")\n",
    "\n",
    "# Create a scenario that might trigger SettingWithCopyWarning\n",
    "df_warning_demo = experimental_data.copy()\n",
    "subset_df = df_warning_demo[df_warning_demo['age'] < 35]\n",
    "\n",
    "print(f\"Working with subset of {len(subset_df)} young subjects\")\n",
    "\n",
    "# Safely modify subset without warnings\n",
    "with stx.pd.ignore_SettingWithCopyWarning():\n",
    "    # This operation might normally trigger a warning\n",
    "    subset_df['age_group'] = 'young'\n",
    "    subset_df['condition_A_normalized'] = (subset_df['condition_A'] - subset_df['condition_A'].mean()) / subset_df['condition_A'].std()\n",
    "\n",
    "print(\"Successfully modified subset without warnings\")\n",
    "print(\"\\nModified subset (first 5 rows):\")\n",
    "print(subset_df[['subject_id', 'age', 'age_group', 'condition_A', 'condition_A_normalized']].head())\n",
    "\n",
    "# Alternative approach using explicit copy\n",
    "safe_subset = df_warning_demo[df_warning_demo['age'] >= 35].copy()\n",
    "safe_subset['age_group'] = 'older'\n",
    "\n",
    "print(f\"\\nSafely created older subjects subset: {len(safe_subset)} subjects\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Scientific Workflow Integration Example\n",
    "\n",
    "Demonstrate how pd utilities work together in a complete scientific analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete scientific workflow using pd utilities\n",
    "print(\"=== Complete Scientific Analysis Workflow ===\")\n",
    "\n",
    "class ExperimentalAnalysis:\n",
    "    def __init__(self, data):\n",
    "        self.raw_data = data.copy()\n",
    "        self.processed_data = None\n",
    "        self.results = {}\n",
    "        \n",
    "    def preprocess_data(self):\n",
    "        \"\"\"Clean and prepare data for analysis.\"\"\"\n",
    "        print(\"1. Preprocessing data...\")\n",
    "        \n",
    "        # Force to DataFrame and clean\n",
    "        self.processed_data = stx.pd.force_df(self.raw_data)\n",
    "        \n",
    "        # Move key columns to front for easy viewing\n",
    "        self.processed_data = stx.pd.mv_to_first(\n",
    "            self.processed_data, \n",
    "            ['subject_id', 'age', 'gender']\n",
    "        )\n",
    "        \n",
    "        # Convert numeric columns properly\n",
    "        numeric_cols = ['age', 'condition_A', 'condition_B', 'condition_C', 'p_value_A', 'p_value_B']\n",
    "        for col in numeric_cols:\n",
    "            if col in self.processed_data.columns:\n",
    "                self.processed_data[col] = stx.pd.to_numeric(self.processed_data[col], errors='coerce')\n",
    "        \n",
    "        # Round p-values to reasonable precision\n",
    "        self.processed_data['p_value_A'] = stx.pd.round(self.processed_data['p_value_A'], 4)\n",
    "        self.processed_data['p_value_B'] = stx.pd.round(self.processed_data['p_value_B'], 4)\n",
    "        \n",
    "        print(f\"   Processed {len(self.processed_data)} subjects\")\n",
    "        \n",
    "    def identify_significant_effects(self):\n",
    "        \"\"\"Find statistically significant effects.\"\"\"\n",
    "        print(\"2. Identifying significant effects...\")\n",
    "        \n",
    "        # Find subjects with significant results\n",
    "        sig_a_indices = stx.pd.find_pval(self.processed_data, 'p_value_A', threshold=0.05)\n",
    "        sig_b_indices = stx.pd.find_pval(self.processed_data, 'p_value_B', threshold=0.05)\n",
    "        \n",
    "        self.results['significant_A'] = len(sig_a_indices)\n",
    "        self.results['significant_B'] = len(sig_b_indices)\n",
    "        \n",
    "        # Find subjects significant in both conditions\n",
    "        both_sig_indices = stx.pd.find_indi(\n",
    "            self.processed_data,\n",
    "            ['significance_A', 'significance_B'],\n",
    "            [True, True]\n",
    "        )\n",
    "        self.results['both_significant'] = len(both_sig_indices)\n",
    "        \n",
    "        print(f\"   Condition A significant: {self.results['significant_A']} subjects\")\n",
    "        print(f\"   Condition B significant: {self.results['significant_B']} subjects\")\n",
    "        print(f\"   Both conditions significant: {self.results['both_significant']} subjects\")\n",
    "        \n",
    "    def analyze_by_demographics(self):\n",
    "        \"\"\"Analyze results by demographic groups.\"\"\"\n",
    "        print(\"3. Analyzing by demographics...\")\n",
    "        \n",
    "        # Slice by gender\n",
    "        male_data = stx.pd.slice(self.processed_data, [('gender', '==', 'M')])\n",
    "        female_data = stx.pd.slice(self.processed_data, [('gender', '==', 'F')])\n",
    "        \n",
    "        # Slice by age groups\n",
    "        young_data = stx.pd.slice(self.processed_data, [('age', '<', 30)])\n",
    "        old_data = stx.pd.slice(self.processed_data, [('age', '>=', 30)])\n",
    "        \n",
    "        self.results['demographics'] = {\n",
    "            'male_subjects': len(male_data),\n",
    "            'female_subjects': len(female_data),\n",
    "            'young_subjects': len(young_data),\n",
    "            'older_subjects': len(old_data),\n",
    "            'male_condition_A_mean': male_data['condition_A'].mean(),\n",
    "            'female_condition_A_mean': female_data['condition_A'].mean(),\n",
    "            'young_condition_A_mean': young_data['condition_A'].mean(),\n",
    "            'older_condition_A_mean': old_data['condition_A'].mean()\n",
    "        }\n",
    "        \n",
    "        print(f\"   Male subjects: {self.results['demographics']['male_subjects']}\")\n",
    "        print(f\"   Female subjects: {self.results['demographics']['female_subjects']}\")\n",
    "        print(f\"   Young (<30) subjects: {self.results['demographics']['young_subjects']}\")\n",
    "        print(f\"   Older (≥30) subjects: {self.results['demographics']['older_subjects']}\")\n",
    "        \n",
    "    def create_analysis_summary(self):\n",
    "        \"\"\"Create summary of analysis.\"\"\"\n",
    "        print(\"4. Creating analysis summary...\")\n",
    "        \n",
    "        # Melt condition data for summary analysis\n",
    "        condition_cols = ['condition_A', 'condition_B', 'condition_C']\n",
    "        melted_conditions = stx.pd.melt_cols(\n",
    "            self.processed_data[['subject_id', 'age', 'gender'] + condition_cols],\n",
    "            id_vars=['subject_id', 'age', 'gender'],\n",
    "            value_vars=condition_cols,\n",
    "            var_name='condition',\n",
    "            value_name='score'\n",
    "        )\n",
    "        \n",
    "        # Calculate summary statistics\n",
    "        summary_stats = melted_conditions.groupby(['condition', 'gender'])['score'].agg([\n",
    "            'count', 'mean', 'std', 'min', 'max'\n",
    "        ]).round(2)\n",
    "        \n",
    "        self.results['summary_stats'] = summary_stats\n",
    "        \n",
    "        print(\"   Summary statistics by condition and gender:\")\n",
    "        print(summary_stats)\n",
    "        \n",
    "    def save_results(self):\n",
    "        \"\"\"Save analysis results.\"\"\"\n",
    "        print(\"5. Saving results...\")\n",
    "        \n",
    "        # Organize final dataset with proper column order\n",
    "        final_data = stx.pd.mv_to_last(\n",
    "            self.processed_data,\n",
    "            ['p_value_A', 'p_value_B', 'significance_A', 'significance_B']\n",
    "        )\n",
    "        \n",
    "        # Save processed data and results\n",
    "        Path('./analysis_output').mkdir(exist_ok=True)\n",
    "        \n",
    "        stx.io.save(final_data, './analysis_output/processed_data.csv', symlink_from_cwd=True)\n",
    "        stx.io.save(self.results, './analysis_output/analysis_results.json', symlink_from_cwd=True)\n",
    "        \n",
    "        print(\"   Results saved to ./analysis_output/\")\n",
    "        \n",
    "    def run_complete_analysis(self):\n",
    "        \"\"\"Execute complete analysis workflow.\"\"\"\n",
    "        print(\"Starting complete experimental analysis...\\n\")\n",
    "        \n",
    "        self.preprocess_data()\n",
    "        self.identify_significant_effects()\n",
    "        self.analyze_by_demographics()\n",
    "        self.create_analysis_summary()\n",
    "        self.save_results()\n",
    "        \n",
    "        print(\"\\n✓ Analysis completed successfully!\")\n",
    "        return self.results\n",
    "\n",
    "# Run the complete analysis\n",
    "analyzer = ExperimentalAnalysis(experimental_data)\n",
    "analysis_results = analyzer.run_complete_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary and Best Practices\n",
    "\n",
    "The SciTeX pd module provides powerful pandas extensions for scientific data analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of key pd utilities demonstrated\n",
    "summary = {\n",
    "    'Statistical Operations': [\n",
    "        'stx.pd.find_indi() - Find rows matching indicators',\n",
    "        'stx.pd.find_pval() - Find significant p-values',\n",
    "        'Statistical threshold analysis',\n",
    "        'Multiple condition filtering'\n",
    "    ],\n",
    "    'Data Transformation': [\n",
    "        'stx.pd.to_xyz() - Convert to pivot format',\n",
    "        'stx.pd.from_xyz() - Convert from pivot format', \n",
    "        'stx.pd.melt_cols() - Reshape to long format',\n",
    "        'stx.pd.force_df() - Ensure DataFrame format'\n",
    "    ],\n",
    "    'Column Management': [\n",
    "        'stx.pd.mv_to_first() - Move columns to front',\n",
    "        'stx.pd.mv_to_last() - Move columns to end',\n",
    "        'stx.pd.mv() - Move to specific position',\n",
    "        'stx.pd.merge_columns() - Combine columns'\n",
    "    ],\n",
    "    'Data Cleaning': [\n",
    "        'stx.pd.to_numeric() - Safe numeric conversion',\n",
    "        'stx.pd.round() - Precision control',\n",
    "        'stx.pd.replace() - Value replacement',\n",
    "        'stx.pd.sort() - Multi-column sorting'\n",
    "    ],\n",
    "    'Advanced Features': [\n",
    "        'stx.pd.slice() - Complex filtering',\n",
    "        'stx.pd.ignore_SettingWithCopyWarning() - Warning management',\n",
    "        'Robust error handling',\n",
    "        'Scientific workflow integration'\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"SciTeX Pandas (pd) Module - Key Utilities Summary\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "for category, utilities in summary.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    for utility in utilities:\n",
    "        print(f\"  • {utility}\")\n",
    "\n",
    "print(f\"\\n{'='*55}\")\n",
    "print(\"Best Practices:\")\n",
    "print(\"  • Use find_indi() and find_pval() for statistical filtering\")\n",
    "print(\"  • Apply to_xyz()/from_xyz() for data reshape operations\")\n",
    "print(\"  • Organize columns with mv_* functions for clarity\")\n",
    "print(\"  • Use force_df() to ensure consistent DataFrame format\")\n",
    "print(\"  • Handle warnings properly with ignore_SettingWithCopyWarning()\")\n",
    "print(\"  • Combine utilities for complete analysis workflows\")\n",
    "\n",
    "print(f\"\\nDemo completed successfully! 🎉\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up temporary files\n",
    "import shutil\n",
    "\n",
    "# Remove temporary directories\n",
    "for temp_path in ['./analysis_output']:\n",
    "    if Path(temp_path).exists():\n",
    "        shutil.rmtree(temp_path)\n",
    "        print(f\"Cleaned up: {temp_path}\")\n",
    "\n",
    "print(\"\\nNotebook cleanup completed.\")"
   ]
  }\n",
 ],\n",
 "metadata": {\n",
  "kernelspec": {\n   "display_name": "Python 3",\n   "language": "python",\n   "name": "python3"\n  },\n  "language_info": {\n   "codemirror_mode": {\n    "name": "ipython",\n    "version": 3\n   },\n   "file_extension": ".py",\n   "mimetype": "text/x-python",\n   "name": "python",\n   "nbconvert_exporter": "python",\n   "pygments_lexer": "ipython3",\n   "version": "3.8.0"\n  }\n },\n "nbformat": 4,\n "nbformat_minor": 4\n}