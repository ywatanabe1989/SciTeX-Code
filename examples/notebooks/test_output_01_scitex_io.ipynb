{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d1bdc9f",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">An Exception was encountered at '<a href=\"#papermill-error-cell\">In [9]</a>'.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884814c2",
   "metadata": {
    "papermill": {
     "duration": 0.006344,
     "end_time": "2025-07-24T19:16:04.526401",
     "exception": false,
     "start_time": "2025-07-24T19:16:04.520057",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# SciTeX I/O Operations Tutorial\n",
    "\n",
    "This comprehensive notebook demonstrates the SciTeX I/O module capabilities, combining features from basic operations, advanced functionality, and complete workflow examples.\n",
    "\n",
    "## Features Covered\n",
    "\n",
    "### Basic I/O Operations\n",
    "* Unified save/load interface with automatic format detection\n",
    "* Symlink creation and management\n",
    "* Basic file operations\n",
    "\n",
    "### Advanced I/O Features\n",
    "* Compression support (gzip, bz2, xz)\n",
    "* HDF5 operations\n",
    "* Configuration file management\n",
    "* Performance comparisons across formats\n",
    "\n",
    "### Complete Workflows\n",
    "* Caching mechanisms\n",
    "* Batch operations\n",
    "* Experiment pipeline integration\n",
    "* Real-world data processing examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "078c4530",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-24T19:16:04.537423Z",
     "iopub.status.busy": "2025-07-24T19:16:04.536569Z",
     "iopub.status.idle": "2025-07-24T19:16:04.543797Z",
     "shell.execute_reply": "2025-07-24T19:16:04.542181Z"
    },
    "papermill": {
     "duration": 0.014642,
     "end_time": "2025-07-24T19:16:04.545710",
     "exception": false,
     "start_time": "2025-07-24T19:16:04.531068",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Detect notebook name for output directory\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Get notebook name (for papermill compatibility)\n",
    "notebook_name = \"01_scitex_io\"\n",
    "if 'PAPERMILL_NOTEBOOK_NAME' in os.environ:\n",
    "    notebook_name = Path(os.environ['PAPERMILL_NOTEBOOK_NAME']).stem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acc7f28f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-24T19:16:04.560507Z",
     "iopub.status.busy": "2025-07-24T19:16:04.559630Z",
     "iopub.status.idle": "2025-07-24T19:16:04.567991Z",
     "shell.execute_reply": "2025-07-24T19:16:04.566566Z"
    },
    "papermill": {
     "duration": 0.018197,
     "end_time": "2025-07-24T19:16:04.570005",
     "exception": false,
     "start_time": "2025-07-24T19:16:04.551808",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "import scitex\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# Set up example data directory\n",
    "# Note: scitex will save files to '01_scitex_io_out/' when running in this notebook\n",
    "data_dir = Path(\"./io_examples\")\n",
    "data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# For files saved by scitex in this notebook, they will be in:\n",
    "notebook_output_dir = Path(\"01_scitex_io_out\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce2967b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-24T19:16:04.581930Z",
     "iopub.status.busy": "2025-07-24T19:16:04.581016Z",
     "iopub.status.idle": "2025-07-24T19:16:04.588765Z",
     "shell.execute_reply": "2025-07-24T19:16:04.587185Z"
    },
    "papermill": {
     "duration": 0.016346,
     "end_time": "2025-07-24T19:16:04.591028",
     "exception": false,
     "start_time": "2025-07-24T19:16:04.574682",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Path compatibility helper\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def ensure_output_dir(subdir: str, notebook_name: str = \"01_scitex_io\"):\n",
    "    \"\"\"Ensure output directory exists with backward compatibility.\"\"\"\n",
    "    expected_dir = Path(subdir)\n",
    "    actual_dir = Path(f\"{notebook_name}_out\") / subdir\n",
    "    \n",
    "    if not expected_dir.exists() and actual_dir.exists():\n",
    "        # Create symlink for backward compatibility\n",
    "        try:\n",
    "            os.symlink(str(actual_dir.resolve()), str(expected_dir))\n",
    "        except (OSError, FileExistsError):\n",
    "            pass\n",
    "    \n",
    "    return expected_dir\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb67286",
   "metadata": {
    "papermill": {
     "duration": 0.005198,
     "end_time": "2025-07-24T19:16:04.601703",
     "exception": false,
     "start_time": "2025-07-24T19:16:04.596505",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Part 1: Basic I/O Operations\n",
    "\n",
    "### 1.1 Unified Save/Load Interface\n",
    "\n",
    "SciTeX provides a unified interface that automatically detects file formats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fe5b742",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-24T19:16:04.613518Z",
     "iopub.status.busy": "2025-07-24T19:16:04.612767Z",
     "iopub.status.idle": "2025-07-24T19:16:04.623321Z",
     "shell.execute_reply": "2025-07-24T19:16:04.621790Z"
    },
    "papermill": {
     "duration": 0.018711,
     "end_time": "2025-07-24T19:16:04.625237",
     "exception": false,
     "start_time": "2025-07-24T19:16:04.606526",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Create sample data\n",
    "sample_data = {\n",
    "    'array': np.random.randn(100, 50),\n",
    "    'dataframe': pd.DataFrame({\n",
    "    'x': np.random.randn(1000),\n",
    "    'y': np.random.randn(1000),\n",
    "    'category': np.random.choice(['A', 'B', 'C'], 1000)\n",
    "    }),\n",
    "    'metadata': {\n",
    "    'experiment': 'demo',\n",
    "    'date': '2024-01-01',\n",
    "    'parameters': {'alpha': 0.05, 'beta': 0.1}\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f09cb7f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-24T19:16:04.638837Z",
     "iopub.status.busy": "2025-07-24T19:16:04.638494Z",
     "iopub.status.idle": "2025-07-24T19:16:05.006766Z",
     "shell.execute_reply": "2025-07-24T19:16:05.005823Z"
    },
    "papermill": {
     "duration": 0.378424,
     "end_time": "2025-07-24T19:16:05.008723",
     "exception": false,
     "start_time": "2025-07-24T19:16:04.630299",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Error occurred while saving: Object of type ndarray is not JSON serializable\n",
      "Debug: Initial script_path = /tmp/ipykernel_356766/2796701369.py\n",
      "Debug: Final spath = /home/ywatanabe/proj/SciTeX-Code/test_output_out/io_examples/sample_data.json\n",
      "Debug: specified_path type = <class 'str'>\n",
      "Debug: specified_path = io_examples/sample_data.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m\n",
      "Saved to: /home/ywatanabe/proj/SciTeX-Code/test_output_out/io_examples/sample_data.pkl (57.5 KiB)\u001b[0m\n",
      "\u001b[93m\n",
      "Saved to: /home/ywatanabe/proj/SciTeX-Code/test_output_out/io_examples/sample_array.npy (39.2 KiB)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m\n",
      "Saved to: /home/ywatanabe/proj/SciTeX-Code/test_output_out/io_examples/sample_dataframe.csv (40.3 KiB)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Save data in multiple formats - automatic format detection\n",
    "formats_to_test = ['pkl', 'json', 'npy', 'csv']\n",
    "\n",
    "for fmt in formats_to_test:\n",
    "    try:\n",
    "        if fmt == 'npy':\n",
    "            # For .npy, save just the array\n",
    "            scitex.io.save(sample_data['array'], data_dir / f\"sample_array.{fmt}\")\n",
    "        elif fmt == 'csv':\n",
    "            # For .csv, save just the dataframe\n",
    "            scitex.io.save(sample_data['dataframe'], data_dir / f\"sample_dataframe.{fmt}\")\n",
    "        else:\n",
    "            # For pkl and json, save the full dictionary\n",
    "            scitex.io.save(sample_data, data_dir / f\"sample_data.{fmt}\")\n",
    "    except Exception as e:        pass  # Fixed incomplete except block\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "646de706",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-24T19:16:05.022308Z",
     "iopub.status.busy": "2025-07-24T19:16:05.021648Z",
     "iopub.status.idle": "2025-07-24T19:16:05.029632Z",
     "shell.execute_reply": "2025-07-24T19:16:05.028414Z"
    },
    "papermill": {
     "duration": 0.015256,
     "end_time": "2025-07-24T19:16:05.031222",
     "exception": false,
     "start_time": "2025-07-24T19:16:05.015966",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load data back - automatic format detection\n",
    "loaded_data = {}\n",
    "\n",
    "# Define a helper function to find files in both locations\n",
    "def find_file(filename):\n",
    "    \"\"\"Check both data_dir and notebook_output_dir for a file.\"\"\"\n",
    "    paths_to_check = [\n",
    "    data_dir / filename,\n",
    "    notebook_output_dir / filename\n",
    "    ]\n",
    "    for path in paths_to_check:\n",
    "        if path.exists():\n",
    "            return path\n",
    "    return None\n",
    "\n",
    "# Load pickle data (full dictionary)\n",
    "pkl_file = find_file(\"sample_data.pkl\")\n",
    "if pkl_file:\n",
    "    loaded_data['from_pkl'] = scitex.io.load(pkl_file)\n",
    "else:\n",
    "    pass  # Fixed incomplete block\n",
    "\n",
    "# Load numpy array\n",
    "npy_file = find_file(\"sample_array.npy\")\n",
    "if npy_file:\n",
    "    loaded_data['from_npy'] = scitex.io.load(npy_file)\n",
    "else:\n",
    "    pass  # Fixed incomplete block\n",
    "\n",
    "# Load CSV dataframe\n",
    "csv_file = find_file(\"sample_dataframe.csv\")\n",
    "if csv_file:\n",
    "    loaded_data['from_csv'] = scitex.io.load(csv_file)\n",
    "else:\n",
    "    pass  # Fixed incomplete block\n",
    "\n",
    "# Show where files were found"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80cb22f",
   "metadata": {
    "papermill": {
     "duration": 0.004221,
     "end_time": "2025-07-24T19:16:05.040316",
     "exception": false,
     "start_time": "2025-07-24T19:16:05.036095",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 1.2 Symlink Creation and Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32b58dce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-24T19:16:05.051349Z",
     "iopub.status.busy": "2025-07-24T19:16:05.050158Z",
     "iopub.status.idle": "2025-07-24T19:16:05.057922Z",
     "shell.execute_reply": "2025-07-24T19:16:05.056571Z"
    },
    "papermill": {
     "duration": 0.014909,
     "end_time": "2025-07-24T19:16:05.059437",
     "exception": false,
     "start_time": "2025-07-24T19:16:05.044528",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create symlinks for easy access\n",
    "symlink_dir = data_dir / \"symlinks\"\n",
    "symlink_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Create symlinks to our saved files\n",
    "original_file = data_dir / \"sample_data.pkl\"\n",
    "if original_file.exists():\n",
    "    symlink_path = symlink_dir / \"latest_data.pkl\"\n",
    "    \n",
    "    # Remove existing symlink if it exists\n",
    "    if symlink_path.is_symlink():\n",
    "        symlink_path.unlink()\n",
    "    \n",
    "    # Create new symlink\n",
    "    symlink_path.symlink_to(original_file.resolve())\n",
    "    \n",
    "    # Verify symlink works\n",
    "    symlink_data = scitex.io.load(symlink_path)\n",
    "else:    pass  # Fixed incomplete block\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43db47b2",
   "metadata": {
    "papermill": {
     "duration": 0.004611,
     "end_time": "2025-07-24T19:16:05.069121",
     "exception": false,
     "start_time": "2025-07-24T19:16:05.064510",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Part 2: Advanced I/O Features\n",
    "\n",
    "### 2.1 Compression Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45d29b2c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-24T19:16:05.082472Z",
     "iopub.status.busy": "2025-07-24T19:16:05.081707Z",
     "iopub.status.idle": "2025-07-24T19:16:05.383929Z",
     "shell.execute_reply": "2025-07-24T19:16:05.382636Z"
    },
    "papermill": {
     "duration": 0.311486,
     "end_time": "2025-07-24T19:16:05.385336",
     "exception": false,
     "start_time": "2025-07-24T19:16:05.073850",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m\n",
      "Saved to: /home/ywatanabe/proj/SciTeX-Code/test_output_out/io_examples/large_data.pkl (3.8 MiB)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ywatanabe/proj/SciTeX-Code/src/scitex/io/_save.py:420: UserWarning: Unsupported file format. /home/ywatanabe/proj/SciTeX-Code/test_output_out/io_examples/large_data.pkl.gzip was not saved.\n",
      "  warnings.warn(f\"Unsupported file format. {spath} was not saved.\")\n",
      "/home/ywatanabe/proj/SciTeX-Code/src/scitex/io/_save.py:420: UserWarning: Unsupported file format. /home/ywatanabe/proj/SciTeX-Code/test_output_out/io_examples/large_data.pkl.bz2 was not saved.\n",
      "  warnings.warn(f\"Unsupported file format. {spath} was not saved.\")\n",
      "/home/ywatanabe/proj/SciTeX-Code/src/scitex/io/_save.py:420: UserWarning: Unsupported file format. /home/ywatanabe/proj/SciTeX-Code/test_output_out/io_examples/large_data.pkl.xz was not saved.\n",
      "  warnings.warn(f\"Unsupported file format. {spath} was not saved.\")\n",
      "/home/ywatanabe/proj/SciTeX-Code/src/scitex/io/_save.py:420: UserWarning: Unsupported file format. /home/ywatanabe/proj/SciTeX-Code/test_output_out/io_examples/large_data.pkl.lzma was not saved.\n",
      "  warnings.warn(f\"Unsupported file format. {spath} was not saved.\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Test compression formats\n",
    "\n",
    "# Create large data for compression testing\n",
    "large_data = {\n",
    "    'arrays': [np.random.randn(1000, 100) for _ in range(5)],\n",
    "    'metadata': {'size': 1000, 'features': 100, 'datasets': 5}\n",
    "}\n",
    "\n",
    "# Test different compression formats\n",
    "compression_formats = ['gzip', 'bz2', 'xz', 'lzma']\n",
    "file_sizes = {}\n",
    "\n",
    "# Save uncompressed first\n",
    "uncompressed_file = data_dir / \"large_data.pkl\"\n",
    "scitex.io.save(large_data, uncompressed_file)\n",
    "\n",
    "# Check both possible locations for the saved file\n",
    "actual_uncompressed = notebook_output_dir / \"large_data.pkl\"\n",
    "if not actual_uncompressed.exists():\n",
    "    actual_uncompressed = uncompressed_file\n",
    "    \n",
    "if actual_uncompressed.exists():\n",
    "    file_sizes[\"uncompressed\"] = actual_uncompressed.stat().st_size\n",
    "else:\n",
    "    file_sizes[\"uncompressed\"] = 0\n",
    "\n",
    "# Save with compression\n",
    "for compression in compression_formats:\n",
    "    try:\n",
    "        compressed_file = data_dir / f\"large_data.pkl.{compression}\"\n",
    "        scitex.io.save(large_data, compressed_file, compression=compression)\n",
    "        \n",
    "        # Check both possible locations\n",
    "        actual_path = notebook_output_dir / f\"large_data.pkl.{compression}\"\n",
    "        if not actual_path.exists():\n",
    "            actual_path = compressed_file\n",
    "            \n",
    "        if actual_path.exists():\n",
    "            file_sizes[compression] = actual_path.stat().st_size\n",
    "        else:\n",
    "            file_sizes[compression] = 0\n",
    "    except Exception as e:\n",
    "        file_sizes[compression] = 0\n",
    "\n",
    "# Display compression results\n",
    "for format_name, size in file_sizes.items():\n",
    "    if size > 0:\n",
    "        size_mb = size / (1024 * 1024)\n",
    "        if format_name != \"uncompressed\" and file_sizes[\"uncompressed\"] > 0:\n",
    "            compression_ratio = file_sizes[\"uncompressed\"] / size\n",
    "        else:\n",
    "            pass  # Fixed incomplete block\n",
    "    else:        pass  # Fixed incomplete block\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f443035d",
   "metadata": {
    "papermill": {
     "duration": 0.004306,
     "end_time": "2025-07-24T19:16:05.393863",
     "exception": false,
     "start_time": "2025-07-24T19:16:05.389557",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 2.2 HDF5 Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11210164",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span id=\"papermill-error-cell\" style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">Execution using papermill encountered an exception here and stopped:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0dc7090",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-24T19:16:05.404204Z",
     "iopub.status.busy": "2025-07-24T19:16:05.403245Z",
     "iopub.status.idle": "2025-07-24T19:16:05.412583Z",
     "shell.execute_reply": "2025-07-24T19:16:05.411030Z"
    },
    "papermill": {
     "duration": 0.01627,
     "end_time": "2025-07-24T19:16:05.414182",
     "exception": true,
     "start_time": "2025-07-24T19:16:05.397912",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after 'for' statement on line 33 (1296637701.py, line 36)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 36\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mexcept ImportError:\u001b[39m\n                       ^\n\u001b[31mIndentationError\u001b[39m\u001b[31m:\u001b[39m expected an indented block after 'for' statement on line 33\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# HDF5 operations for hierarchical data\n",
    "try:\n",
    "    import h5py\n",
    "    \n",
    "    # Create hierarchical data structure\n",
    "    hdf5_data = {\n",
    "    'experiment_1': {\n",
    "    'raw_data': np.random.randn(500, 100),\n",
    "    'processed_data': np.random.randn(500, 50),\n",
    "    'metadata': {\n",
    "    'sampling_rate': 1000,\n",
    "    'channels': 100\n",
    "    }\n",
    "    },\n",
    "    'experiment_2': {\n",
    "    'raw_data': np.random.randn(300, 100),\n",
    "    'processed_data': np.random.randn(300, 50),\n",
    "    'metadata': {\n",
    "    'sampling_rate': 500,\n",
    "    'channels': 100\n",
    "    }\n",
    "    }\n",
    "    }\n",
    "    \n",
    "    # Save as HDF5\n",
    "    hdf5_file = data_dir / \"experiments.h5\"\n",
    "    scitex.io.save(hdf5_data, hdf5_file)\n",
    "    \n",
    "    # Load HDF5 data\n",
    "    loaded_hdf5 = scitex.io.load(hdf5_file)\n",
    "    \n",
    "    for exp_name, exp_data in loaded_hdf5.items():\n",
    "        # Loop body\n",
    "        \n",
    "except ImportError:\n",
    "    pass  # Fixed incomplete except block\n",
    "except Exception as e:    pass  # Fixed incomplete except block\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2b7df1",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 2.3 Performance Comparison Across Formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ab538c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# Performance benchmark for different formats\n",
    "benchmark_data = {\n",
    "    'numeric_array': np.random.randn(1000, 100),\n",
    "    'dataframe': pd.DataFrame({\n",
    "    'col_' + str(i): np.random.randn(5000)\n",
    "    for i in range(20)\n",
    "    }),\n",
    "    'mixed_data': {\n",
    "    'numbers': list(range(10000)),\n",
    "    'strings': [f'item_{i}' for i in range(1000)],\n",
    "    'nested': {'a': [1, 2, 3], 'b': {'c': 4, 'd': 5}}\n",
    "    }\n",
    "}\n",
    "\n",
    "formats_to_benchmark = ['pkl', 'json', 'h5']\n",
    "benchmark_results = {}\n",
    "\n",
    "for fmt in formats_to_benchmark:\n",
    "    try:\n",
    "        test_file = data_dir / f\"benchmark.{fmt}\"\n",
    "        \n",
    "        # Time save operation\n",
    "        start_time = time.time()\n",
    "        if fmt == 'json':\n",
    "            # JSON can't handle numpy arrays directly\n",
    "            json_safe_data = {\n",
    "            'numeric_array': benchmark_data['numeric_array'].tolist(),\n",
    "            'mixed_data': benchmark_data['mixed_data']\n",
    "            }\n",
    "            scitex.io.save(json_safe_data, test_file)\n",
    "        else:\n",
    "            scitex.io.save(benchmark_data, test_file)\n",
    "        save_time = time.time() - start_time\n",
    "        \n",
    "        # Check where file was actually saved\n",
    "        actual_file = notebook_output_dir / f\"benchmark.{fmt}\"\n",
    "        if not actual_file.exists():\n",
    "            actual_file = test_file\n",
    "        \n",
    "        if actual_file.exists():\n",
    "            # Time load operation\n",
    "            start_time = time.time()\n",
    "            loaded = scitex.io.load(actual_file)\n",
    "            load_time = time.time() - start_time\n",
    "            \n",
    "            # Get file size\n",
    "            file_size = actual_file.stat().st_size / (1024 * 1024)  # MB\n",
    "            \n",
    "            benchmark_results[fmt] = {\n",
    "            'save_time': save_time,\n",
    "            'load_time': load_time,\n",
    "            'file_size_mb': file_size\n",
    "            }\n",
    "            \n",
    "        else:\n",
    "            pass  # Fixed incomplete block\n",
    "        \n",
    "    except Exception as e:\n",
    "        pass  # Fixed incomplete except block\n",
    "\n",
    "# Visualize benchmark results\n",
    "if benchmark_results:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    formats = list(benchmark_results.keys())\n",
    "    save_times = [benchmark_results[fmt]['save_time'] for fmt in formats]\n",
    "    load_times = [benchmark_results[fmt]['load_time'] for fmt in formats]\n",
    "    file_sizes = [benchmark_results[fmt]['file_size_mb'] for fmt in formats]\n",
    "    \n",
    "    axes[0].bar(formats, save_times)\n",
    "    axes[0].set_title('Save Time (seconds)')\n",
    "    axes[0].set_ylabel('Time (s)')\n",
    "    \n",
    "    axes[1].bar(formats, load_times)\n",
    "    axes[1].set_title('Load Time (seconds)')\n",
    "    axes[1].set_ylabel('Time (s)')\n",
    "    \n",
    "    axes[2].bar(formats, file_sizes)\n",
    "    axes[2].set_title('File Size (MB)')\n",
    "    axes[2].set_ylabel('Size (MB)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919a069e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Part 3: Complete Workflows and Caching\n",
    "\n",
    "### 3.1 Caching Mechanisms\n",
    "\n",
    "SciTeX provides multiple caching approaches:\n",
    "- `scitex.decorators.cache_disk` - Decorator for caching function results to disk using joblib\n",
    "- `scitex.io.cache()` - Function for caching/loading variables by name\n",
    "- `scitex.gen.cache` - In-memory caching (functools.lru_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2869eb",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Demonstrate caching for expensive operations\n",
    "cache_dir = data_dir / \"cache\"\n",
    "cache_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Use scitex.decorators.cache_disk for disk-based caching\n",
    "@scitex.decorators.cache_disk\n",
    "def expensive_computation(n_samples=10000, n_features=100):\n",
    "    \"\"\"Simulate an expensive computation that we want to cache.\"\"\"\n",
    "    time.sleep(1)  # Simulate computation time\n",
    "    \n",
    "    # Generate some \"computed\" result\n",
    "    data = np.random.randn(n_samples, n_features)\n",
    "    features = np.mean(data, axis=0)\n",
    "    correlations = np.corrcoef(data.T)\n",
    "    \n",
    "    return {\n",
    "    'raw_data': data,\n",
    "    'features': features,\n",
    "    'correlations': correlations,\n",
    "    'metadata': {\n",
    "    'n_samples': n_samples,\n",
    "    'n_features': n_features,\n",
    "    'computed_at': time.time()\n",
    "    }\n",
    "    }\n",
    "\n",
    "# First call - will compute and cache\n",
    "start_time = time.time()\n",
    "result1 = expensive_computation(5000, 50)\n",
    "first_call_time = time.time() - start_time\n",
    "\n",
    "# Second call - will load from cache\n",
    "start_time = time.time()\n",
    "result2 = expensive_computation(5000, 50)\n",
    "second_call_time = time.time() - start_time\n",
    "\n",
    "\n",
    "# Note: The results won't be identical because cache_disk saves to disk\n",
    "# and reloads, which may introduce small numerical differences\n",
    "\n",
    "# Alternative: Demonstrate scitex.io.cache() for variable caching\n",
    "# Create some expensive variables\n",
    "expensive_array = np.random.randn(1000, 1000)\n",
    "expensive_dict = {'data': expensive_array.mean(axis=0), 'stats': {'mean': expensive_array.mean(), 'std': expensive_array.std()}}\n",
    "\n",
    "# Cache these variables\n",
    "expensive_array, expensive_dict = scitex.io.cache(\"my_expensive_vars\", \"expensive_array\", \"expensive_dict\")\n",
    "\n",
    "# Now delete and reload from cache\n",
    "del expensive_array, expensive_dict\n",
    "expensive_array, expensive_dict = scitex.io.cache(\"my_expensive_vars\", \"expensive_array\", \"expensive_dict\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e98703e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 3.2 Batch Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd275fa6",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Batch file operations\n",
    "batch_dir = data_dir / \"batch_processing\"\n",
    "batch_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Create multiple data files for batch processing\n",
    "batch_files = []\n",
    "for i in range(5):\n",
    "    batch_data = {\n",
    "    'id': i,\n",
    "    'data': np.random.randn(100, 10),\n",
    "    'labels': np.random.choice(['A', 'B', 'C'], 100),\n",
    "    'timestamp': time.time() + i\n",
    "    }\n",
    "    \n",
    "    filename = batch_dir / f\"batch_data_{i:03d}.pkl\"\n",
    "    scitex.io.save(batch_data, filename)\n",
    "    batch_files.append(filename)\n",
    "\n",
    "\n",
    "# Check where files were actually saved (in notebook output directory)\n",
    "actual_batch_dir = notebook_output_dir / \"batch_processing\"\n",
    "if actual_batch_dir.exists() and not batch_dir.exists():\n",
    "    batch_dir = actual_batch_dir\n",
    "\n",
    "# Batch loading with pattern matching\n",
    "pattern = batch_dir / \"batch_data_*.pkl\"\n",
    "all_batch_files = list(batch_dir.glob(\"batch_data_*.pkl\"))\n",
    "\n",
    "# Load and combine all batch files\n",
    "combined_data = []\n",
    "if all_batch_files:\n",
    "    for file_path in sorted(all_batch_files):\n",
    "        data = scitex.io.load(file_path)\n",
    "        combined_data.append(data)\n",
    "    \n",
    "    \n",
    "    # Combine all data into single arrays\n",
    "    all_data = np.vstack([d['data'] for d in combined_data])\n",
    "    all_labels = np.hstack([d['labels'] for d in combined_data])\n",
    "    \n",
    "else:    pass  # Fixed incomplete block\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a220668",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 3.3 Experiment Pipeline Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03031992",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# Complete experiment pipeline with I/O\n",
    "class ExperimentPipeline:\n",
    "    def __init__(self, experiment_name, output_dir):\n",
    "        self.experiment_name = experiment_name\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.output_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Create subdirectories\n",
    "        (self.output_dir / \"raw\").mkdir(exist_ok=True)\n",
    "        (self.output_dir / \"processed\").mkdir(exist_ok=True)\n",
    "        (self.output_dir / \"results\").mkdir(exist_ok=True)\n",
    "        \n",
    "    def generate_data(self, n_samples=1000, noise_level=0.1):\n",
    "        \"\"\"Generate synthetic experimental data.\"\"\"\n",
    "        \n",
    "        # Simulate different experimental conditions\n",
    "        conditions = ['control', 'treatment_A', 'treatment_B']\n",
    "        raw_data = {}\n",
    "        \n",
    "        for condition in conditions:\n",
    "            # Different signal patterns for each condition\n",
    "            if condition == 'control':\n",
    "                signal = np.sin(np.linspace(0, 4*np.pi, n_samples))\n",
    "            elif condition == 'treatment_A':\n",
    "                signal = np.sin(np.linspace(0, 4*np.pi, n_samples)) * 1.5\n",
    "            else:  # treatment_B\n",
    "                signal = np.sin(np.linspace(0, 6*np.pi, n_samples)) * 0.8\n",
    "            \n",
    "            # Add noise\n",
    "            noisy_signal = signal + np.random.normal(0, noise_level, n_samples)\n",
    "            \n",
    "            raw_data[condition] = {\n",
    "                'signal': noisy_signal,\n",
    "                'time': np.linspace(0, 10, n_samples),\n",
    "                'metadata': {\n",
    "                'condition': condition,\n",
    "                'n_samples': n_samples,\n",
    "                'noise_level': noise_level\n",
    "                }\n",
    "            }\n",
    "        \n",
    "        # Save raw data\n",
    "        raw_file = self.output_dir / \"raw\" / \"raw_data.pkl\"\n",
    "        scitex.io.save(raw_data, raw_file)\n",
    "        \n",
    "        return raw_data\n",
    "    \n",
    "    def process_data(self, raw_data=None):\n",
    "        \"\"\"Process the raw experimental data.\"\"\"\n",
    "        if raw_data is None:\n",
    "            # Load from file\n",
    "            raw_file = self.output_dir / \"raw\" / \"raw_data.pkl\"\n",
    "            raw_data = scitex.io.load(raw_file)\n",
    "        \n",
    "        processed_data = {}\n",
    "        \n",
    "        for condition, data in raw_data.items():\n",
    "            signal = data['signal']\n",
    "            time = data['time']\n",
    "            \n",
    "            # Apply processing steps\n",
    "            # 1. Smoothing\n",
    "            from scipy import ndimage\n",
    "            smoothed = ndimage.gaussian_filter1d(signal, sigma=2)\n",
    "            \n",
    "            # 2. Feature extraction\n",
    "            features = {\n",
    "            'mean': np.mean(smoothed),\n",
    "            'std': np.std(smoothed),\n",
    "            'max': np.max(smoothed),\n",
    "            'min': np.min(smoothed),\n",
    "            'peak_to_peak': np.ptp(smoothed)\n",
    "            }\n",
    "            \n",
    "            # 3. Spectral analysis\n",
    "            fft = np.fft.fft(smoothed)\n",
    "            freqs = np.fft.fftfreq(len(smoothed), d=time[1]-time[0])\n",
    "            power_spectrum = np.abs(fft)**2\n",
    "            \n",
    "            processed_data[condition] = {\n",
    "            'original_signal': signal,\n",
    "            'smoothed_signal': smoothed,\n",
    "            'features': features,\n",
    "            'power_spectrum': power_spectrum[:len(power_spectrum)//2],\n",
    "            'frequencies': freqs[:len(freqs)//2],\n",
    "            'time': time,\n",
    "            'metadata': data['metadata']\n",
    "            }\n",
    "        \n",
    "        # Save processed data\n",
    "        processed_file = self.output_dir / \"processed\" / \"processed_data.pkl\"\n",
    "        scitex.io.save(processed_data, processed_file)\n",
    "        \n",
    "        return processed_data\n",
    "    \n",
    "    def analyze_results(self, processed_data=None):\n",
    "        \"\"\"Analyze processed data and generate results.\"\"\"\n",
    "        if processed_data is None:\n",
    "            processed_file = self.output_dir / \"processed\" / \"processed_data.pkl\"\n",
    "            processed_data = scitex.io.load(processed_file)\n",
    "        \n",
    "        \n",
    "        # Statistical analysis\n",
    "        results = {\n",
    "            'summary_statistics': {},\n",
    "            'comparisons': {},\n",
    "            'figures': {}\n",
    "        }\n",
    "        \n",
    "        # Extract features for all conditions\n",
    "        all_features = {}\n",
    "        for condition, data in processed_data.items():\n",
    "            all_features[condition] = data['features']\n",
    "            results['summary_statistics'][condition] = data['features']\n",
    "        \n",
    "        # Generate comparison plots\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "        fig.suptitle(f'Experiment Results: {self.experiment_name}')\n",
    "        \n",
    "        # Plot 1: Original signals\n",
    "        for condition, data in processed_data.items():\n",
    "            axes[0, 0].plot(data['time'], data['smoothed_signal'], label=condition)\n",
    "        axes[0, 0].set_title('Processed Signals')\n",
    "        axes[0, 0].set_xlabel('Time')\n",
    "        axes[0, 0].set_ylabel('Amplitude')\n",
    "        axes[0, 0].legend()\n",
    "        \n",
    "        # Plot 2: Feature comparison\n",
    "        feature_names = list(all_features['control'].keys())\n",
    "        x_pos = np.arange(len(feature_names))\n",
    "        width = 0.25\n",
    "        \n",
    "        for i, condition in enumerate(all_features.keys()):\n",
    "            values = [all_features[condition][feat] for feat in feature_names]\n",
    "            axes[0, 1].bar(x_pos + i*width, values, width, label=condition)\n",
    "        \n",
    "        axes[0, 1].set_title('Feature Comparison')\n",
    "        axes[0, 1].set_xlabel('Features')\n",
    "        axes[0, 1].set_ylabel('Value')\n",
    "        axes[0, 1].set_xticks(x_pos + width)\n",
    "        axes[0, 1].set_xticklabels(feature_names, rotation=45)\n",
    "        axes[0, 1].legend()\n",
    "        \n",
    "        # Plot 3: Power spectra\n",
    "        for condition, data in processed_data.items():\n",
    "            axes[1, 0].semilogy(data['frequencies'], data['power_spectrum'], label=condition)\n",
    "        axes[1, 0].set_title('Power Spectra')\n",
    "        axes[1, 0].set_xlabel('Frequency (Hz)')\n",
    "        axes[1, 0].set_ylabel('Power')\n",
    "        axes[1, 0].legend()\n",
    "        \n",
    "        # Plot 4: Summary statistics\n",
    "        conditions = list(all_features.keys())\n",
    "        means = [all_features[cond]['mean'] for cond in conditions]\n",
    "        stds = [all_features[cond]['std'] for cond in conditions]\n",
    "        \n",
    "        axes[1, 1].bar(conditions, means, yerr=stds, capsize=5)\n",
    "        axes[1, 1].set_title('Mean ± Std by Condition')\n",
    "        axes[1, 1].set_ylabel('Signal Mean')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save figure\n",
    "        figure_file = self.output_dir / \"results\" / \"analysis_summary.png\"\n",
    "        plt.savefig(figure_file, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        results['figures']['summary_plot'] = str(figure_file)\n",
    "        \n",
    "        # Save results\n",
    "        results_file = self.output_dir / \"results\" / \"analysis_results.pkl\"\n",
    "        scitex.io.save(results, results_file)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def run_complete_pipeline(self, n_samples=1000, noise_level=0.1):\n",
    "        \"\"\"Run the complete experiment pipeline.\"\"\"\n",
    "        \n",
    "        # Step 1: Generate data\n",
    "        raw_data = self.generate_data(n_samples, noise_level)\n",
    "        \n",
    "        # Step 2: Process data\n",
    "        processed_data = self.process_data(raw_data)\n",
    "        \n",
    "        # Step 3: Analyze results\n",
    "        results = self.analyze_results(processed_data)\n",
    "        \n",
    "        for file in self.output_dir.rglob(\"*\"):\n",
    "            if file.is_file():\n",
    "                # Condition met\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Run the complete pipeline\n",
    "pipeline = ExperimentPipeline(\n",
    "    experiment_name=\"SciTeX_IO_Demo\",\n",
    "    output_dir=data_dir / \"experiment_pipeline\"\n",
    ")\n",
    "\n",
    "final_results = pipeline.run_complete_pipeline(n_samples=500, noise_level=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70153c8",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Part 4: Configuration Management and Advanced Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64b8a40",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configuration file management\n",
    "config_dir = data_dir / \"configs\"\n",
    "config_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Create experiment configurations\n",
    "configs = {\n",
    "    'default': {\n",
    "    'data_params': {\n",
    "    'n_samples': 1000,\n",
    "    'noise_level': 0.1,\n",
    "    'sampling_rate': 100\n",
    "    },\n",
    "    'processing_params': {\n",
    "    'smoothing_sigma': 2.0,\n",
    "    'filter_cutoff': 0.5\n",
    "    },\n",
    "    'analysis_params': {\n",
    "    'significance_level': 0.05,\n",
    "    'bootstrap_iterations': 1000\n",
    "    }\n",
    "    },\n",
    "    'high_resolution': {\n",
    "    'data_params': {\n",
    "    'n_samples': 5000,\n",
    "    'noise_level': 0.05,\n",
    "    'sampling_rate': 1000\n",
    "    },\n",
    "    'processing_params': {\n",
    "    'smoothing_sigma': 1.0,\n",
    "    'filter_cutoff': 0.1\n",
    "    },\n",
    "    'analysis_params': {\n",
    "    'significance_level': 0.01,\n",
    "    'bootstrap_iterations': 5000\n",
    "    }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save configurations in different formats\n",
    "for config_name, config_data in configs.items():\n",
    "    # Save as JSON (human-readable)\n",
    "    json_file = config_dir / f\"{config_name}_config.json\"\n",
    "    scitex.io.save(config_data, json_file)\n",
    "    \n",
    "    # Save as YAML (if available)\n",
    "    try:\n",
    "        yaml_file = config_dir / f\"{config_name}_config.yaml\"\n",
    "        scitex.io.save(config_data, yaml_file)\n",
    "    except Exception:\n",
    "        pass  # Fixed incomplete except block\n",
    "\n",
    "# Load and use configuration\n",
    "loaded_config = scitex.io.load(config_dir / \"high_resolution_config.json\")\n",
    "for section, params in loaded_config.items():\n",
    "    for key, value in params.items():\n",
    "        # Loop body"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20388daf",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Summary and Best Practices\n",
    "\n",
    "This tutorial demonstrated the comprehensive I/O capabilities of the SciTeX library:\n",
    "\n",
    "### Key Features Covered:\n",
    "1. **Unified Interface**: Automatic format detection for save/load operations\n",
    "2. **Multiple Formats**: Support for pickle, JSON, HDF5, CSV, NumPy, and compressed formats\n",
    "3. **Performance Optimization**: Caching, compression, and format-specific optimizations\n",
    "4. **Batch Operations**: Efficient handling of multiple files\n",
    "5. **Complete Workflows**: Integration with experimental pipelines\n",
    "6. **Configuration Management**: Flexible configuration file handling\n",
    "\n",
    "### Best Practices:\n",
    "- Use **pickle** for complex Python objects and mixed data types\n",
    "- Use **HDF5** for large, hierarchical datasets\n",
    "- Use **JSON/YAML** for human-readable configuration files\n",
    "- Apply **compression** for large files when storage space is limited\n",
    "- Implement **caching** for expensive computations\n",
    "- Organize data with **clear directory structures**\n",
    "- Use **symlinks** for easy access to frequently used files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71274cc2",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cleanup - remove example files (optional)\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# For papermill compatibility, default to not cleaning up\n",
    "cleanup = False\n",
    "\n",
    "# When running interactively (not with papermill), you can uncomment this to enable cleanup:\n",
    "# cleanup = input(\"Clean up example files? (y/n): \").lower().startswith(\"y\")\n",
    "\n",
    "# Count created files\n",
    "total_files = 0\n",
    "total_size = 0\n",
    "\n",
    "if data_dir.exists():\n",
    "    for f in data_dir.rglob(\"*\"):\n",
    "        if f.is_file():\n",
    "            total_files += 1\n",
    "            total_size += f.stat().st_size\n",
    "\n",
    "if notebook_output_dir.exists():\n",
    "    for f in notebook_output_dir.rglob(\"*\"):\n",
    "        if f.is_file():\n",
    "            total_files += 1\n",
    "            total_size += f.stat().st_size\n",
    "\n",
    "\n",
    "if cleanup:\n",
    "    if data_dir.exists():\n",
    "        shutil.rmtree(data_dir)\n",
    "    if notebook_output_dir.exists():\n",
    "        shutil.rmtree(notebook_output_dir)\n",
    "else:\n",
    "    if data_dir.exists():\n",
    "        # Condition met\n",
    "    if notebook_output_dir.exists():\n",
    "        # Condition met"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 21.957261,
   "end_time": "2025-07-24T19:16:08.140514",
   "environment_variables": {},
   "exception": true,
   "input_path": "examples/notebooks/01_scitex_io.ipynb",
   "output_path": "examples/notebooks/test_output_01_scitex_io.ipynb",
   "parameters": {},
   "start_time": "2025-07-24T19:15:46.183253",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}