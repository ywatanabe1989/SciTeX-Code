{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SciTeX Input/Output Operations\n",
    "\n",
    "This notebook demonstrates the I/O capabilities provided by the `scitex.io` module, which offers unified file operations with automatic format detection, caching, and scientific data handling features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import json\n",
    "import scitex as stx\n",
    "from pathlib import Path\n",
    "import tempfile\n",
    "import shutil\n",
    "\n",
    "# Set up reproducible environment\n",
    "stx.repro.fix_seeds(42)\n",
    "\n",
    "# Create temporary directory for examples\n",
    "temp_dir = tempfile.mkdtemp(prefix='scitex_io_demo_')\n",
    "print(f\"Working directory: {temp_dir}\")\n",
    "print(f\"SciTeX version: {stx.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Universal Save and Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SciTeX provides universal save/load functions that handle multiple formats\n",
    "# The format is automatically detected from the file extension\n",
    "\n",
    "# Create sample data in different formats\n",
    "# NumPy array\n",
    "numpy_data = np.random.randn(100, 50)\n",
    "print(f\"NumPy array shape: {numpy_data.shape}\")\n",
    "\n",
    "# Pandas DataFrame\n",
    "df_data = pd.DataFrame({\n",
    "    'subject_id': range(100),\n",
    "    'measurement_1': np.random.randn(100),\n",
    "    'measurement_2': np.random.randn(100),\n",
    "    'category': np.random.choice(['A', 'B', 'C'], 100)\n",
    "})\n",
    "print(f\"\\nDataFrame shape: {df_data.shape}\")\n",
    "print(df_data.head())\n",
    "\n",
    "# Dictionary with mixed types\n",
    "dict_data = {\n",
    "    'experiment_name': 'Neural Recording Session 1',\n",
    "    'parameters': {\n",
    "        'sampling_rate': 1000,\n",
    "        'n_channels': 64,\n",
    "        'duration': 300\n",
    "    },\n",
    "    'results': {\n",
    "        'accuracy': 0.95,\n",
    "        'f1_score': 0.93\n",
    "    },\n",
    "    'data_array': numpy_data[:10, :10].tolist()\n",
    "}\n",
    "print(f\"\\nDictionary keys: {list(dict_data.keys())}\")\n",
    "\n",
    "# PyTorch tensor\n",
    "torch_data = torch.randn(32, 128, 1000)  # batch x features x time\n",
    "print(f\"\\nTorch tensor shape: {torch_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save different data types\n",
    "# NumPy - automatically saved as .npy\n",
    "npy_path = Path(temp_dir) / 'array_data.npy'\n",
    "stx.io.save(numpy_data, npy_path)\n",
    "print(f\"Saved NumPy array to: {npy_path}\")\n",
    "\n",
    "# Pandas DataFrame - can save as CSV, Pickle, or HDF5\n",
    "csv_path = Path(temp_dir) / 'dataframe.csv'\n",
    "pkl_path = Path(temp_dir) / 'dataframe.pkl'\n",
    "stx.io.save(df_data, csv_path)\n",
    "stx.io.save(df_data, pkl_path)\n",
    "print(f\"Saved DataFrame to: {csv_path} and {pkl_path}\")\n",
    "\n",
    "# Dictionary - save as JSON or Pickle\n",
    "json_path = Path(temp_dir) / 'experiment_config.json'\n",
    "stx.io.save(dict_data, json_path)\n",
    "print(f\"Saved dictionary to: {json_path}\")\n",
    "\n",
    "# PyTorch tensor\n",
    "pt_path = Path(temp_dir) / 'model_output.pt'\n",
    "stx.io.save(torch_data, pt_path)\n",
    "print(f\"Saved PyTorch tensor to: {pt_path}\")\n",
    "\n",
    "# Create a symlink from current working directory (optional)\n",
    "# This helps with organization and quick access\n",
    "stx.io.save(numpy_data, npy_path, symlink_from_cwd=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data back - format automatically detected\n",
    "loaded_numpy = stx.io.load(npy_path)\n",
    "loaded_csv = stx.io.load(csv_path)\n",
    "loaded_pkl = stx.io.load(pkl_path)\n",
    "loaded_json = stx.io.load(json_path)\n",
    "loaded_torch = stx.io.load(pt_path)\n",
    "\n",
    "# Verify data integrity\n",
    "print(\"Data integrity checks:\")\n",
    "print(f\"NumPy arrays match: {np.allclose(numpy_data, loaded_numpy)}\")\n",
    "print(f\"CSV DataFrame shape matches: {df_data.shape == loaded_csv.shape}\")\n",
    "print(f\"Pickle DataFrame equals: {df_data.equals(loaded_pkl)}\")\n",
    "print(f\"JSON keys match: {set(dict_data.keys()) == set(loaded_json.keys())}\")\n",
    "print(f\"Torch tensors match: {torch.allclose(torch_data, loaded_torch)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. HDF5 Exploration and Hierarchical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create complex hierarchical data structure\n",
    "experiment_data = {\n",
    "    'metadata': {\n",
    "        'date': '2024-01-15',\n",
    "        'experimenter': 'Dr. Smith',\n",
    "        'protocol': 'Neural Recording v2.1'\n",
    "    },\n",
    "    'subjects': {\n",
    "        'subject_001': {\n",
    "            'neural_data': np.random.randn(64, 10000),  # channels x time\n",
    "            'behavior': np.random.randn(1000, 3),       # time x xyz\n",
    "            'timestamps': np.linspace(0, 10, 10000),\n",
    "            'events': pd.DataFrame({\n",
    "                'time': np.sort(np.random.uniform(0, 10, 50)),\n",
    "                'event_type': np.random.choice(['stimulus', 'response', 'reward'], 50)\n",
    "            })\n",
    "        },\n",
    "        'subject_002': {\n",
    "            'neural_data': np.random.randn(64, 10000),\n",
    "            'behavior': np.random.randn(1000, 3),\n",
    "            'timestamps': np.linspace(0, 10, 10000),\n",
    "            'events': pd.DataFrame({\n",
    "                'time': np.sort(np.random.uniform(0, 10, 45)),\n",
    "                'event_type': np.random.choice(['stimulus', 'response', 'reward'], 45)\n",
    "            })\n",
    "        }\n",
    "    },\n",
    "    'analysis': {\n",
    "        'spike_rates': np.random.poisson(10, (2, 64)),  # subjects x channels\n",
    "        'correlations': np.random.rand(64, 64),\n",
    "        'summary_stats': {\n",
    "            'mean_rate': 10.5,\n",
    "            'std_rate': 2.3\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save as HDF5 (hierarchical data format)\n",
    "h5_path = Path(temp_dir) / 'experiment_data.h5'\n",
    "stx.io.save(experiment_data, h5_path)\n",
    "print(f\"Saved hierarchical data to: {h5_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore HDF5 file structure\n",
    "print(\"HDF5 File Structure:\")\n",
    "print(\"=\" * 50)\n",
    "stx.io.explore_h5(h5_path)\n",
    "\n",
    "# Check if specific keys exist\n",
    "key_to_check = '/subjects/subject_001/neural_data'\n",
    "exists = stx.io.has_h5_key(h5_path, key_to_check)\n",
    "print(f\"\\nKey '{key_to_check}' exists: {exists}\")\n",
    "\n",
    "# Load specific parts of the HDF5 file\n",
    "# Load only subject_001 data\n",
    "subject_001_data = stx.io.load(h5_path, key='/subjects/subject_001')\n",
    "print(f\"\\nLoaded subject_001 keys: {list(subject_001_data.keys())}\")\n",
    "\n",
    "# Load only analysis results\n",
    "analysis_data = stx.io.load(h5_path, key='/analysis')\n",
    "print(f\"Loaded analysis keys: {list(analysis_data.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Configuration Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create configuration files in different formats\n",
    "config_dir = Path(temp_dir) / 'configs'\n",
    "config_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# YAML-style config (saved as JSON for this example)\n",
    "main_config = {\n",
    "    'experiment': {\n",
    "        'name': 'Neural Decoding Study',\n",
    "        'version': '1.0.0',\n",
    "        'random_seed': 42\n",
    "    },\n",
    "    'model': {\n",
    "        'architecture': 'ResNet1D',\n",
    "        'n_layers': 4,\n",
    "        'hidden_size': 256,\n",
    "        'dropout': 0.5\n",
    "    },\n",
    "    'training': {\n",
    "        'batch_size': 32,\n",
    "        'learning_rate': 0.001,\n",
    "        'n_epochs': 100,\n",
    "        'optimizer': 'Adam'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Data preprocessing config\n",
    "preprocess_config = {\n",
    "    'sampling_rate': 1000,\n",
    "    'filter': {\n",
    "        'type': 'bandpass',\n",
    "        'low_freq': 1,\n",
    "        'high_freq': 100,\n",
    "        'order': 4\n",
    "    },\n",
    "    'normalization': 'z-score',\n",
    "    'window_size': 1000,\n",
    "    'overlap': 0.5\n",
    "}\n",
    "\n",
    "# Save configs\n",
    "stx.io.save(main_config, config_dir / 'main_config.json')\n",
    "stx.io.save(preprocess_config, config_dir / 'preprocessing.json')\n",
    "\n",
    "print(\"Saved configuration files:\")\n",
    "for f in config_dir.glob('*.json'):\n",
    "    print(f\"  - {f.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all configs from directory\n",
    "configs = stx.io.load_configs(config_dir)\n",
    "\n",
    "print(\"Loaded configurations:\")\n",
    "for name, config in configs.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(json.dumps(config, indent=2)[:200] + '...')\n",
    "\n",
    "# Access specific config values\n",
    "print(f\"\\nModel architecture: {configs['main_config']['model']['architecture']}\")\n",
    "print(f\"Learning rate: {configs['main_config']['training']['learning_rate']}\")\n",
    "print(f\"Filter type: {configs['preprocessing']['filter']['type']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. File Pattern Matching with Glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multiple files with patterns\n",
    "data_dir = Path(temp_dir) / 'data'\n",
    "data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Create files for multiple subjects and sessions\n",
    "for subject in ['S001', 'S002', 'S003']:\n",
    "    for session in range(1, 4):\n",
    "        for data_type in ['neural', 'behavior', 'events']:\n",
    "            filename = f\"{subject}_session{session}_{data_type}.npy\"\n",
    "            data = np.random.randn(100, 10)\n",
    "            stx.io.save(data, data_dir / filename)\n",
    "\n",
    "# Also create some analysis files\n",
    "for subject in ['S001', 'S002', 'S003']:\n",
    "    analysis_data = {'accuracy': np.random.rand(), 'f1_score': np.random.rand()}\n",
    "    stx.io.save(analysis_data, data_dir / f\"{subject}_analysis_results.json\")\n",
    "\n",
    "print(f\"Created {len(list(data_dir.glob('*')))} files in {data_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use glob to find files with patterns\n",
    "# Find all neural data files\n",
    "neural_files = stx.io.glob(data_dir, '*neural.npy')\n",
    "print(f\"Found {len(neural_files)} neural data files:\")\n",
    "for f in sorted(neural_files)[:5]:\n",
    "    print(f\"  - {Path(f).name}\")\n",
    "\n",
    "# Find all files for subject S001\n",
    "s001_files = stx.io.glob(data_dir, 'S001_*.npy')\n",
    "print(f\"\\nFound {len(s001_files)} files for subject S001\")\n",
    "\n",
    "# Find all session 2 files\n",
    "session2_files = stx.io.glob(data_dir, '*_session2_*.npy')\n",
    "print(f\"\\nFound {len(session2_files)} files for session 2\")\n",
    "\n",
    "# Use parse_glob to extract pattern information\n",
    "pattern = '{subject}_session{session:d}_{dtype}.npy'\n",
    "parsed_results = []\n",
    "\n",
    "for filepath in neural_files[:3]:\n",
    "    parsed = stx.io.parse_glob(filepath, pattern)\n",
    "    if parsed:\n",
    "        parsed_results.append(parsed)\n",
    "        print(f\"\\nParsed {Path(filepath).name}:\")\n",
    "        print(f\"  Subject: {parsed['subject']}\")\n",
    "        print(f\"  Session: {parsed['session']}\")\n",
    "        print(f\"  Data type: {parsed['dtype']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Caching for Expensive Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an expensive computation\n",
    "def expensive_analysis(data, n_components=10):\n",
    "    \"\"\"Simulate expensive analysis like PCA or spectral decomposition.\"\"\"\n",
    "    import time\n",
    "    print(\"Running expensive analysis...\")\n",
    "    time.sleep(2)  # Simulate computation time\n",
    "    \n",
    "    # Fake PCA-like results\n",
    "    components = np.random.randn(n_components, data.shape[1])\n",
    "    explained_variance = np.random.rand(n_components)\n",
    "    explained_variance = explained_variance / explained_variance.sum()\n",
    "    \n",
    "    return {\n",
    "        'components': components,\n",
    "        'explained_variance': explained_variance,\n",
    "        'total_variance': np.var(data)\n",
    "    }\n",
    "\n",
    "# Create cache directory\n",
    "cache_dir = Path(temp_dir) / '.cache'\n",
    "cache_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# First call - will compute and cache\n",
    "test_data = np.random.randn(1000, 50)\n",
    "cache_key = 'pca_analysis_v1'\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "result1 = stx.io.cache(\n",
    "    expensive_analysis,\n",
    "    test_data,\n",
    "    n_components=5,\n",
    "    cache_key=cache_key,\n",
    "    cache_dir=cache_dir\n",
    ")\n",
    "first_call_time = time.time() - start_time\n",
    "print(f\"First call took: {first_call_time:.2f} seconds\")\n",
    "\n",
    "# Second call - will load from cache\n",
    "start_time = time.time()\n",
    "result2 = stx.io.cache(\n",
    "    expensive_analysis,\n",
    "    test_data,\n",
    "    n_components=5,\n",
    "    cache_key=cache_key,\n",
    "    cache_dir=cache_dir\n",
    ")\n",
    "second_call_time = time.time() - start_time\n",
    "print(f\"Second call (cached) took: {second_call_time:.2f} seconds\")\n",
    "print(f\"Speedup: {first_call_time / second_call_time:.1f}x\")\n",
    "\n",
    "# Verify results are identical\n",
    "print(f\"\\nResults match: {np.allclose(result1['components'], result2['components'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Specialized Save Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save multiple DataFrames as separate CSV files\n",
    "results_dir = Path(temp_dir) / 'results'\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Create list of DataFrames (e.g., results from multiple experiments)\n",
    "dfs = []\n",
    "for i in range(5):\n",
    "    df = pd.DataFrame({\n",
    "        'experiment_id': i,\n",
    "        'accuracy': np.random.rand(10),\n",
    "        'precision': np.random.rand(10),\n",
    "        'recall': np.random.rand(10),\n",
    "        'f1_score': np.random.rand(10)\n",
    "    })\n",
    "    dfs.append(df)\n",
    "\n",
    "# Save all DataFrames\n",
    "if stx.io.save_listed_dfs_as_csv:\n",
    "    stx.io.save_listed_dfs_as_csv(\n",
    "        dfs,\n",
    "        names=[f'experiment_{i}' for i in range(5)],\n",
    "        save_dir=results_dir\n",
    "    )\n",
    "    print(f\"Saved {len(dfs)} DataFrames to {results_dir}\")\n",
    "\n",
    "# Save scalar metrics over time\n",
    "epochs = list(range(100))\n",
    "train_loss = [1.0 / (i + 1) + 0.1 * np.random.randn() for i in epochs]\n",
    "val_loss = [1.2 / (i + 1) + 0.15 * np.random.randn() for i in epochs]\n",
    "accuracy = [min(0.99, i / 100 + 0.1 * np.random.rand()) for i in epochs]\n",
    "\n",
    "if stx.io.save_listed_scalars_as_csv:\n",
    "    stx.io.save_listed_scalars_as_csv(\n",
    "        [train_loss, val_loss, accuracy],\n",
    "        names=['train_loss', 'val_loss', 'accuracy'],\n",
    "        index=epochs,\n",
    "        index_name='epoch',\n",
    "        save_path=results_dir / 'training_metrics.csv'\n",
    "    )\n",
    "    print(\"Saved training metrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and save images\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 10))\n",
    "\n",
    "# Plot 1: Signal visualization\n",
    "t = np.linspace(0, 2, 1000)\n",
    "signal = np.sin(2 * np.pi * 5 * t) + 0.5 * np.sin(2 * np.pi * 20 * t)\n",
    "axes[0, 0].plot(t, signal)\n",
    "axes[0, 0].set_title('Multi-frequency Signal')\n",
    "axes[0, 0].set_xlabel('Time (s)')\n",
    "axes[0, 0].set_ylabel('Amplitude')\n",
    "\n",
    "# Plot 2: Correlation matrix\n",
    "corr_matrix = np.random.rand(10, 10)\n",
    "corr_matrix = (corr_matrix + corr_matrix.T) / 2  # Make symmetric\n",
    "np.fill_diagonal(corr_matrix, 1)\n",
    "im = axes[0, 1].imshow(corr_matrix, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "axes[0, 1].set_title('Correlation Matrix')\n",
    "plt.colorbar(im, ax=axes[0, 1])\n",
    "\n",
    "# Plot 3: Training curves\n",
    "if 'epochs' in locals():\n",
    "    axes[1, 0].plot(epochs, train_loss, label='Train Loss')\n",
    "    axes[1, 0].plot(epochs, val_loss, label='Val Loss')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Loss')\n",
    "    axes[1, 0].set_title('Training Progress')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].set_yscale('log')\n",
    "\n",
    "# Plot 4: Bar chart\n",
    "categories = ['Method A', 'Method B', 'Method C', 'Method D']\n",
    "values = np.random.rand(4) * 0.3 + 0.7\n",
    "axes[1, 1].bar(categories, values)\n",
    "axes[1, 1].set_ylabel('Accuracy')\n",
    "axes[1, 1].set_title('Method Comparison')\n",
    "axes[1, 1].set_ylim([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save figure\n",
    "fig_path = results_dir / 'analysis_results.png'\n",
    "if stx.io.save_image:\n",
    "    stx.io.save_image(fig, fig_path, dpi=300)\n",
    "else:\n",
    "    fig.savefig(fig_path, dpi=300)\n",
    "    \n",
    "plt.close()\n",
    "print(f\"Saved figure to {fig_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Hot Reload for Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple module file that can be modified\n",
    "module_path = Path(temp_dir) / 'my_analysis.py'\n",
    "module_content = '''\n",
    "def analyze_data(data):\n",
    "    \"\"\"Simple analysis function.\"\"\"\n",
    "    return {\n",
    "        'mean': data.mean(),\n",
    "        'std': data.std(),\n",
    "        'version': 1\n",
    "    }\n",
    "'''\n",
    "\n",
    "with open(module_path, 'w') as f:\n",
    "    f.write(module_content)\n",
    "\n",
    "# Import the module\n",
    "import sys\n",
    "sys.path.insert(0, str(temp_dir))\n",
    "import my_analysis\n",
    "\n",
    "# Use the function\n",
    "data = np.random.randn(100)\n",
    "result1 = my_analysis.analyze_data(data)\n",
    "print(f\"First version result: {result1}\")\n",
    "\n",
    "# Modify the module (simulate development)\n",
    "updated_content = '''\n",
    "import numpy as np\n",
    "\n",
    "def analyze_data(data):\n",
    "    \"\"\"Enhanced analysis function.\"\"\"\n",
    "    return {\n",
    "        'mean': data.mean(),\n",
    "        'std': data.std(),\n",
    "        'median': np.median(data),\n",
    "        'q25': np.percentile(data, 25),\n",
    "        'q75': np.percentile(data, 75),\n",
    "        'version': 2\n",
    "    }\n",
    "'''\n",
    "\n",
    "with open(module_path, 'w') as f:\n",
    "    f.write(updated_content)\n",
    "\n",
    "# Reload the module\n",
    "stx.io.reload(my_analysis)\n",
    "\n",
    "# Use the updated function\n",
    "result2 = my_analysis.analyze_data(data)\n",
    "print(f\"\\nUpdated version result: {result2}\")\n",
    "print(f\"\\nNew keys added: {set(result2.keys()) - set(result1.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Batch Operations and Organization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organize experiment outputs with automatic directory creation\n",
    "experiment_name = \"neural_decoding_2024\"\n",
    "timestamp = pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')\n",
    "output_base = Path(temp_dir) / 'experiments' / experiment_name / timestamp\n",
    "\n",
    "# Save will automatically create directories\n",
    "# Save different types of outputs\n",
    "outputs = {\n",
    "    'raw_data': np.random.randn(1000, 64),\n",
    "    'processed_data': np.random.randn(1000, 32),\n",
    "    'features': np.random.randn(1000, 128),\n",
    "    'predictions': np.random.randint(0, 3, 1000),\n",
    "    'metadata': {\n",
    "        'n_subjects': 10,\n",
    "        'n_trials': 100,\n",
    "        'conditions': ['A', 'B', 'C']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save all outputs in organized structure\n",
    "for name, data in outputs.items():\n",
    "    if isinstance(data, np.ndarray):\n",
    "        path = output_base / 'arrays' / f'{name}.npy'\n",
    "    else:\n",
    "        path = output_base / 'configs' / f'{name}.json'\n",
    "    \n",
    "    stx.io.save(data, path)\n",
    "    print(f\"Saved {name} to {path.relative_to(temp_dir)}\")\n",
    "\n",
    "# Create summary report\n",
    "summary = {\n",
    "    'experiment': experiment_name,\n",
    "    'timestamp': timestamp,\n",
    "    'outputs': list(outputs.keys()),\n",
    "    'data_shapes': {k: v.shape if isinstance(v, np.ndarray) else 'metadata' \n",
    "                   for k, v in outputs.items()}\n",
    "}\n",
    "\n",
    "summary_path = output_base / 'summary.json'\n",
    "stx.io.save(summary, summary_path)\n",
    "print(f\"\\nSaved experiment summary to {summary_path.relative_to(temp_dir)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Integration Example: Complete Experiment Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment_pipeline(config_path, data_path, output_dir):\n",
    "    \"\"\"Complete experiment pipeline with SciTeX I/O.\"\"\"\n",
    "    \n",
    "    # Load configuration\n",
    "    config = stx.io.load(config_path)\n",
    "    print(f\"Loaded config: {config['experiment']['name']}\")\n",
    "    \n",
    "    # Load data\n",
    "    data = stx.io.load(data_path)\n",
    "    print(f\"Loaded data shape: {data.shape}\")\n",
    "    \n",
    "    # Create output directory structure\n",
    "    output_dir = Path(output_dir)\n",
    "    dirs = {\n",
    "        'processed': output_dir / 'processed_data',\n",
    "        'features': output_dir / 'features',\n",
    "        'models': output_dir / 'models',\n",
    "        'results': output_dir / 'results',\n",
    "        'figures': output_dir / 'figures'\n",
    "    }\n",
    "    \n",
    "    # Process data (cached)\n",
    "    @stx.gen.cache\n",
    "    def process_data(data, config):\n",
    "        # Simulate preprocessing\n",
    "        processed = data - data.mean(axis=0)\n",
    "        processed = processed / (data.std(axis=0) + 1e-8)\n",
    "        return processed\n",
    "    \n",
    "    processed_data = process_data(data, config)\n",
    "    stx.io.save(processed_data, dirs['processed'] / 'normalized_data.npy')\n",
    "    \n",
    "    # Extract features\n",
    "    features = {\n",
    "        'mean_features': processed_data.mean(axis=1),\n",
    "        'std_features': processed_data.std(axis=1),\n",
    "        'pca_features': np.random.randn(data.shape[0], 10)  # Simulated PCA\n",
    "    }\n",
    "    \n",
    "    for name, feat in features.items():\n",
    "        stx.io.save(feat, dirs['features'] / f'{name}.npy')\n",
    "    \n",
    "    # Simulate model training\n",
    "    model_results = {\n",
    "        'accuracy': 0.92,\n",
    "        'precision': 0.91,\n",
    "        'recall': 0.93,\n",
    "        'f1_score': 0.92,\n",
    "        'confusion_matrix': np.random.randint(0, 50, (3, 3))\n",
    "    }\n",
    "    \n",
    "    stx.io.save(model_results, dirs['results'] / 'model_performance.json')\n",
    "    \n",
    "    # Generate and save report\n",
    "    report = {\n",
    "        'experiment': config['experiment'],\n",
    "        'data_info': {\n",
    "            'n_samples': data.shape[0],\n",
    "            'n_features': data.shape[1]\n",
    "        },\n",
    "        'results': model_results,\n",
    "        'output_structure': {k: str(v) for k, v in dirs.items()}\n",
    "    }\n",
    "    \n",
    "    stx.io.save(report, output_dir / 'experiment_report.json')\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "    im = ax.imshow(model_results['confusion_matrix'], cmap='Blues')\n",
    "    ax.set_title('Confusion Matrix')\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_ylabel('True')\n",
    "    plt.colorbar(im, ax=ax)\n",
    "    \n",
    "    # Add text annotations\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            ax.text(j, i, str(model_results['confusion_matrix'][i, j]),\n",
    "                   ha='center', va='center', color='white' if model_results['confusion_matrix'][i, j] > 25 else 'black')\n",
    "    \n",
    "    fig_path = dirs['figures'] / 'confusion_matrix.png'\n",
    "    fig.savefig(fig_path, dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"\\nExperiment complete! Results saved to {output_dir}\")\n",
    "    return report\n",
    "\n",
    "# Run the pipeline\n",
    "pipeline_output = Path(temp_dir) / 'pipeline_output'\n",
    "report = run_experiment_pipeline(\n",
    "    config_path=config_dir / 'main_config.json',\n",
    "    data_path=npy_path,\n",
    "    output_dir=pipeline_output\n",
    ")\n",
    "\n",
    "print(\"\\nFinal report:\")\n",
    "print(json.dumps(report, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up temporary directory\n",
    "shutil.rmtree(temp_dir)\n",
    "print(f\"Cleaned up temporary directory: {temp_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The `scitex.io` module provides comprehensive I/O functionality for scientific computing:\n",
    "\n",
    "1. **Universal Save/Load**: Automatic format detection for NumPy, Pandas, PyTorch, JSON, HDF5, etc.\n",
    "2. **HDF5 Support**: Hierarchical data storage with selective loading and exploration tools\n",
    "3. **Configuration Management**: Easy loading of multiple config files from directories\n",
    "4. **Pattern Matching**: Powerful glob functionality with pattern parsing\n",
    "5. **Caching**: Speed up expensive computations with automatic caching\n",
    "6. **Batch Operations**: Save multiple DataFrames, scalars, and images efficiently\n",
    "7. **Hot Reload**: Reload modules during development without restarting\n",
    "8. **Automatic Organization**: Creates directory structures automatically\n",
    "\n",
    "These features enable:\n",
    "- Reproducible data management\n",
    "- Efficient experiment organization\n",
    "- Fast prototyping with caching\n",
    "- Clean separation of data, configs, and results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}