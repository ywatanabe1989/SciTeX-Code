{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SciTeX Decorators Module - Comprehensive Tutorial\n",
    "\n",
    "This notebook demonstrates the complete functionality of the `scitex.decorators` module for type conversion, batch processing, caching, and advanced function enhancement.\n",
    "\n",
    "## Features Covered\n",
    "* Type conversion decorators (numpy, torch, pandas)\n",
    "* Batch processing with automatic vectorization\n",
    "* Memory and disk caching for performance\n",
    "* Function utilities (timeout, deprecation)\n",
    "* Decorator composition and auto-ordering\n",
    "* Real-world application patterns\n",
    "* Custom decorator creation\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup and Auto-Ordering](#1-setup-and-auto-ordering)\n",
    "2. [Type Conversion Decorators](#2-type-conversion-decorators)\n",
    "3. [Batch Processing Decorators](#3-batch-processing-decorators)\n",
    "4. [Caching Decorators](#4-caching-decorators)\n",
    "5. [Utility Decorators](#5-utility-decorators)\n",
    "6. [Decorator Composition](#6-decorator-composition)\n",
    "7. [Real-World Applications](#7-real-world-applications)\n",
    "8. [Advanced Patterns](#8-advanced-patterns)\n",
    "9. [Complete Processing Pipeline](#9-complete-processing-pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26ccbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect notebook name for output directory\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Get notebook name (for papermill compatibility)\n",
    "notebook_name = \"21_scitex_decorators\"\n",
    "if 'PAPERMILL_NOTEBOOK_NAME' in os.environ:\n",
    "    notebook_name = Path(os.environ['PAPERMILL_NOTEBOOK_NAME']).stem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Auto-Ordering\n",
    "\n",
    "The decorators module provides automatic ordering to ensure decorators are applied in the correct sequence."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "import scitex as stx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Check for optional dependencies\n",
    "try:\n",
    "    import torch\n",
    "    TORCH_AVAILABLE = True\n",
    "    print(\"✓ PyTorch available\")\n",
    "except ImportError:\n",
    "    TORCH_AVAILABLE = False\n",
    "    print(\"✗ PyTorch not available\")\n",
    "\n",
    "try:\n",
    "    import xarray as xr\n",
    "    XARRAY_AVAILABLE = True\n",
    "    print(\"✓ Xarray available\")\n",
    "except ImportError:\n",
    "    XARRAY_AVAILABLE = False\n",
    "    print(\"✗ Xarray not available\")\n",
    "\n",
    "# Enable auto-ordering for decorators (IMPORTANT!)\n",
    "stx.decorators.enable_auto_order()\n",
    "print(\"\\n✓ Auto-ordering enabled for decorators\")\n",
    "print(\"  This ensures decorators are applied in optimal order automatically\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Type Conversion Decorators\n",
    "\n",
    "These decorators automatically convert function inputs to the appropriate data types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 NumPy Function Decorator\n",
    "\n",
    "The `@numpy_fn` decorator converts inputs to NumPy arrays automatically."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Basic numpy_fn usage\n@stx.decorators.numpy_fn\ndef compute_statistics(x):\n    \"\"\"Compute comprehensive statistics of data.\"\"\"\n    return {\n        'mean': x.mean(),\n        'std': x.std(),\n        'min': x.min(),\n        'max': x.max(),\n        'median': np.median(x),\n        'shape': x.shape,\n        'dtype': str(x.dtype)\n    }\n\nprint(\"=== numpy_fn Decorator Examples ===\")\n\n# Test with different input types\n# 1. Python list\nlist_data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\nresult = compute_statistics(list_data)\nprint(f\"\\n1. List input: {list_data}\")\nprint(f\"   Statistics: mean={result['mean']:.2f}, std={result['std']:.2f}\")\nprint(f\"   Type converted to: {result['dtype']}\")\n\n# 2. Pandas Series\nseries_data = pd.Series([10, 20, 30, 40, 50], name='values')\nresult = compute_statistics(series_data)\nprint(f\"\\n2. Pandas Series input (name: {series_data.name})\")\nprint(f\"   Statistics: mean={result['mean']:.2f}, std={result['std']:.2f}\")\nprint(f\"   Shape: {result['shape']}\")\n\n# 3. Already NumPy array\narray_data = np.random.randn(3, 4)\nresult = compute_statistics(array_data)\nprint(f\"\\n3. NumPy array input shape: {array_data.shape}\")\nprint(f\"   Statistics: mean={result['mean']:.2f}, std={result['std']:.2f}\")\nprint(f\"   Range: [{result['min']:.2f}, {result['max']:.2f}]\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 PyTorch Function Decorator\n",
    "\n",
    "The `@torch_fn` decorator converts inputs to PyTorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TORCH_AVAILABLE:\n",
    "    print(\"=== torch_fn Decorator Examples ===\")\n",
    "    \n",
    "    @stx.decorators.torch_fn\n",
    "    def neural_operations(x, temperature=1.0):\n",
    "        \"\"\"Perform neural network-style operations.\"\"\"\n",
    "        # Softmax with temperature\n",
    "        softmax = torch.softmax(x / temperature, dim=-1)\n",
    "        \n",
    "        # L2 normalization\n",
    "        l2_norm = torch.nn.functional.normalize(x, p=2, dim=-1)\n",
    "        \n",
    "        # Attention weights (simplified)\n",
    "        attention = torch.softmax(torch.matmul(x, x.transpose(-1, -2)), dim=-1)\n",
    "        \n",
    "        return {\n",
    "            'softmax': softmax,\n",
    "            'l2_normalized': l2_norm,\n",
    "            'attention_shape': attention.shape,\n",
    "            'input_device': x.device,\n",
    "            'input_dtype': x.dtype\n",
    "        }\n",
    "    \n",
    "    # Test with NumPy array\n",
    "    np_data = np.random.randn(3, 4)\n",
    "    result = neural_operations(np_data, temperature=0.5)\n",
    "    print(f\"\\nNumPy input shape: {np_data.shape}\")\n",
    "    print(f\"Converted to device: {result['input_device']}\")\n",
    "    print(f\"Converted to dtype: {result['input_dtype']}\")\n",
    "    print(f\"Softmax output shape: {result['softmax'].shape}\")\n",
    "    print(f\"Softmax sums to 1: {torch.allclose(result['softmax'].sum(dim=-1), torch.ones(3))}\")\n",
    "    \n",
    "    # Test with different temperature values\n",
    "    temperatures = [0.1, 1.0, 10.0]\n",
    "    data = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "    \n",
    "    print(f\"\\nTemperature effects on softmax:\")\n",
    "    for temp in temperatures:\n",
    "        result = neural_operations(data, temperature=temp)\n",
    "        entropy = -(result['softmax'] * torch.log(result['softmax'] + 1e-8)).sum(dim=-1).mean()\n",
    "        max_prob = result['softmax'].max(dim=-1)[0].mean()\n",
    "        print(f\"  T={temp:4.1f}: entropy={entropy:.3f}, max_prob={max_prob:.3f}\")\n",
    "        \n",
    "else:\n",
    "    print(\"PyTorch not available, skipping torch_fn examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Pandas Function Decorator\n",
    "\n",
    "The `@pandas_fn` decorator converts inputs to pandas DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== pandas_fn Decorator Examples ===\")\n",
    "\n",
    "@stx.decorators.pandas_fn\n",
    "def comprehensive_dataframe_analysis(df):\n",
    "    \"\"\"Perform comprehensive DataFrame analysis.\"\"\"\n",
    "    analysis = {\n",
    "        'basic_info': {\n",
    "            'shape': df.shape,\n",
    "            'columns': df.columns.tolist(),\n",
    "            'dtypes': df.dtypes.to_dict(),\n",
    "            'memory_usage': df.memory_usage(deep=True).sum()\n",
    "        },\n",
    "        'missing_data': {\n",
    "            'missing_counts': df.isnull().sum().to_dict(),\n",
    "            'missing_percentage': (df.isnull().sum() / len(df) * 100).to_dict(),\n",
    "            'complete_rows': len(df.dropna())\n",
    "        },\n",
    "        'summary_stats': df.describe().to_dict() if len(df.select_dtypes(include=[np.number]).columns) > 0 else {},\n",
    "        'categorical_info': {}\n",
    "    }\n",
    "    \n",
    "    # Analyze categorical columns\n",
    "    categorical_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "    for col in categorical_cols:\n",
    "        analysis['categorical_info'][col] = {\n",
    "            'unique_count': df[col].nunique(),\n",
    "            'top_values': df[col].value_counts().head(3).to_dict()\n",
    "        }\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "# Test with dictionary input\n",
    "dict_data = {\n",
    "    'age': [25, 30, 35, 40, 45, None, 50],\n",
    "    'salary': [50000, 60000, 70000, 80000, 90000, 95000, 100000],\n",
    "    'department': ['IT', 'Finance', 'IT', 'HR', 'Finance', 'IT', 'HR'],\n",
    "    'experience': [2, 5, 8, 12, 15, 18, 20]\n",
    "}\n",
    "\n",
    "result = comprehensive_dataframe_analysis(dict_data)\n",
    "print(\"\\nDictionary input analysis:\")\n",
    "print(f\"  Shape: {result['basic_info']['shape']}\")\n",
    "print(f\"  Columns: {result['basic_info']['columns']}\")\n",
    "print(f\"  Missing data: {result['missing_data']['missing_counts']}\")\n",
    "print(f\"  Complete rows: {result['missing_data']['complete_rows']}\")\n",
    "\n",
    "# # Test with NumPy array input(creates DataFrame with default column names)\n",
    "array_data = np.random.randn(100, 4)\n",
    "result = comprehensive_dataframe_analysis(array_data)\n",
    "print(f\"\\nNumPy array input analysis:\")\n",
    "print(f\"  Shape: {result['basic_info']['shape']}\")\n",
    "print(f\"  Auto-generated columns: {result['basic_info']['columns']}\")\n",
    "print(f\"  Data types: {list(set(result['basic_info']['dtypes'].values()))}\")\n",
    "\n",
    "# Show summary statistics for numeric columns\n",
    "if result['summary_stats']:\n",
    "    print(f\"  Mean values: {[f'{k}: {v[\"mean\"]:.2f}' for k, v in result['summary_stats'].items()]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Batch Processing Decorators\n",
    "\n",
    "Batch processing decorators allow functions written for single samples to automatically work with batches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Basic Batch Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== batch_fn Decorator Examples ===\")\n",
    "\n",
    "# Simple scalar function\n",
    "@stx.decorators.batch_fn\n",
    "def complex_transform(x):\n",
    "    \"\"\"Apply complex mathematical transformation to single value.\"\"\"\n",
    "    return x**3 - 2*x**2 + 3*x - 1\n",
    "\n",
    "# Test with single value\n",
    "single_result = complex_transform(2.5)\n",
    "print(f\"Single value (2.5): {single_result:.2f}\")\n",
    "\n",
    "# Test with array of values\n",
    "batch_values = np.array([1, 2, 3, 4, 5])\n",
    "batch_results = complex_transform(batch_values)\n",
    "print(f\"\\nBatch input: {batch_values}\")\n",
    "print(f\"Batch output: {batch_results}\")\n",
    "\n",
    "# Vector function\n",
    "@stx.decorators.batch_fn\n",
    "def normalize_and_analyze(vector):\n",
    "    \"\"\"Normalize vector and return analysis.\"\"\"\n",
    "    norm = np.linalg.norm(vector)\n",
    "    if norm > 0:\n",
    "        normalized = vector / norm\n",
    "    else:\n",
    "        normalized = vector\n",
    "    \n",
    "    return {\n",
    "        'original_norm': norm,\n",
    "        'normalized': normalized,\n",
    "        'mean': normalized.mean(),\n",
    "        'std': normalized.std()\n",
    "    }\n",
    "\n",
    "# Test with batch of vectors\n",
    "vectors = np.random.randn(5, 3)  # 5 vectors of dimension 3\n",
    "results = normalize_and_analyze(vectors)\n",
    "\n",
    "print(f\"\\nBatch of vectors shape: {vectors.shape}\")\n",
    "print(\"Analysis results:\")\n",
    "for i in range(len(results)):\n",
    "    print(f\"  Vector {i}: norm={results[i]['original_norm']:.3f}, mean={results[i]['mean']:.3f}\")\n",
    "\n",
    "# Verify normalization\n",
    "final_norms = [np.linalg.norm(results[i]['normalized']) for i in range(len(results))]\n",
    "print(f\"Final norms after normalization: {[f'{n:.3f}' for n in final_norms]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Multi-dimensional Batch Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix operations with batch processing\n",
    "@stx.decorators.batch_fn(n_batch_dims=2)  # Process 2D matrices\n",
    "def matrix_properties(matrix):\n",
    "    \"\"\"Compute properties of a 2D matrix.\"\"\"\n",
    "    return {\n",
    "        'determinant': np.linalg.det(matrix),\n",
    "        'trace': np.trace(matrix),\n",
    "        'frobenius_norm': np.linalg.norm(matrix, 'fro'),\n",
    "        'condition_number': np.linalg.cond(matrix),\n",
    "        'rank': np.linalg.matrix_rank(matrix)\n",
    "    }\n",
    "\n",
    "# Create batch of 3x3 matrices\n",
    "batch_size = 4\n",
    "matrices = np.random.randn(batch_size, 3, 3)\n",
    "properties = matrix_properties(matrices)\n",
    "\n",
    "print(\"Batch matrix analysis:\")\n",
    "print(f\"Input shape: {matrices.shape} (batch_size=4, matrix_size=3x3)\")\n",
    "print(\"\\nMatrix properties:\")\n",
    "for i in range(batch_size):\n",
    "    props = properties[i]\n",
    "    print(f\"  Matrix {i}:\")\n",
    "    print(f\"    Determinant: {props['determinant']:8.3f}\")\n",
    "    print(f\"    Trace: {props['trace']:8.3f}\")\n",
    "    print(f\"    Condition: {props['condition_number']:8.2e}\")\n",
    "    print(f\"    Rank: {props['rank']:8d}\")\n",
    "\n",
    "# Complex classification example\n",
    "@stx.decorators.batch_fn\n",
    "def classify_data_point(point):\n",
    "    \"\"\"Classify a multi-dimensional point.\"\"\"\n",
    "    if len(point) < 2:\n",
    "        return 'invalid'\n",
    "    \n",
    "    x, y = point[0], point[1]\n",
    "    \n",
    "    # Distance from origin\n",
    "    distance = np.sqrt(x**2 + y**2)\n",
    "    \n",
    "    # Classify based on multiple criteria\n",
    "    if distance < 1:\n",
    "        category = 'center'\n",
    "    elif distance < 2:\n",
    "        category = 'middle'\n",
    "    else:\n",
    "        category = 'outer'\n",
    "    \n",
    "    # Add quadrant information\n",
    "    if x >= 0 and y >= 0:\n",
    "        quadrant = 'Q1'\n",
    "    elif x < 0 and y >= 0:\n",
    "        quadrant = 'Q2'\n",
    "    elif x < 0 and y < 0:\n",
    "        quadrant = 'Q3'\n",
    "    else:\n",
    "        quadrant = 'Q4'\n",
    "    \n",
    "    return {\n",
    "        'category': category,\n",
    "        'quadrant': quadrant,\n",
    "        'distance': distance,\n",
    "        'angle': np.arctan2(y, x)\n",
    "    }\n",
    "\n",
    "# Generate random 2D points\n",
    "points = np.random.randn(8, 2) * 2  # 8 points in 2D\n",
    "classifications = classify_data_point(points)\n",
    "\n",
    "print(f\"\\nPoint classification (8 random 2D points):\")\n",
    "for i, (point, cls) in enumerate(zip(points, classifications)):\n",
    "    print(f\"  Point {i}: ({point[0]:6.2f}, {point[1]:6.2f}) -> {cls['category']:6s} {cls['quadrant']} (d={cls['distance']:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Caching Decorators\n",
    "\n",
    "Caching decorators improve performance by storing function results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Memory Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== cache_mem Decorator Examples ===\")\n",
    "\n",
    "# Expensive computation with memory caching\n",
    "@stx.decorators.cache_mem\n",
    "def fibonacci_recursive(n):\n",
    "    \"\"\"Compute Fibonacci number recursively (expensive without caching).\"\"\"\n",
    "    if n <= 1:\n",
    "        return n\n",
    "    return fibonacci_recursive(n-1) + fibonacci_recursive(n-2)\n",
    "\n",
    "@stx.decorators.cache_mem\n",
    "def expensive_matrix_operation(size, seed=42):\n",
    "    \"\"\"Simulate expensive matrix computation.\"\"\"\n",
    "    print(f\"Computing matrix operation for size={size}, seed={seed}...\")\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Simulate expensive computation\n",
    "    matrix = np.random.randn(size, size)\n",
    "    time.sleep(0.5)  # Simulate computation time\n",
    "    \n",
    "    # Perform expensive operations\n",
    "    eigenvals = np.linalg.eigvals(matrix)\n",
    "    svd = np.linalg.svd(matrix)\n",
    "    \n",
    "    return {\n",
    "        'matrix': matrix,\n",
    "        'eigenvalues': eigenvals,\n",
    "        'singular_values': svd[1],\n",
    "        'condition_number': np.linalg.cond(matrix)\n",
    "    }\n",
    "\n",
    "# Test Fibonacci (dramatic speedup with caching)\n",
    "print(\"Fibonacci computation (with automatic caching):\")\n",
    "start = time.time()\n",
    "fib_30 = fibonacci_recursive(30)\n",
    "time_30 = time.time() - start\n",
    "print(f\"  fibonacci(30) = {fib_30} (computed in {time_30:.3f}s)\")\n",
    "\n",
    "start = time.time()\n",
    "fib_35 = fibonacci_recursive(35)  # Uses cached results from smaller values\n",
    "time_35 = time.time() - start\n",
    "print(f\"  fibonacci(35) = {fib_35} (computed in {time_35:.3f}s)\")\n",
    "\n",
    "# Test matrix operation caching\n",
    "print(\"\\nMatrix operation caching:\")\n",
    "\n",
    "# First call (slow - computes)\n",
    "start = time.time()\n",
    "result1 = expensive_matrix_operation(50)\n",
    "time1 = time.time() - start\n",
    "print(f\"  First call: {time1:.3f}s\")\n",
    "\n",
    "# Second call with same parameters (fast - cached)\n",
    "start = time.time()\n",
    "result2 = expensive_matrix_operation(50)\n",
    "time2 = time.time() - start\n",
    "print(f\"  Second call (cached): {time2:.3f}s\")\n",
    "print(f\"  Speedup: {time1/time2:.1f}x\")\n",
    "print(f\"  Results identical: {np.array_equal(result1['matrix'], result2['matrix'])}\")\n",
    "\n",
    "# Different parameters (computes again)\n",
    "start = time.time()\n",
    "result3 = expensive_matrix_operation(50, seed=123)  # Different seed\n",
    "time3 = time.time() - start\n",
    "print(f\"  Different parameters: {time3:.3f}s (new computation)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Disk Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== cache_disk Decorator Examples ===\")\n",
    "\n",
    "# Large-scale computation with disk caching\n",
    "@stx.decorators.cache_disk\n",
    "def generate_synthetic_dataset(n_samples, n_features, noise_level=0.1, random_state=42):\n",
    "    \"\"\"Generate large synthetic dataset for machine learning.\"\"\"\n",
    "    print(f\"Generating dataset: {n_samples} samples, {n_features} features...\")\n",
    "    \n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Generate feature matrix\n",
    "    X = np.random.randn(n_samples, n_features)\n",
    "    \n",
    "    # Generate synthetic target with some structure\n",
    "    true_weights = np.random.randn(n_features)\n",
    "    y = X @ true_weights + noise_level * np.random.randn(n_samples)\n",
    "    \n",
    "    # Add some categorical features\n",
    "    categories = np.random.choice(['A', 'B', 'C'], size=n_samples)\n",
    "    \n",
    "    # Simulate expensive preprocessing\n",
    "    time.sleep(1)  # Simulate computation time\n",
    "    \n",
    "    return {\n",
    "        'X': X,\n",
    "        'y': y,\n",
    "        'categories': categories,\n",
    "        'true_weights': true_weights,\n",
    "        'feature_stats': {\n",
    "            'mean': X.mean(axis=0),\n",
    "            'std': X.std(axis=0)\n",
    "        },\n",
    "        'target_stats': {\n",
    "            'mean': y.mean(),\n",
    "            'std': y.std()\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Test disk caching\n",
    "print(\"Testing disk caching with large dataset:\")\n",
    "\n",
    "# First call (generates and saves to disk)\n",
    "start = time.time()\n",
    "dataset1 = generate_synthetic_dataset(10000, 100)\n",
    "time1 = time.time() - start\n",
    "print(f\"  First call: {time1:.3f}s\")\n",
    "print(f\"  Dataset shape: {dataset1['X'].shape}\")\n",
    "print(f\"  Target mean: {dataset1['target_stats']['mean']:.4f}\")\n",
    "\n",
    "# Second call (loads from disk)\n",
    "start = time.time()\n",
    "dataset2 = generate_synthetic_dataset(10000, 100)\n",
    "time2 = time.time() - start\n",
    "print(f\"\\n  Second call (from cache): {time2:.3f}s\")\n",
    "print(f\"  Speedup: {time1/time2:.1f}x\")\n",
    "print(f\"  Data identical: {np.array_equal(dataset1['X'], dataset2['X'])}\")\n",
    "\n",
    "# Different parameters (new computation)\n",
    "start = time.time()\n",
    "dataset3 = generate_synthetic_dataset(5000, 50)  # Different size\n",
    "time3 = time.time() - start\n",
    "print(f\"\\n  Different parameters: {time3:.3f}s\")\n",
    "print(f\"  New dataset shape: {dataset3['X'].shape}\")\n",
    "\n",
    "# Show cache information\n",
    "cache_dir = Path.home() / \".cache\" / \"scitex\" / \"cache\"\n",
    "if cache_dir.exists():\n",
    "    cache_files = list(cache_dir.rglob(\"*\"))\n",
    "    cache_size = sum(f.stat().st_size for f in cache_files if f.is_file())\n",
    "    print(f\"\\nCache information:\")\n",
    "    print(f\"  Cache directory: {cache_dir}\")\n",
    "    print(f\"  Cache files: {len([f for f in cache_files if f.is_file()])}\")\n",
    "    print(f\"  Total cache size: {cache_size / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Utility Decorators\n",
    "\n",
    "Utility decorators provide additional function enhancements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Timeout Decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== timeout Decorator Examples ===\")\n",
    "\n",
    "# Function with timeout protection\n",
    "@stx.decorators.timeout(seconds=2, error_message=\"Operation timed out after 2 seconds!\")\n",
    "def potentially_slow_operation(duration, computation_type=\"simple\"):\n",
    "    \"\"\"Simulate operation that might take too long.\"\"\"\n",
    "    print(f\"Starting {computation_type} computation for {duration}s...\")\n",
    "    \n",
    "    if computation_type == \"matrix\":\n",
    "        # Simulate matrix computation\n",
    "        for i in range(int(duration * 10)):\n",
    "            _ = np.random.randn(100, 100) @ np.random.randn(100, 100)\n",
    "            time.sleep(0.1)\n",
    "    else:\n",
    "        # Simple sleep\n",
    "        time.sleep(duration)\n",
    "    \n",
    "    return f\"Completed {computation_type} computation in {duration}s\"\n",
    "\n",
    "# Test cases\n",
    "test_cases = [\n",
    "    (1, \"simple\"),    # Should succeed\n",
    "    (1.5, \"matrix\"),  # Should succeed\n",
    "    (3, \"simple\"),    # Should timeout\n",
    "    (2.5, \"matrix\")   # Should timeout\n",
    "]\n",
    "\n",
    "for duration, comp_type in test_cases:\n",
    "    try:\n",
    "        start = time.time()\n",
    "        result = potentially_slow_operation(duration, comp_type)\n",
    "        elapsed = time.time() - start\n",
    "        print(f\"  ✓ Success: {result} (actual time: {elapsed:.2f}s)\")\n",
    "    except Exception as e:\n",
    "        elapsed = time.time() - start\n",
    "        print(f\"  ✗ Failed: {e} (stopped after: {elapsed:.2f}s)\")\n",
    "\n",
    "# Advanced timeout with custom handling\n",
    "@stx.decorators.timeout(seconds=1)\n",
    "def iterative_computation(n_iterations):\n",
    "    \"\"\"Computation that can be interrupted gracefully.\"\"\"\n",
    "    results = []\n",
    "    for i in range(n_iterations):\n",
    "        # Simulate work\n",
    "        result = sum(j**2 for j in range(1000))\n",
    "        results.append(result)\n",
    "        time.sleep(0.1)\n",
    "    return results\n",
    "\n",
    "print(\"\\nIterative computation with timeout:\")\n",
    "try:\n",
    "    results = iterative_computation(20)  # Would take ~2s, but timeout is 1s\n",
    "    print(f\"  Completed all {len(results)} iterations\")\n",
    "except Exception as e:\n",
    "    print(f\"  Computation interrupted: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Deprecation Decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== deprecated Decorator Examples ===\")\n",
    "\n",
    "# Mark functions as deprecated\n",
    "@stx.decorators.deprecated(\"Use calculate_advanced_stats() instead. This function will be removed in v2.0\")\n",
    "def calculate_basic_stats(data):\n",
    "    \"\"\"Old function for basic statistics (deprecated).\"\"\"\n",
    "    return {\n",
    "        'mean': np.mean(data),\n",
    "        'std': np.std(data)\n",
    "    }\n",
    "\n",
    "def calculate_advanced_stats(data):\n",
    "    \"\"\"New improved function for statistics.\"\"\"\n",
    "    return {\n",
    "        'mean': np.mean(data),\n",
    "        'std': np.std(data),\n",
    "        'median': np.median(data),\n",
    "        'q25': np.percentile(data, 25),\n",
    "        'q75': np.percentile(data, 75),\n",
    "        'skewness': float(pd.Series(data).skew()),\n",
    "        'kurtosis': float(pd.Series(data).kurtosis())\n",
    "    }\n",
    "\n",
    "# Test data\n",
    "test_data = np.random.randn(1000)\n",
    "\n",
    "# Capture deprecation warnings\n",
    "import warnings\n",
    "with warnings.catch_warnings(record=True) as w:\n",
    "    warnings.simplefilter(\"always\")\n",
    "    \n",
    "    # Using deprecated function\n",
    "    old_result = calculate_basic_stats(test_data)\n",
    "    print(f\"Old function result: mean={old_result['mean']:.3f}, std={old_result['std']:.3f}\")\n",
    "    \n",
    "    if w:\n",
    "        print(f\"⚠️  Deprecation warning: {w[0].message}\")\n",
    "\n",
    "# Using new function (no warning)\n",
    "new_result = calculate_advanced_stats(test_data)\n",
    "print(f\"\\nNew function result:\")\n",
    "for key, value in new_result.items():\n",
    "    print(f\"  {key}: {value:.3f}\")\n",
    "print(\"✓ No warnings with new function\")\n",
    "\n",
    "# Multiple levels of deprecation\n",
    "@stx.decorators.deprecated(\"This is seriously outdated! Use modern_function() instead.\", \n",
    "                          category=FutureWarning)\n",
    "def very_old_function():\n",
    "    \"\"\"Really old function that should definitely not be used.\"\"\"\n",
    "    return \"This is very old!\"\n",
    "\n",
    "print(\"\\nTesting severe deprecation:\")\n",
    "with warnings.catch_warnings(record=True) as w:\n",
    "    warnings.simplefilter(\"always\")\n",
    "    result = very_old_function()\n",
    "    if w:\n",
    "        print(f\"⚠️  {w[0].category.__name__}: {w[0].message}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Decorator Composition\n",
    "\n",
    "Combining multiple decorators for powerful functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Decorator Composition Examples ===\")\n",
    "\n",
    "# Auto-ordering ensures decorators are applied in optimal order\n",
    "@stx.decorators.cache_mem      # Applied third (outermost)\n",
    "@stx.decorators.batch_fn       # Applied second\n",
    "@stx.decorators.numpy_fn       # Applied first (innermost)\n",
    "def advanced_feature_extraction(signal):\n",
    "    \"\"\"Extract advanced features from signal with full decorator stack.\"\"\"\n",
    "    # Time domain features\n",
    "    time_features = np.array([\n",
    "        signal.mean(),\n",
    "        signal.std(),\n",
    "        np.sqrt(np.mean(signal**2)),  # RMS\n",
    "        np.mean(np.abs(signal)),      # Mean absolute value\n",
    "        signal.max() - signal.min(),  # Range\n",
    "    ])\n",
    "    \n",
    "    # Frequency domain features\n",
    "    fft = np.fft.fft(signal)\n",
    "    power_spectrum = np.abs(fft)**2\n",
    "    freqs = np.fft.fftfreq(len(signal))\n",
    "    \n",
    "    # Find dominant frequency\n",
    "    dominant_freq_idx = np.argmax(power_spectrum[1:len(signal)//2]) + 1\n",
    "    dominant_freq = abs(freqs[dominant_freq_idx])\n",
    "    \n",
    "    freq_features = np.array([\n",
    "        dominant_freq,\n",
    "        power_spectrum.sum(),\n",
    "        np.mean(power_spectrum),\n",
    "        np.std(power_spectrum)\n",
    "    ])\n",
    "    \n",
    "    # Statistical features\n",
    "    stat_features = np.array([\n",
    "        float(pd.Series(signal).skew()),\n",
    "        float(pd.Series(signal).kurtosis()),\n",
    "        np.percentile(signal, 95) - np.percentile(signal, 5)  # 90% range\n",
    "    ])\n",
    "    \n",
    "    return np.concatenate([time_features, freq_features, stat_features])\n",
    "\n",
    "# Generate test signals\n",
    "t = np.linspace(0, 1, 1000)\n",
    "signals = [\n",
    "    np.sin(2 * np.pi * 5 * t) + 0.1 * np.random.randn(1000),   # 5 Hz sine + noise\n",
    "    np.sin(2 * np.pi * 10 * t) + 0.2 * np.random.randn(1000),  # 10 Hz sine + noise\n",
    "    np.sin(2 * np.pi * 15 * t) + np.sin(2 * np.pi * 25 * t),   # Mixed frequencies\n",
    "    np.random.randn(1000),                                       # Pure noise\n",
    "    np.exp(-t * 5) * np.sin(2 * np.pi * 20 * t)                # Damped oscillation\n",
    "]\n",
    "\n",
    "# Extract features (first call - computed and cached)\n",
    "print(\"Extracting features from 5 signals...\")\n",
    "start = time.time()\n",
    "features = advanced_feature_extraction(signals)\n",
    "time1 = time.time() - start\n",
    "print(f\"First extraction: {time1:.3f}s\")\n",
    "print(f\"Feature matrix shape: {features.shape}\")\n",
    "\n",
    "# Second call (cached)\n",
    "start = time.time()\n",
    "features_cached = advanced_feature_extraction(signals)\n",
    "time2 = time.time() - start\n",
    "print(f\"Second extraction (cached): {time2:.3f}s\")\n",
    "print(f\"Speedup: {time1/time2:.1f}x\")\n",
    "\n",
    "# Analyze features\n",
    "feature_names = [\n",
    "    'mean', 'std', 'rms', 'mav', 'range',           # Time domain\n",
    "    'dom_freq', 'total_power', 'mean_power', 'std_power',  # Frequency domain\n",
    "    'skewness', 'kurtosis', 'percentile_range'     # Statistical\n",
    "]\n",
    "\n",
    "print(\"\\nFeature analysis:\")\n",
    "signal_types = ['5Hz sine', '10Hz sine', 'Mixed freq', 'Noise', 'Damped osc']\n",
    "for i, (signal_type, feature_vec) in enumerate(zip(signal_types, features)):\n",
    "    print(f\"\\n{signal_type}:\")\n",
    "    for name, value in zip(feature_names[:5], feature_vec[:5]):  # Show first 5 features\n",
    "        print(f\"  {name:12s}: {value:8.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 PyTorch Integration with Decorators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TORCH_AVAILABLE:\n",
    "    print(\"=== PyTorch Integration Examples ===\")\n",
    "    \n",
    "    # Pre-combined decorator for PyTorch batch processing\n",
    "    @stx.decorators.batch_torch_fn\n",
    "    def neural_network_simulation(x, hidden_dim=64):\n",
    "        \"\"\"Simulate simple neural network forward pass.\"\"\"\n",
    "        input_dim = x.shape[-1]\n",
    "        \n",
    "        # Create random weights (in practice, these would be learned)\n",
    "        W1 = torch.randn(input_dim, hidden_dim) * 0.1\n",
    "        b1 = torch.zeros(hidden_dim)\n",
    "        W2 = torch.randn(hidden_dim, 1) * 0.1\n",
    "        b2 = torch.zeros(1)\n",
    "        \n",
    "        # Forward pass\n",
    "        h = torch.relu(x @ W1 + b1)  # Hidden layer\n",
    "        output = h @ W2 + b2         # Output layer\n",
    "        \n",
    "        return {\n",
    "            'output': output.squeeze(),\n",
    "            'hidden_activation': h,\n",
    "            'input_norm': torch.norm(x),\n",
    "            'hidden_sparsity': (h == 0).float().mean()\n",
    "        }\n",
    "    \n",
    "    # Test with batch of data\n",
    "    batch_data = np.random.randn(10, 20)  # 10 samples, 20 features\n",
    "    \n",
    "    print(f\"Processing batch of {batch_data.shape[0]} samples...\")\n",
    "    results = neural_network_simulation(batch_data, hidden_dim=32)\n",
    "    \n",
    "    print(f\"Results:\")\n",
    "    print(f\"  Output shape: {results[0]['output'].shape}\")\n",
    "    print(f\"  Hidden activation shape: {results[0]['hidden_activation'].shape}\")\n",
    "    print(f\"  Average input norm: {torch.stack([r['input_norm'] for r in results]).mean():.3f}\")\n",
    "    print(f\"  Average sparsity: {torch.stack([r['hidden_sparsity'] for r in results]).mean():.3f}\")\n",
    "    \n",
    "    # Attention mechanism simulation\n",
    "    @stx.decorators.torch_fn\n",
    "    @stx.decorators.cache_mem\n",
    "    def compute_attention_weights(query, key, value, temperature=1.0):\n",
    "        \"\"\"Compute attention weights and apply to values.\"\"\"\n",
    "        # Compute attention scores\n",
    "        scores = torch.matmul(query, key.transpose(-1, -2)) / temperature\n",
    "        \n",
    "        # Apply softmax\n",
    "        attention_weights = torch.softmax(scores, dim=-1)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        attended_output = torch.matmul(attention_weights, value)\n",
    "        \n",
    "        return {\n",
    "            'attention_weights': attention_weights,\n",
    "            'attended_output': attended_output,\n",
    "            'attention_entropy': -(attention_weights * torch.log(attention_weights + 1e-8)).sum(dim=-1).mean()\n",
    "        }\n",
    "    \n",
    "    # Test attention mechanism\n",
    "    seq_len, d_model = 8, 16\n",
    "    query = np.random.randn(seq_len, d_model)\n",
    "    key = np.random.randn(seq_len, d_model)\n",
    "    value = np.random.randn(seq_len, d_model)\n",
    "    \n",
    "    attention_result = compute_attention_weights(query, key, value, temperature=0.1)\n",
    "    \n",
    "    print(f\"\\nAttention mechanism:\")\n",
    "    print(f\"  Attention weights shape: {attention_result['attention_weights'].shape}\")\n",
    "    print(f\"  Output shape: {attention_result['attended_output'].shape}\")\n",
    "    print(f\"  Attention entropy: {attention_result['attention_entropy']:.3f}\")\n",
    "    print(f\"  Weights sum to 1: {torch.allclose(attention_result['attention_weights'].sum(dim=-1), torch.ones(seq_len))}\")\n",
    "    \n",
    "else:\n",
    "    print(\"PyTorch not available, skipping PyTorch integration examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Real-World Applications\n",
    "\n",
    "Complete examples showing how decorators work in real data science workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Real-World Application: Data Processing Pipeline ===\")\n",
    "\n",
    "class AdvancedDataProcessor:\n",
    "    \"\"\"Complete data processing pipeline using decorators.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.processing_steps = []\n",
    "    \n",
    "    @stx.decorators.cache_disk\n",
    "    @stx.decorators.pandas_fn\n",
    "    def load_and_validate_data(self, data, validation_rules=None):\n",
    "        \"\"\"Load data and apply validation rules.\"\"\"\n",
    "        self.processing_steps.append(\"Data loading and validation\")\n",
    "        \n",
    "        # Simulate data loading delay\n",
    "        time.sleep(0.2)\n",
    "        \n",
    "        validated_data = data.copy()\n",
    "        validation_report = {\n",
    "            'total_rows': len(validated_data),\n",
    "            'total_columns': len(validated_data.columns),\n",
    "            'missing_values': validated_data.isnull().sum().sum(),\n",
    "            'duplicate_rows': validated_data.duplicated().sum(),\n",
    "            'memory_usage_mb': validated_data.memory_usage(deep=True).sum() / 1024**2\n",
    "        }\n",
    "        \n",
    "        # Apply validation rules if provided\n",
    "        if validation_rules:\n",
    "            for rule in validation_rules:\n",
    "                # Simple validation rule application\n",
    "                if rule['type'] == 'range':\n",
    "                    col = rule['column']\n",
    "                    if col in validated_data.columns:\n",
    "                        mask = (validated_data[col] >= rule['min']) & (validated_data[col] <= rule['max'])\n",
    "                        validation_report[f'{col}_out_of_range'] = (~mask).sum()\n",
    "        \n",
    "        return validated_data, validation_report\n",
    "    \n",
    "    @stx.decorators.batch_fn\n",
    "    @stx.decorators.numpy_fn\n",
    "    def preprocess_features(self, feature_vector, method='standardize'):\n",
    "        \"\"\"Preprocess individual feature vectors.\"\"\"\n",
    "        if method == 'standardize':\n",
    "            # Z-score normalization\n",
    "            mean = feature_vector.mean()\n",
    "            std = feature_vector.std()\n",
    "            if std > 0:\n",
    "                processed = (feature_vector - mean) / std\n",
    "            else:\n",
    "                processed = feature_vector - mean\n",
    "        elif method == 'minmax':\n",
    "            # Min-max scaling\n",
    "            min_val = feature_vector.min()\n",
    "            max_val = feature_vector.max()\n",
    "            if max_val > min_val:\n",
    "                processed = (feature_vector - min_val) / (max_val - min_val)\n",
    "            else:\n",
    "                processed = feature_vector - min_val\n",
    "        else:\n",
    "            processed = feature_vector\n",
    "        \n",
    "        return {\n",
    "            'processed': processed,\n",
    "            'original_mean': feature_vector.mean(),\n",
    "            'original_std': feature_vector.std(),\n",
    "            'processed_mean': processed.mean(),\n",
    "            'processed_std': processed.std()\n",
    "        }\n",
    "    \n",
    "    @stx.decorators.timeout(seconds=10)\n",
    "    @stx.decorators.cache_mem\n",
    "    def extract_engineered_features(self, data):\n",
    "        \"\"\"Extract engineered features from processed data.\"\"\"\n",
    "        self.processing_steps.append(\"Feature engineering\")\n",
    "        \n",
    "        numeric_cols = data.select_dtypes(include=[np.number]).columns\n",
    "        \n",
    "        engineered_features = pd.DataFrame(index=data.index)\n",
    "        \n",
    "        # Statistical features\n",
    "        for col in numeric_cols:\n",
    "            values = data[col].dropna()\n",
    "            if len(values) > 0:\n",
    "                engineered_features[f'{col}_mean'] = values.mean()\n",
    "                engineered_features[f'{col}_std'] = values.std()\n",
    "                engineered_features[f'{col}_skew'] = values.skew()\n",
    "                engineered_features[f'{col}_kurt'] = values.kurtosis()\n",
    "        \n",
    "        # Interaction features (sample)\n",
    "        if len(numeric_cols) >= 2:\n",
    "            col1, col2 = numeric_cols[0], numeric_cols[1]\n",
    "            engineered_features[f'{col1}_x_{col2}'] = data[col1] * data[col2]\n",
    "            engineered_features[f'{col1}_div_{col2}'] = data[col1] / (data[col2] + 1e-8)\n",
    "        \n",
    "        return engineered_features\n",
    "    \n",
    "    def full_pipeline(self, raw_data, preprocessing_method='standardize'):\n",
    "        \"\"\"Execute complete data processing pipeline.\"\"\"\n",
    "        print(\"Executing full data processing pipeline...\")\n",
    "        \n",
    "        # Step 1: Load and validate\n",
    "        validation_rules = [\n",
    "            {'type': 'range', 'column': 'feature1', 'min': -10, 'max': 10}\n",
    "        ]\n",
    "        \n",
    "        validated_data, validation_report = self.load_and_validate_data(\n",
    "            raw_data, validation_rules\n",
    "        )\n",
    "        \n",
    "        print(f\"  ✓ Data validation complete\")\n",
    "        print(f\"    Rows: {validation_report['total_rows']}\")\n",
    "        print(f\"    Missing values: {validation_report['missing_values']}\")\n",
    "        \n",
    "        # Step 2: Preprocess features\n",
    "        numeric_data = validated_data.select_dtypes(include=[np.number])\n",
    "        if len(numeric_data.columns) > 0:\n",
    "            preprocessing_results = self.preprocess_features(\n",
    "                numeric_data.values, method=preprocessing_method\n",
    "            )\n",
    "            \n",
    "            print(f\"  ✓ Feature preprocessing complete ({preprocessing_method})\")\n",
    "            print(f\"    Original mean: {np.mean([r['original_mean'] for r in preprocessing_results]):.3f}\")\n",
    "            print(f\"    Processed mean: {np.mean([r['processed_mean'] for r in preprocessing_results]):.3f}\")\n",
    "        \n",
    "        # Step 3: Feature engineering\n",
    "        engineered_features = self.extract_engineered_features(validated_data)\n",
    "        \n",
    "        print(f\"  ✓ Feature engineering complete\")\n",
    "        print(f\"    Original features: {len(validated_data.columns)}\")\n",
    "        print(f\"    Engineered features: {len(engineered_features.columns)}\")\n",
    "        \n",
    "        return {\n",
    "            'validated_data': validated_data,\n",
    "            'validation_report': validation_report,\n",
    "            'preprocessing_results': preprocessing_results if len(numeric_data.columns) > 0 else None,\n",
    "            'engineered_features': engineered_features,\n",
    "            'processing_steps': self.processing_steps.copy()\n",
    "        }\n",
    "\n",
    "# Test the complete pipeline\n",
    "processor = AdvancedDataProcessor()\n",
    "\n",
    "# Generate sample dataset\n",
    "np.random.seed(42)\n",
    "sample_data = pd.DataFrame({\n",
    "    'feature1': np.random.randn(1000),\n",
    "    'feature2': np.random.randn(1000) * 2 + 1,\n",
    "    'feature3': np.random.exponential(2, 1000),\n",
    "    'category': np.random.choice(['A', 'B', 'C'], 1000),\n",
    "    'target': np.random.randint(0, 2, 1000)\n",
    "})\n",
    "\n",
    "# Add some missing values\n",
    "sample_data.loc[sample_data.sample(50).index, 'feature1'] = np.nan\n",
    "\n",
    "# Run pipeline\n",
    "start_time = time.time()\n",
    "results = processor.full_pipeline(sample_data, preprocessing_method='standardize')\n",
    "pipeline_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nPipeline completed in {pipeline_time:.3f}s\")\n",
    "print(f\"Processing steps executed: {len(results['processing_steps'])}\")\n",
    "\n",
    "# Run again to test caching\n",
    "start_time = time.time()\n",
    "results_cached = processor.full_pipeline(sample_data, preprocessing_method='standardize')\n",
    "cached_time = time.time() - start_time\n",
    "\n",
    "print(f\"Second run (cached): {cached_time:.3f}s\")\n",
    "print(f\"Speedup from caching: {pipeline_time/cached_time:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Advanced Patterns\n",
    "\n",
    "Advanced decorator patterns and custom decorator creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Advanced Decorator Patterns ===\")\n",
    "\n",
    "# Custom decorator factory\n",
    "def robust_processor(timeout_seconds=30, use_cache=True, handle_errors=True):\n",
    "    \"\"\"Create a robust processing decorator with multiple features.\"\"\"\n",
    "    def decorator(func):\n",
    "        # Build decorator chain\n",
    "        decorated_func = func\n",
    "        \n",
    "        # Add error handling\n",
    "        if handle_errors:\n",
    "            def error_handler(*args, **kwargs):\n",
    "                try:\n",
    "                    return decorated_func(*args, **kwargs)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in {func.__name__}: {e}\")\n",
    "                    return None\n",
    "            decorated_func = error_handler\n",
    "        \n",
    "        # Add caching\n",
    "        if use_cache:\n",
    "            decorated_func = stx.decorators.cache_mem(decorated_func)\n",
    "        \n",
    "        # Add timeout\n",
    "        decorated_func = stx.decorators.timeout(seconds=timeout_seconds)(decorated_func)\n",
    "        \n",
    "        # Add type conversion\n",
    "        decorated_func = stx.decorators.numpy_fn(decorated_func)\n",
    "        \n",
    "        return decorated_func\n",
    "    \n",
    "    return decorator\n",
    "\n",
    "# Use custom decorator\n",
    "@robust_processor(timeout_seconds=5, use_cache=True, handle_errors=True)\n",
    "def complex_signal_analysis(signal):\n",
    "    \"\"\"Perform complex signal analysis with full robustness.\"\"\"\n",
    "    # Simulate complex computation\n",
    "    time.sleep(0.1)\n",
    "    \n",
    "    # Multiple analysis steps\n",
    "    fft = np.fft.fft(signal)\n",
    "    power_spectrum = np.abs(fft)**2\n",
    "    \n",
    "    # Statistical analysis\n",
    "    stats = {\n",
    "        'mean': signal.mean(),\n",
    "        'std': signal.std(),\n",
    "        'energy': np.sum(signal**2),\n",
    "        'peak_frequency': np.argmax(power_spectrum[:len(signal)//2]),\n",
    "        'spectral_centroid': np.sum(np.arange(len(power_spectrum)) * power_spectrum) / np.sum(power_spectrum)\n",
    "    }\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# Test robust processor\n",
    "test_signal = np.sin(2 * np.pi * 10 * np.linspace(0, 1, 1000)) + 0.1 * np.random.randn(1000)\n",
    "\n",
    "print(\"Testing robust processor:\")\n",
    "result = complex_signal_analysis(test_signal)\n",
    "if result:\n",
    "    print(f\"  Analysis successful:\")\n",
    "    for key, value in result.items():\n",
    "        print(f\"    {key}: {value:.3f}\")\n",
    "\n",
    "# # Test with problematic input(should handle gracefully)\n",
    "print(\"\\nTesting error handling:\")\n",
    "problematic_input = \"not a valid signal\"  # This will cause an error\n",
    "result = complex_signal_analysis(problematic_input)\n",
    "print(f\"Result with bad input: {result}\")\n",
    "\n",
    "# Decorator introspection\n",
    "def analyze_function_decorators(func):\n",
    "    \"\"\"Analyze the decorator chain of a function.\"\"\"\n",
    "    print(f\"\\nAnalyzing function: {func.__name__}\")\n",
    "    print(f\"Module: {func.__module__}\")\n",
    "    print(f\"Docstring: {func.__doc__[:50]}...\" if func.__doc__ else \"No docstring\")\n",
    "    \n",
    "    # Check for wrapped functions (decorator chain)\n",
    "    current = func\n",
    "    depth = 0\n",
    "    decorators_found = []\n",
    "    \n",
    "    while hasattr(current, '__wrapped__'):\n",
    "        decorator_name = getattr(current, '__class__', {}).get('__name__', 'Unknown')\n",
    "        decorators_found.append(f\"Level {depth}: {decorator_name}\")\n",
    "        current = current.__wrapped__\n",
    "        depth += 1\n",
    "        if depth > 10:  # Safety limit\n",
    "            break\n",
    "    \n",
    "    if decorators_found:\n",
    "        print(\"Decorator chain detected:\")\n",
    "        for decorator in decorators_found:\n",
    "            print(f\"  {decorator}\")\n",
    "    else:\n",
    "        print(\"No decorator chain detected\")\n",
    "    \n",
    "    return depth\n",
    "\n",
    "# Analyze our decorated function\n",
    "decorator_depth = analyze_function_decorators(complex_signal_analysis)\n",
    "print(f\"Total decorator depth: {decorator_depth}\")\n",
    "\n",
    "# Performance comparison\n",
    "print(\"\\nPerformance comparison:\")\n",
    "\n",
    "# Undecorated version\n",
    "def simple_analysis(signal):\n",
    "    time.sleep(0.1)\n",
    "    return {'mean': np.mean(signal), 'std': np.std(signal)}\n",
    "\n",
    "# Test performance\n",
    "test_signals = [np.random.randn(1000) for _ in range(3)]\n",
    "\n",
    "# Time decorated version (first call)\n",
    "start = time.time()\n",
    "for signal in test_signals:\n",
    "    _ = complex_signal_analysis(signal)\n",
    "decorated_time = time.time() - start\n",
    "\n",
    "# Time decorated version (second call - cached)\n",
    "start = time.time()\n",
    "for signal in test_signals:\n",
    "    _ = complex_signal_analysis(signal)\n",
    "cached_time = time.time() - start\n",
    "\n",
    "print(f\"  Decorated (first call): {decorated_time:.3f}s\")\n",
    "print(f\"  Decorated (cached): {cached_time:.3f}s\")\n",
    "print(f\"  Cache speedup: {decorated_time/cached_time:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Complete Processing Pipeline\n",
    "\n",
    "A comprehensive example showing all decorators working together in a machine learning pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Complete ML Pipeline with Decorators ===\")\n",
    "\n",
    "class MLPipelineWithDecorators:\n",
    "    \"\"\"Complete machine learning pipeline using all decorator features.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.pipeline_steps = []\n",
    "        self.model_cache = {}\n",
    "    \n",
    "    @stx.decorators.cache_disk\n",
    "    @stx.decorators.timeout(seconds=30)\n",
    "    @stx.decorators.pandas_fn\n",
    "    def load_and_clean_data(self, data, cleaning_strategy='default'):\n",
    "        \"\"\"Load and clean raw data with caching.\"\"\"\n",
    "        self.pipeline_steps.append(f\"Data loading ({cleaning_strategy})\")\n",
    "        \n",
    "        # Simulate data loading time\n",
    "        time.sleep(0.2)\n",
    "        \n",
    "        cleaned_data = data.copy()\n",
    "        \n",
    "        if cleaning_strategy == 'default':\n",
    "            # Remove duplicates\n",
    "            cleaned_data = cleaned_data.drop_duplicates()\n",
    "            \n",
    "            # Handle missing values\n",
    "            numeric_cols = cleaned_data.select_dtypes(include=[np.number]).columns\n",
    "            for col in numeric_cols:\n",
    "                cleaned_data[col] = cleaned_data[col].fillna(cleaned_data[col].median())\n",
    "            \n",
    "            categorical_cols = cleaned_data.select_dtypes(include=['object']).columns\n",
    "            for col in categorical_cols:\n",
    "                cleaned_data[col] = cleaned_data[col].fillna(cleaned_data[col].mode()[0] if not cleaned_data[col].mode().empty else 'Unknown')\n",
    "        \n",
    "        return cleaned_data\n",
    "    \n",
    "    @stx.decorators.batch_fn\n",
    "    @stx.decorators.numpy_fn\n",
    "    @stx.decorators.cache_mem\n",
    "    def extract_ml_features(self, sample):\n",
    "        \"\"\"Extract machine learning features from individual samples.\"\"\"\n",
    "        # Statistical features\n",
    "        basic_stats = np.array([\n",
    "            sample.mean(),\n",
    "            sample.std(),\n",
    "            np.median(sample),\n",
    "            np.percentile(sample, 25),\n",
    "            np.percentile(sample, 75)\n",
    "        ])\n",
    "        \n",
    "        # Distribution features\n",
    "        dist_features = np.array([\n",
    "            float(pd.Series(sample).skew()),\n",
    "            float(pd.Series(sample).kurtosis()),\n",
    "            sample.max() - sample.min(),\n",
    "            np.sum(np.abs(sample))\n",
    "        ])\n",
    "        \n",
    "        # Relative features\n",
    "        if len(sample) > 1:\n",
    "            diff_features = np.array([\n",
    "                np.mean(np.diff(sample)),\n",
    "                np.std(np.diff(sample)),\n",
    "                np.sum(np.abs(np.diff(sample)))\n",
    "            ])\n",
    "        else:\n",
    "            diff_features = np.zeros(3)\n",
    "        \n",
    "        return np.concatenate([basic_stats, dist_features, diff_features])\n",
    "    \n",
    "    @stx.decorators.timeout(seconds=60)\n",
    "    def train_model(self, X, y, model_type='random_forest'):\n",
    "        \"\"\"Train machine learning model with timeout protection.\"\"\"\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        from sklearn.linear_model import LogisticRegression\n",
    "        from sklearn.model_selection import cross_val_score\n",
    "        \n",
    "        self.pipeline_steps.append(f\"Model training ({model_type})\")\n",
    "        \n",
    "        # Select model\n",
    "        if model_type == 'random_forest':\n",
    "            model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        elif model_type == 'logistic':\n",
    "            model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model type: {model_type}\")\n",
    "        \n",
    "        # Train model\n",
    "        model.fit(X, y)\n",
    "        \n",
    "        # Cross-validation\n",
    "        cv_scores = cross_val_score(model, X, y, cv=5)\n",
    "        \n",
    "        # Cache model\n",
    "        model_key = f\"{model_type}_{hash(str(X.shape) + str(y.shape))}\"\n",
    "        self.model_cache[model_key] = model\n",
    "        \n",
    "        return {\n",
    "            'model': model,\n",
    "            'cv_scores': cv_scores,\n",
    "            'cv_mean': cv_scores.mean(),\n",
    "            'cv_std': cv_scores.std(),\n",
    "            'feature_importance': getattr(model, 'feature_importances_', None),\n",
    "            'model_key': model_key\n",
    "        }\n",
    "    \n",
    "    if TORCH_AVAILABLE:\n",
    "        @stx.decorators.batch_torch_fn\n",
    "        @stx.decorators.cache_mem\n",
    "        def neural_feature_extraction(self, x):\n",
    "            \"\"\"Extract features using simple neural network.\"\"\"\n",
    "            input_dim = x.shape[-1]\n",
    "            hidden_dim = min(64, input_dim * 2)\n",
    "            \n",
    "            # Simple autoencoder-style feature extraction\n",
    "            W_encode = torch.randn(input_dim, hidden_dim) * 0.1\n",
    "            W_decode = torch.randn(hidden_dim, input_dim) * 0.1\n",
    "            \n",
    "            # Encode\n",
    "            encoded = torch.relu(x @ W_encode)\n",
    "            \n",
    "            # Decode (for reconstruction error)\n",
    "            decoded = encoded @ W_decode\n",
    "            reconstruction_error = torch.mean((x - decoded)**2)\n",
    "            \n",
    "            return {\n",
    "                'encoded_features': encoded,\n",
    "                'reconstruction_error': reconstruction_error,\n",
    "                'sparsity': (encoded == 0).float().mean()\n",
    "            }\n",
    "    \n",
    "    def complete_pipeline(self, raw_data, target_column, model_types=['random_forest']):\n",
    "        \"\"\"Execute complete ML pipeline.\"\"\"\n",
    "        print(\"\\nExecuting complete ML pipeline...\")\n",
    "        \n",
    "        # Step 1: Data cleaning\n",
    "        cleaned_data = self.load_and_clean_data(raw_data)\n",
    "        print(f\"  ✓ Data cleaning: {len(raw_data)} -> {len(cleaned_data)} samples\")\n",
    "        \n",
    "        # Step 2: Feature extraction\n",
    "        feature_columns = [col for col in cleaned_data.columns if col != target_column]\n",
    "        numeric_features = cleaned_data[feature_columns].select_dtypes(include=[np.number])\n",
    "        \n",
    "        if len(numeric_features.columns) > 0:\n",
    "            extracted_features = self.extract_ml_features(numeric_features.values)\n",
    "            print(f\"  ✓ Feature extraction: {numeric_features.shape[1]} -> {extracted_features.shape[1]} features\")\n",
    "            \n",
    "            # Add neural features if PyTorch is available\n",
    "            if TORCH_AVAILABLE and len(numeric_features.columns) >= 2:\n",
    "                neural_results = self.neural_feature_extraction(numeric_features.values)\n",
    "                neural_features = neural_results[0]['encoded_features'].numpy()\n",
    "                \n",
    "                # Combine features\n",
    "                combined_features = np.hstack([extracted_features, neural_features])\n",
    "                print(f\"  ✓ Neural features added: {extracted_features.shape[1]} + {neural_features.shape[1]} = {combined_features.shape[1]}\")\n",
    "                X = combined_features\n",
    "            else:\n",
    "                X = extracted_features\n",
    "        else:\n",
    "            print(\"  ⚠ No numeric features found\")\n",
    "            return None\n",
    "        \n",
    "        # Step 3: Model training\n",
    "        y = cleaned_data[target_column].values\n",
    "        \n",
    "        models = {}\n",
    "        for model_type in model_types:\n",
    "            try:\n",
    "                model_result = self.train_model(X, y, model_type)\n",
    "                models[model_type] = model_result\n",
    "                print(f\"  ✓ {model_type}: CV score = {model_result['cv_mean']:.3f} ± {model_result['cv_std']:.3f}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  ✗ {model_type} failed: {e}\")\n",
    "        \n",
    "        return {\n",
    "            'cleaned_data': cleaned_data,\n",
    "            'features': X,\n",
    "            'target': y,\n",
    "            'models': models,\n",
    "            'pipeline_steps': self.pipeline_steps.copy(),\n",
    "            'feature_shape': X.shape\n",
    "        }\n",
    "\n",
    "# Test complete pipeline\n",
    "pipeline = MLPipelineWithDecorators()\n",
    "\n",
    "# Generate comprehensive test dataset\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "\n",
    "# Create correlated features\n",
    "base_features = np.random.randn(n_samples, 3)\n",
    "feature_matrix = np.column_stack([\n",
    "    base_features,\n",
    "    base_features[:, 0] * 2 + np.random.randn(n_samples) * 0.1,  # Correlated\n",
    "    base_features[:, 1] ** 2,  # Non-linear\n",
    "    np.random.exponential(1, n_samples),  # Different distribution\n",
    "])\n",
    "\n",
    "# Create target with some structure\n",
    "target = (feature_matrix[:, 0] + 0.5 * feature_matrix[:, 2] - 0.3 * feature_matrix[:, 4] > 0).astype(int)\n",
    "\n",
    "# Add some noise and missing values\n",
    "feature_matrix[np.random.choice(n_samples, 50), 0] = np.nan\n",
    "\n",
    "# Create DataFrame\n",
    "ml_data = pd.DataFrame(\n",
    "    feature_matrix,\n",
    "    columns=[f'feature_{i}' for i in range(feature_matrix.shape[1])]\n",
    ")\n",
    "ml_data['target'] = target\n",
    "ml_data['category'] = np.random.choice(['A', 'B', 'C'], n_samples)\n",
    "\n",
    "# Run complete pipeline\n",
    "start_time = time.time()\n",
    "results = pipeline.complete_pipeline(\n",
    "    ml_data, \n",
    "    target_column='target',\n",
    "    model_types=['random_forest', 'logistic']\n",
    ")\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "if results:\n",
    "    print(f\"\\nPipeline completed in {total_time:.2f}s\")\n",
    "    print(f\"Final feature shape: {results['feature_shape']}\")\n",
    "    print(f\"Best model: {max(results['models'].keys(), key=lambda k: results['models'][k]['cv_mean'])}\")\n",
    "    \n",
    "    # Show model comparison\n",
    "    print(\"\\nModel comparison:\")\n",
    "    for model_name, model_result in results['models'].items():\n",
    "        print(f\"  {model_name:15s}: {model_result['cv_mean']:.3f} ± {model_result['cv_std']:.3f}\")\n",
    "    \n",
    "    print(f\"\\nPipeline steps executed: {len(results['pipeline_steps'])}\")\n",
    "    for i, step in enumerate(results['pipeline_steps'], 1):\n",
    "        print(f\"  {i}. {step}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This comprehensive tutorial has demonstrated the full power of the SciTeX decorators module:\n",
    "\n",
    "### Key Features Covered:\n",
    "1. **Type Conversion** - Automatic conversion to NumPy, PyTorch, and Pandas formats\n",
    "2. **Batch Processing** - Seamlessly process individual samples or batches\n",
    "3. **Caching** - Memory and disk caching for performance optimization\n",
    "4. **Utilities** - Timeout protection, deprecation warnings, and error handling\n",
    "5. **Auto-Ordering** - Automatic optimal ordering of decorator chains\n",
    "6. **Real-World Integration** - Complete ML and data processing pipelines\n",
    "\n",
    "### Best Practices:\n",
    "1. **Always enable auto-ordering** with `stx.decorators.enable_auto_order()`\n",
    "2. **Use appropriate caching** - `@cache_mem` for small/frequent, `@cache_disk` for large/persistent\n",
    "3. **Combine decorators wisely** - type conversion → batch processing → caching → utilities\n",
    "4. **Add timeout protection** for long-running operations\n",
    "5. **Handle errors gracefully** with try-catch or custom error handlers\n",
    "\n",
    "### Performance Benefits:\n",
    "- **Caching**: 10-100x speedup for repeated computations\n",
    "- **Batch processing**: Automatic vectorization without code changes\n",
    "- **Type safety**: Seamless integration between NumPy, PyTorch, and Pandas\n",
    "- **Robustness**: Timeout protection and error handling\n",
    "\n",
    "The decorators module transforms simple functions into robust, high-performance, and type-safe components for scientific computing and machine learning workflows."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}