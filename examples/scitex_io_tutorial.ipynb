{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SciTeX IO Module Tutorial\n",
    "\n",
    "This notebook demonstrates the powerful file I/O capabilities of the scitex.io module.\n",
    "\n",
    "## Key Features\n",
    "\n",
    "- **Universal Load/Save**: Automatic format detection for 20+ file types\n",
    "- **Scientific Data Support**: NumPy, MATLAB, HDF5, EEG formats\n",
    "- **Configuration Management**: YAML config loading with debug modes\n",
    "- **Smart File Patterns**: Enhanced glob with parsing\n",
    "- **Caching System**: Speed up expensive computations\n",
    "- **matplotlib Integration**: Plots with automatic data export\n",
    "\n",
    "Let's explore these capabilities with practical examples!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the scitex io module\n",
    "import scitex as stx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "# Create a temporary directory for examples\n",
    "temp_dir = Path(tempfile.mkdtemp())\n",
    "print(f\"Working in temporary directory: {temp_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Universal Load/Save Operations\n",
    "\n",
    "The most powerful feature of scitex.io is its universal `load()` and `save()` functions that automatically detect file formats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data\n",
    "data_dict = {\n",
    "    'numbers': np.random.randn(100),\n",
    "    'dataframe': pd.DataFrame({\n",
    "        'A': np.random.randn(50),\n",
    "        'B': np.random.randint(0, 10, 50),\n",
    "        'C': np.random.choice(['cat', 'dog', 'bird'], 50)\n",
    "    }),\n",
    "    'metadata': {'experiment': 'tutorial', 'version': 1.0}\n",
    "}\n",
    "\n",
    "print(\"Created sample data:\")\n",
    "print(f\"- Array shape: {data_dict['numbers'].shape}\")\n",
    "print(f\"- DataFrame shape: {data_dict['dataframe'].shape}\")\n",
    "print(f\"- Metadata: {data_dict['metadata']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to different formats - scitex automatically detects the format!\n",
    "formats_to_demo = {\n",
    "    'pickle': temp_dir / 'data.pkl',\n",
    "    'json': temp_dir / 'metadata.json', \n",
    "    'csv': temp_dir / 'dataframe.csv',\n",
    "    'numpy': temp_dir / 'numbers.npy',\n",
    "    'yaml': temp_dir / 'config.yaml'\n",
    "}\n",
    "\n",
    "# Save complete data as pickle\n",
    "stx.io.save(data_dict, formats_to_demo['pickle'])\n",
    "print(f\"✅ Saved complete data to: {formats_to_demo['pickle'].name}\")\n",
    "\n",
    "# Save just metadata as JSON\n",
    "stx.io.save(data_dict['metadata'], formats_to_demo['json'])\n",
    "print(f\"✅ Saved metadata to: {formats_to_demo['json'].name}\")\n",
    "\n",
    "# Save DataFrame as CSV\n",
    "stx.io.save(data_dict['dataframe'], formats_to_demo['csv'])\n",
    "print(f\"✅ Saved DataFrame to: {formats_to_demo['csv'].name}\")\n",
    "\n",
    "# Save array as NumPy\n",
    "stx.io.save(data_dict['numbers'], formats_to_demo['numpy'])\n",
    "print(f\"✅ Saved array to: {formats_to_demo['numpy'].name}\")\n",
    "\n",
    "# Save config as YAML\n",
    "config = {'data_path': './data/', 'batch_size': 32, 'learning_rate': 0.001}\n",
    "stx.io.save(config, formats_to_demo['yaml'])\n",
    "print(f\"✅ Saved config to: {formats_to_demo['yaml'].name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load back the data - scitex automatically detects formats!\n",
    "print(\"Loading data back:\")\n",
    "\n",
    "loaded_data = stx.io.load(formats_to_demo['pickle'])\n",
    "print(f\"✅ Loaded complete data: {type(loaded_data)}\")\n",
    "\n",
    "loaded_metadata = stx.io.load(formats_to_demo['json'])\n",
    "print(f\"✅ Loaded metadata: {loaded_metadata}\")\n",
    "\n",
    "loaded_df = stx.io.load(formats_to_demo['csv'])\n",
    "print(f\"✅ Loaded DataFrame: {loaded_df.shape}\")\n",
    "\n",
    "loaded_array = stx.io.load(formats_to_demo['numpy'])\n",
    "print(f\"✅ Loaded array: {loaded_array.shape}\")\n",
    "\n",
    "loaded_config = stx.io.load(formats_to_demo['yaml'])\n",
    "print(f\"✅ Loaded config: {loaded_config}\")\n",
    "\n",
    "# Verify data integrity\n",
    "print(\"\\nData integrity check:\")\n",
    "print(f\"Arrays equal: {np.allclose(data_dict['numbers'], loaded_array)}\")\n",
    "print(f\"DataFrames equal: {data_dict['dataframe'].equals(loaded_df)}\")\n",
    "print(f\"Metadata equal: {data_dict['metadata'] == loaded_metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Enhanced File Pattern Matching\n",
    "\n",
    "SciTeX provides powerful file pattern matching with parsing capabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample file structure for demonstration\n",
    "sample_files = [\n",
    "    'subject_001/session_01.csv',\n",
    "    'subject_001/session_02.csv', \n",
    "    'subject_002/session_01.csv',\n",
    "    'subject_002/session_02.csv',\n",
    "    'subject_003/session_01.csv',\n",
    "    'train/data_001.txt',\n",
    "    'train/data_002.txt',\n",
    "    'test/data_001.txt',\n",
    "    'validation/data_001.txt'\n",
    "]\n",
    "\n",
    "# Create the directory structure\n",
    "for file_path in sample_files:\n",
    "    full_path = temp_dir / file_path\n",
    "    full_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    # Create dummy data\n",
    "    dummy_data = pd.DataFrame({\n",
    "        'x': np.random.randn(10), \n",
    "        'y': np.random.randn(10)\n",
    "    })\n",
    "    full_path.write_text(dummy_data.to_csv(index=False))\n",
    "\n",
    "print(f\"Created {len(sample_files)} sample files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to temp directory for glob examples\n",
    "original_cwd = os.getcwd()\n",
    "os.chdir(temp_dir)\n",
    "\n",
    "try:\n",
    "    # Basic glob with natural sorting\n",
    "    all_csv = stx.io.glob('subject_*/session_*.csv')\n",
    "    print(f\"Found {len(all_csv)} CSV files:\")\n",
    "    for f in all_csv:\n",
    "        print(f\"  {f}\")\n",
    "        \n",
    "    # Curly brace expansion\n",
    "    train_test_files = stx.io.glob('{train,test}/data_*.txt')\n",
    "    print(f\"\\nFound {len(train_test_files)} train/test files:\")\n",
    "    for f in train_test_files:\n",
    "        print(f\"  {f}\")\n",
    "        \n",
    "    # Glob with parsing - extract parameters from filenames\n",
    "    paths, parsed = stx.io.glob('subject_{subject_id}/session_{session_num}.csv', parse=True)\n",
    "    print(f\"\\nParsed {len(paths)} files with parameters:\")\n",
    "    for path, params in zip(paths, parsed):\n",
    "        print(f\"  {path} → Subject: {params['subject_id']}, Session: {params['session_num']}\")\n",
    "        \n",
    "finally:\n",
    "    os.chdir(original_cwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuration Management\n",
    "\n",
    "SciTeX provides powerful YAML-based configuration management:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a config directory structure\n",
    "config_dir = temp_dir / 'config'\n",
    "config_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Create different config files\n",
    "configs = {\n",
    "    'PATH.yaml': {\n",
    "        'data_dir': './data',\n",
    "        'output_dir': './results',\n",
    "        'model_dir': './models',\n",
    "        'log_dir': './logs'\n",
    "    },\n",
    "    'PARAMS.yaml': {\n",
    "        'model': {\n",
    "            'learning_rate': 0.001,\n",
    "            'batch_size': 32,\n",
    "            'epochs': 100\n",
    "        },\n",
    "        'data': {\n",
    "            'train_split': 0.8,\n",
    "            'validation_split': 0.1,\n",
    "            'test_split': 0.1\n",
    "        },\n",
    "        'experiment': {\n",
    "            'name': 'baseline_model',\n",
    "            'seed': 42,\n",
    "            'device': 'auto'\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save configuration files\n",
    "for filename, config_data in configs.items():\n",
    "    config_path = config_dir / filename\n",
    "    stx.io.save(config_data, config_path)\n",
    "    print(f\"✅ Created config: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to temp directory and load configurations\n",
    "os.chdir(temp_dir)\n",
    "\n",
    "try:\n",
    "    # Load all configurations at once\n",
    "    CONFIG = stx.io.load_configs()\n",
    "    \n",
    "    print(\"Loaded configurations:\")\n",
    "    print(f\"Data directory: {CONFIG.PATH.data_dir}\")\n",
    "    print(f\"Learning rate: {CONFIG.PARAMS.model.learning_rate}\")\n",
    "    print(f\"Batch size: {CONFIG.PARAMS.model.batch_size}\")\n",
    "    print(f\"Train split: {CONFIG.PARAMS.data.train_split}\")\n",
    "    print(f\"Experiment name: {CONFIG.PARAMS.experiment.name}\")\n",
    "    \n",
    "    # The CONFIG object supports both dict and dot notation\n",
    "    print(\"\\nAccess methods:\")\n",
    "    print(f\"Dict style: {CONFIG['PARAMS']['model']['learning_rate']}\")\n",
    "    print(f\"Dot style: {CONFIG.PARAMS.model.learning_rate}\")\n",
    "    \n",
    "finally:\n",
    "    os.chdir(original_cwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Caching for Expensive Computations\n",
    "\n",
    "SciTeX provides a simple but effective caching system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expensive_computation(n=1000000):\n",
    "    \"\"\"Simulate an expensive computation.\"\"\"\n",
    "    print(f\"Running expensive computation with n={n}...\")\n",
    "    import time\n",
    "    time.sleep(2)  # Simulate computation time\n",
    "    result = np.sum(np.random.randn(n) ** 2)\n",
    "    return result\n",
    "\n",
    "# First run - not cached\n",
    "import time\n",
    "print(\"First run (no cache):\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Check if result is cached\n",
    "cache_key = \"expensive_result_1M\"\n",
    "result = stx.io.cache(cache_key)\n",
    "\n",
    "if result is None:\n",
    "    # Not cached, compute and cache\n",
    "    result = expensive_computation(1000000)\n",
    "    stx.io.cache(cache_key, result)\n",
    "    print(f\"Computed and cached result: {result:.4f}\")\nelse:\n",
    "    print(f\"Loaded from cache: {result:.4f}\")\n",
    "    \n",
    "first_time = time.time() - start_time\n",
    "print(f\"Time taken: {first_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second run - should be cached\n",
    "print(\"\\nSecond run (from cache):\")\n",
    "start_time = time.time()\n",
    "\n",
    "result = stx.io.cache(cache_key)\n",
    "if result is None:\n",
    "    result = expensive_computation(1000000)\n",
    "    stx.io.cache(cache_key, result)\n",
    "    print(f\"Computed and cached result: {result:.4f}\")\n",
    "else:\n",
    "    print(f\"Loaded from cache: {result:.4f}\")\n",
    "    \n",
    "second_time = time.time() - start_time\n",
    "print(f\"Time taken: {second_time:.2f} seconds\")\n",
    "print(f\"Speedup: {first_time/second_time:.1f}x faster!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. HDF5 Interactive Exploration\n",
    "\n",
    "For large scientific datasets, SciTeX provides powerful HDF5 exploration tools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample HDF5 file with hierarchical data\n",
    "h5_file = temp_dir / 'experiment_data.h5'\n",
    "\n",
    "# Create sample scientific data\n",
    "experiment_data = {\n",
    "    'experiment_1': {\n",
    "        'raw_data': np.random.randn(1000, 64),  # 1000 samples, 64 channels\n",
    "        'metadata': {\n",
    "            'sampling_rate': 1000,\n",
    "            'channels': [f'Ch{i:02d}' for i in range(64)],\n",
    "            'experiment_date': '2025-07-03'\n",
    "        },\n",
    "        'processed': {\n",
    "            'filtered': np.random.randn(1000, 64) * 0.8,\n",
    "            'features': np.random.randn(100, 10)\n",
    "        }\n",
    "    },\n",
    "    'experiment_2': {\n",
    "        'raw_data': np.random.randn(1200, 64),\n",
    "        'metadata': {\n",
    "            'sampling_rate': 1000,\n",
    "            'channels': [f'Ch{i:02d}' for i in range(64)],\n",
    "            'experiment_date': '2025-07-04'\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save as HDF5\n",
    "stx.io.save(experiment_data, h5_file)\n",
    "print(f\"Created HDF5 file: {h5_file}\")\n",
    "print(f\"File size: {h5_file.stat().st_size / 1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the HDF5 file structure\n",
    "print(\"HDF5 File Structure:\")\n",
    "explorer = stx.io.H5Explorer(h5_file)\n",
    "explorer.explore()\n",
    "\n",
    "# Load specific parts of the data\n",
    "print(\"\\nLoading specific datasets:\")\n",
    "exp1_raw = stx.io.load(h5_file, key='experiment_1/raw_data')\n",
    "print(f\"Experiment 1 raw data shape: {exp1_raw.shape}\")\n",
    "\n",
    "exp1_metadata = stx.io.load(h5_file, key='experiment_1/metadata')\n",
    "print(f\"Experiment 1 metadata: {exp1_metadata}\")\n",
    "\n",
    "# Check if specific keys exist\n",
    "keys_to_check = [\n",
    "    'experiment_1/raw_data',\n",
    "    'experiment_1/processed/features', \n",
    "    'experiment_3/data',  # This doesn't exist\n",
    "]\n",
    "\n",
    "print(\"\\nKey existence check:\")\n",
    "for key in keys_to_check:\n",
    "    exists = stx.io.has_h5_key(h5_file, key)\n",
    "    print(f\"  {key}: {'✅ exists' if exists else '❌ not found'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. matplotlib Integration with Data Export\n",
    "\n",
    "SciTeX automatically exports plot data for reproducibility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data for plotting\n",
    "np.random.seed(42)\n",
    "x = np.linspace(0, 10, 100)\n",
    "y1 = np.sin(x) + 0.1 * np.random.randn(100)\n",
    "y2 = np.cos(x) + 0.1 * np.random.randn(100)\n",
    "y3 = np.sin(2*x) * np.exp(-x/5) + 0.05 * np.random.randn(100)\n",
    "\n",
    "# Create a publication-ready plot\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8))\n",
    "\n",
    "# Top subplot\n",
    "ax1.plot(x, y1, 'b-', label='sin(x) + noise', linewidth=2)\n",
    "ax1.plot(x, y2, 'r--', label='cos(x) + noise', linewidth=2)\n",
    "ax1.set_xlabel('Time (s)')\n",
    "ax1.set_ylabel('Amplitude')\n",
    "ax1.set_title('Trigonometric Functions with Noise')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Bottom subplot\n",
    "ax2.plot(x, y3, 'g-', label='Damped oscillation', linewidth=2)\n",
    "ax2.fill_between(x, y3-0.1, y3+0.1, alpha=0.3, color='green')\n",
    "ax2.set_xlabel('Time (s)')\n",
    "ax2.set_ylabel('Amplitude')\n",
    "ax2.set_title('Damped Oscillation')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save the figure - SciTeX will automatically export the data too!\n",
    "plot_file = temp_dir / 'scientific_plot.png'\n",
    "stx.io.save(fig, plot_file)\n",
    "print(f\"Saved plot to: {plot_file}\")\n",
    "\n",
    "# Check what files were created\n",
    "plot_files = list(temp_dir.glob('scientific_plot*'))\n",
    "print(\"\\nGenerated files:\")\n",
    "for f in sorted(plot_files):\n",
    "    print(f\"  {f.name} ({f.stat().st_size} bytes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Machine Learning Model Persistence\n",
    "\n",
    "SciTeX handles various ML model formats seamlessly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate different types of model artifacts\n",
    "models_data = {\n",
    "    # Scikit-learn style model\n",
    "    'sklearn_model': {\n",
    "        'model_type': 'RandomForestClassifier',\n",
    "        'parameters': {'n_estimators': 100, 'max_depth': 10},\n",
    "        'feature_names': ['feature_1', 'feature_2', 'feature_3'],\n",
    "        'training_score': 0.95,\n",
    "        'validation_score': 0.87\n",
    "    },\n",
    "    \n",
    "    # PyTorch-style model state\n",
    "    'pytorch_weights': {\n",
    "        'layer1.weight': np.random.randn(128, 64),\n",
    "        'layer1.bias': np.random.randn(128),\n",
    "        'layer2.weight': np.random.randn(64, 32),\n",
    "        'layer2.bias': np.random.randn(64),\n",
    "        'output.weight': np.random.randn(10, 32),\n",
    "        'output.bias': np.random.randn(10)\n",
    "    },\n",
    "    \n",
    "    # Training history\n",
    "    'training_history': {\n",
    "        'epoch': list(range(1, 51)),\n",
    "        'train_loss': np.exp(-np.linspace(0, 3, 50)) + 0.1 * np.random.randn(50),\n",
    "        'val_loss': np.exp(-np.linspace(0, 2.5, 50)) + 0.15 * np.random.randn(50),\n",
    "        'train_acc': 1 - np.exp(-np.linspace(0, 3, 50)) + 0.05 * np.random.randn(50),\n",
    "        'val_acc': 1 - np.exp(-np.linspace(0, 2.5, 50)) + 0.1 * np.random.randn(50)\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save different model components in appropriate formats\n",
    "model_files = {\n",
    "    'sklearn_model.pkl': models_data['sklearn_model'],\n",
    "    'pytorch_weights.pth': models_data['pytorch_weights'], \n",
    "    'training_history.json': models_data['training_history']\n",
    "}\n",
    "\n",
    "print(\"Saving model artifacts:\")\n",
    "for filename, data in model_files.items():\n",
    "    filepath = temp_dir / filename\n",
    "    stx.io.save(data, filepath)\n",
    "    print(f\"✅ Saved {filename} ({filepath.stat().st_size} bytes)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and verify model artifacts\n",
    "print(\"Loading model artifacts:\")\n",
    "\n",
    "loaded_sklearn = stx.io.load(temp_dir / 'sklearn_model.pkl')\n",
    "print(f\"✅ Loaded sklearn model: {loaded_sklearn['model_type']}\")\n",
    "print(f\"   Training score: {loaded_sklearn['training_score']:.3f}\")\n",
    "\n",
    "loaded_weights = stx.io.load(temp_dir / 'pytorch_weights.pth')\n",
    "print(f\"✅ Loaded PyTorch weights: {len(loaded_weights)} layers\")\n",
    "for layer_name, weights in loaded_weights.items():\n",
    "    if hasattr(weights, 'shape'):\n",
    "        print(f\"   {layer_name}: {weights.shape}\")\n",
    "\n",
    "loaded_history = stx.io.load(temp_dir / 'training_history.json')\n",
    "print(f\"✅ Loaded training history: {len(loaded_history['epoch'])} epochs\")\n",
    "print(f\"   Final train loss: {loaded_history['train_loss'][-1]:.4f}\")\n",
    "print(f\"   Final val loss: {loaded_history['val_loss'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Batch Processing with File Patterns\n",
    "\n",
    "Real-world example of processing multiple data files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a more realistic batch processing scenario\n",
    "os.chdir(temp_dir)\n",
    "\n",
    "try:\n",
    "    # Find all subject data files\n",
    "    subject_files, parsed_params = stx.io.glob('subject_{subject_id}/session_{session_num}.csv', parse=True)\n",
    "    \n",
    "    print(f\"Processing {len(subject_files)} data files:\")\n",
    "    \n",
    "    results = []\n",
    "    for filepath, params in zip(subject_files, parsed_params):\n",
    "        # Load the data\n",
    "        data = stx.io.load(filepath)\n",
    "        \n",
    "        # Process the data (example: compute statistics)\n",
    "        stats = {\n",
    "            'subject_id': params['subject_id'],\n",
    "            'session_num': params['session_num'],\n",
    "            'n_samples': len(data),\n",
    "            'mean_x': data['x'].mean(),\n",
    "            'std_x': data['x'].std(),\n",
    "            'mean_y': data['y'].mean(),\n",
    "            'std_y': data['y'].std(),\n",
    "            'correlation': data['x'].corr(data['y'])\n",
    "        }\n",
    "        \n",
    "        results.append(stats)\n",
    "        print(f\"  ✅ Processed {filepath} → corr = {stats['correlation']:.3f}\")\n",
    "    \n",
    "    # Combine results into a summary DataFrame\n",
    "    summary_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Save the summary\n",
    "    summary_file = 'batch_processing_summary.csv'\n",
    "    stx.io.save(summary_df, summary_file)\n",
    "    print(f\"\\n✅ Saved summary to: {summary_file}\")\n",
    "    \n",
    "    # Display the summary\n",
    "    print(\"\\nBatch Processing Summary:\")\n",
    "    print(summary_df.to_string(index=False))\n",
    "    \n",
    "finally:\n",
    "    os.chdir(original_cwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Advanced Features Demo\n",
    "\n",
    "Let's explore some advanced features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-format data pipeline example\n",
    "print(\"Multi-format data pipeline:\")\n",
    "\n",
    "# Start with raw data\n",
    "raw_data = {\n",
    "    'experiment_id': 'EXP_001',\n",
    "    'timestamp': '2025-07-03T10:00:00',\n",
    "    'measurements': np.random.randn(1000),\n",
    "    'metadata': {\n",
    "        'device': 'sensor_v2',\n",
    "        'calibration': 1.05,\n",
    "        'units': 'mV'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save in different formats for different purposes\n",
    "formats = {\n",
    "    'archive.pkl': raw_data,  # Complete data for archival\n",
    "    'measurements.npy': raw_data['measurements'],  # Just data for analysis\n",
    "    'metadata.json': raw_data['metadata'],  # Metadata for documentation\n",
    "    'config.yaml': {'experiment_id': raw_data['experiment_id'], \n",
    "                   'timestamp': raw_data['timestamp']}  # Config for reproducibility\n",
    "}\n",
    "\n",
    "pipeline_dir = temp_dir / 'pipeline'\n",
    "pipeline_dir.mkdir(exist_ok=True)\n",
    "\n",
    "for filename, data in formats.items():\n",
    "    filepath = pipeline_dir / filename\n",
    "    stx.io.save(data, filepath)\n",
    "    print(f\"  ✅ {filename} → {filepath.stat().st_size} bytes\")\n",
    "\n",
    "print(\"\\nPipeline files created - each optimized for its purpose!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate robust error handling\n",
    "print(\"Error handling demonstrations:\")\n",
    "\n",
    "# Try to load non-existent file\n",
    "try:\n",
    "    result = stx.io.load(temp_dir / 'nonexistent_file.pkl')\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"  ✅ Handled missing file gracefully: {type(e).__name__}\")\n",
    "\n",
    "# Try to save to read-only location (simulated)\n",
    "try:\n",
    "    # This will work, but shows how errors would be handled\n",
    "    test_data = {'key': 'value'}\n",
    "    stx.io.save(test_data, temp_dir / 'test_error_handling.json')\n",
    "    print(f\"  ✅ Save operation successful\")\n",
    "except Exception as e:\n",
    "    print(f\"  ⚠️ Save error handled: {type(e).__name__}\")\n",
    "\n",
    "# Demonstrate format auto-detection\n",
    "ambiguous_file = temp_dir / 'data_without_extension'\n",
    "test_data = {'auto_detected': True, 'format': 'pickle'}\n",
    "stx.io.save(test_data, ambiguous_file)\n",
    "loaded_data = stx.io.load(ambiguous_file)\n",
    "print(f\"  ✅ Auto-detection worked: {loaded_data['format']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Cleanup and Summary\n",
    "\n",
    "Let's clean up and summarize what we've learned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of files created during this tutorial\n",
    "all_files = list(temp_dir.rglob('*'))\n",
    "file_types = {}\n",
    "\n",
    "for file_path in all_files:\n",
    "    if file_path.is_file():\n",
    "        suffix = file_path.suffix or 'no_extension'\n",
    "        if suffix not in file_types:\n",
    "            file_types[suffix] = []\n",
    "        file_types[suffix].append(file_path)\n",
    "\n",
    "print(\"📊 SciTeX IO Tutorial Summary\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total files created: {len([f for f in all_files if f.is_file()])}\")\n",
    "print(f\"Total directories: {len([f for f in all_files if f.is_dir()])}\")\n",
    "print(\"\\nFile types handled:\")\n",
    "\n",
    "for suffix, files in sorted(file_types.items()):\n",
    "    total_size = sum(f.stat().st_size for f in files)\n",
    "    print(f\"  {suffix:15} {len(files):3d} files ({total_size:7,d} bytes)\")\n",
    "\n",
    "print(\"\\n✅ Key Features Demonstrated:\")\n",
    "features = [\n",
    "    \"Universal load/save with automatic format detection\",\n",
    "    \"Enhanced file pattern matching with parsing\", \n",
    "    \"YAML configuration management with dot notation\",\n",
    "    \"Caching system for expensive computations\",\n",
    "    \"HDF5 interactive exploration for large datasets\",\n",
    "    \"matplotlib integration with automatic data export\",\n",
    "    \"Machine learning model persistence\", \n",
    "    \"Batch processing with parameter extraction\",\n",
    "    \"Multi-format data pipelines\",\n",
    "    \"Robust error handling and format auto-detection\"\n",
    "]\n",
    "\n",
    "for i, feature in enumerate(features, 1):\n",
    "    print(f\"  {i:2d}. {feature}\")\n",
    "\n",
    "print(f\"\\n🗂️ Temporary files location: {temp_dir}\")\n",
    "print(\"   (Files will be cleaned up when the notebook session ends)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 Next Steps\n",
    "\n",
    "Now that you've seen the power of SciTeX IO, here are some ways to use it in your projects:\n",
    "\n",
    "### **Quick Start Templates**\n",
    "\n",
    "```python\n",
    "# Universal data loading\n",
    "import scitex as stx\n",
    "data = stx.io.load('your_file.any_format')\n",
    "\n",
    "# Configuration management\n",
    "CONFIG = stx.io.load_configs()  # Loads all YAML files from ./config/\n",
    "\n",
    "# Batch processing\n",
    "files, params = stx.io.glob('data/subject_{id}_session_{num}.csv', parse=True)\n",
    "for file, param in zip(files, params):\n",
    "    data = stx.io.load(file)\n",
    "    # Process data using param['id'] and param['num']\n",
    "\n",
    "# Caching expensive computations\n",
    "result = stx.io.cache('computation_key')\n",
    "if result is None:\n",
    "    result = expensive_function()\n",
    "    stx.io.cache('computation_key', result)\n",
    "```\n",
    "\n",
    "### **Advanced Patterns**\n",
    "\n",
    "- **Scientific Workflows**: Load instrument data, process, cache results, export plots\n",
    "- **ML Pipelines**: Load configs, batch process datasets, save models and histories\n",
    "- **Data Analysis**: Explore HDF5 datasets, extract features, generate reports\n",
    "- **Reproducible Research**: Version configurations, cache computations, export plot data\n",
    "\n",
    "The SciTeX IO module handles the complexity of file formats so you can focus on your research and analysis!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}