{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SciTeX Parallel Tutorial\n",
    "\n",
    "This notebook demonstrates how to use the `scitex.parallel` module for parallel processing and multiprocessing tasks.\n",
    "\n",
    "## Features Covered\n",
    "\n",
    "* Parallel function execution with ThreadPoolExecutor\n",
    "* Automatic CPU core detection and utilization\n",
    "* Progress tracking with tqdm integration\n",
    "* Handling multiple argument functions\n",
    "* Error handling and performance optimization\n",
    "* Scientific computing applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import multiprocessing\n",
    "from scitex import parallel as stx_parallel\n",
    "import matplotlib.pyplot as plt\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "print(\"SciTeX Parallel Tutorial\")\n",
    "print(\"Available functions:\", dir(stx_parallel))\n",
    "print(f\"Available CPU cores: {multiprocessing.cpu_count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic Parallel Processing\n",
    "\n",
    "### Simple Mathematical Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple function to run in parallel\n",
    "def square_number(x):\n",
    "    \"\"\"Square a number with a small delay to simulate computation.\"\"\"\n",
    "    time.sleep(0.1)  # Simulate computational work\n",
    "    return x ** 2\n",
    "\n",
    "# Create arguments list - each tuple contains arguments for one function call\n",
    "numbers = list(range(1, 11))\n",
    "args_list = [(num,) for num in numbers]  # Convert to tuple format\n",
    "\n",
    "print(f\"Numbers to process: {numbers}\")\n",
    "print(f\"Arguments list format: {args_list[:3]}...\")  # Show first 3\n",
    "\n",
    "# Time sequential execution\n",
    "start_time = time.time()\n",
    "sequential_results = [square_number(num) for num in numbers]\n",
    "sequential_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nSequential execution:\")\n",
    "print(f\"  Time: {sequential_time:.3f} seconds\")\n",
    "print(f\"  Results: {sequential_results}\")\n",
    "\n",
    "# Time parallel execution\n",
    "start_time = time.time()\n",
    "parallel_results = stx_parallel.run(square_number, args_list, n_jobs=4, desc=\"Squaring numbers\")\n",
    "parallel_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nParallel execution (4 workers):\")\n",
    "print(f\"  Time: {parallel_time:.3f} seconds\")\n",
    "print(f\"  Results: {parallel_results}\")\n",
    "print(f\"  Speedup: {sequential_time/parallel_time:.2f}x\")\n",
    "\n",
    "# Verify results are identical\n",
    "print(f\"\\nResults match: {sequential_results == parallel_results}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Argument Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function with multiple arguments\n",
    "def compute_distance(x1, y1, x2, y2):\n",
    "    \"\"\"Compute Euclidean distance between two points.\"\"\"\n",
    "    time.sleep(0.05)  # Simulate computation\n",
    "    return np.sqrt((x2 - x1)**2 + (y2 - y1)**2)\n",
    "\n",
    "# Generate random point pairs\n",
    "np.random.seed(42)\n",
    "n_pairs = 20\n",
    "point_pairs = []\n",
    "for i in range(n_pairs):\n",
    "    x1, y1 = np.random.uniform(-10, 10, 2)\n",
    "    x2, y2 = np.random.uniform(-10, 10, 2)\n",
    "    point_pairs.append((x1, y1, x2, y2))\n",
    "\n",
    "print(f\"Computing distances for {n_pairs} point pairs\")\n",
    "print(f\"Sample point pair: ({point_pairs[0][0]:.2f}, {point_pairs[0][1]:.2f}) to ({point_pairs[0][2]:.2f}, {point_pairs[0][3]:.2f})\")\n",
    "\n",
    "# Run in parallel\n",
    "start_time = time.time()\n",
    "distances = stx_parallel.run(compute_distance, point_pairs, n_jobs=-1, desc=\"Computing distances\")\n",
    "parallel_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nParallel computation completed in {parallel_time:.3f} seconds\")\n",
    "print(f\"Sample distances: {[f'{d:.2f}' for d in distances[:5]]}\")\n",
    "print(f\"Average distance: {np.mean(distances):.2f}\")\n",
    "print(f\"Min/Max distances: {np.min(distances):.2f} / {np.max(distances):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Functions Returning Multiple Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that returns multiple values\n",
    "def analyze_number(x):\n",
    "    \"\"\"Analyze a number and return multiple statistics.\"\"\"\n",
    "    time.sleep(0.02)  # Simulate computation\n",
    "    square = x ** 2\n",
    "    cube = x ** 3\n",
    "    sqrt = np.sqrt(abs(x)) if x >= 0 else np.sqrt(-x) * 1j\n",
    "    factorial = np.math.factorial(x) if x >= 0 and x <= 10 else None\n",
    "    return square, cube, sqrt, factorial\n",
    "\n",
    "# Test with a range of numbers\n",
    "test_numbers = list(range(-5, 11))\n",
    "args_list = [(num,) for num in test_numbers]\n",
    "\n",
    "print(f\"Analyzing numbers: {test_numbers}\")\n",
    "\n",
    "# Run parallel analysis\n",
    "results = stx_parallel.run(analyze_number, args_list, n_jobs=4, desc=\"Analyzing numbers\")\n",
    "\n",
    "print(f\"\\nResults type: {type(results)}\")\n",
    "print(f\"Number of result arrays: {len(results)}\")\n",
    "\n",
    "# Extract individual result arrays\n",
    "squares, cubes, sqrts, factorials = results\n",
    "\n",
    "print(f\"\\nSquares: {squares}\")\n",
    "print(f\"Cubes: {cubes}\")\n",
    "print(f\"Square roots: {[f'{abs(s):.2f}' + ('i' if isinstance(s, complex) else '') for s in sqrts]}\")\n",
    "print(f\"Factorials: {factorials}\")\n",
    "\n",
    "# Visualize results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "fig.suptitle('Parallel Number Analysis Results')\n",
    "\n",
    "# Plot squares\n",
    "axes[0, 0].plot(test_numbers, squares, 'bo-')\n",
    "axes[0, 0].set_title('Squares')\n",
    "axes[0, 0].set_xlabel('Number')\n",
    "axes[0, 0].set_ylabel('Square')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot cubes\n",
    "axes[0, 1].plot(test_numbers, cubes, 'ro-')\n",
    "axes[0, 1].set_title('Cubes')\n",
    "axes[0, 1].set_xlabel('Number')\n",
    "axes[0, 1].set_ylabel('Cube')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot square roots (magnitude)\n",
    "sqrt_magnitudes = [abs(s) for s in sqrts]\n",
    "axes[1, 0].plot(test_numbers, sqrt_magnitudes, 'go-')\n",
    "axes[1, 0].set_title('Square Root Magnitudes')\n",
    "axes[1, 0].set_xlabel('Number')\n",
    "axes[1, 0].set_ylabel('|√x|')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot factorials (for valid values)\n",
    "valid_factorials = [(n, f) for n, f in zip(test_numbers, factorials) if f is not None]\n",
    "if valid_factorials:\n",
    "    numbers, facts = zip(*valid_factorials)\n",
    "    axes[1, 1].semilogy(numbers, facts, 'mo-')\n",
    "axes[1, 1].set_title('Factorials (log scale)')\n",
    "axes[1, 1].set_xlabel('Number')\n",
    "axes[1, 1].set_ylabel('n! (log scale)')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Scientific Computing Applications\n",
    "\n",
    "### Monte Carlo Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_pi(n_samples, seed):\n",
    "    \"\"\"Estimate π using Monte Carlo method.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Generate random points in unit square\n",
    "    x = np.random.uniform(-1, 1, n_samples)\n",
    "    y = np.random.uniform(-1, 1, n_samples)\n",
    "    \n",
    "    # Count points inside unit circle\n",
    "    inside_circle = (x**2 + y**2) <= 1\n",
    "    pi_estimate = 4 * np.sum(inside_circle) / n_samples\n",
    "    \n",
    "    return pi_estimate, np.sum(inside_circle), n_samples\n",
    "\n",
    "# Run multiple Monte Carlo simulations in parallel\n",
    "n_simulations = 20\n",
    "samples_per_sim = 100000\n",
    "\n",
    "# Create arguments: (n_samples, seed) for each simulation\n",
    "mc_args = [(samples_per_sim, i) for i in range(n_simulations)]\n",
    "\n",
    "print(f\"Running {n_simulations} Monte Carlo simulations\")\n",
    "print(f\"Each simulation uses {samples_per_sim:,} samples\")\n",
    "\n",
    "start_time = time.time()\n",
    "mc_results = stx_parallel.run(monte_carlo_pi, mc_args, n_jobs=-1, desc=\"Monte Carlo π estimation\")\n",
    "parallel_time = time.time() - start_time\n",
    "\n",
    "# Extract results\n",
    "pi_estimates, points_inside, total_points = mc_results\n",
    "\n",
    "print(f\"\\nParallel execution completed in {parallel_time:.3f} seconds\")\n",
    "print(f\"π estimates: {[f'{est:.4f}' for est in pi_estimates[:5]]}...\")\n",
    "print(f\"Average π estimate: {np.mean(pi_estimates):.6f}\")\n",
    "print(f\"Standard deviation: {np.std(pi_estimates):.6f}\")\n",
    "print(f\"True π value: {np.pi:.6f}\")\n",
    "print(f\"Error: {abs(np.mean(pi_estimates) - np.pi):.6f}\")\n",
    "\n",
    "# Visualize convergence\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(pi_estimates, 'bo-', alpha=0.7, label='π estimates')\n",
    "plt.axhline(y=np.pi, color='r', linestyle='--', label='True π')\n",
    "plt.axhline(y=np.mean(pi_estimates), color='g', linestyle='--', label='Average estimate')\n",
    "plt.xlabel('Simulation number')\n",
    "plt.ylabel('π estimate')\n",
    "plt.title('Monte Carlo π Estimates')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(pi_estimates, bins=10, alpha=0.7, edgecolor='black')\n",
    "plt.axvline(x=np.pi, color='r', linestyle='--', label='True π')\n",
    "plt.axvline(x=np.mean(pi_estimates), color='g', linestyle='--', label='Average estimate')\n",
    "plt.xlabel('π estimate')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of π Estimates')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Signal Processing in Parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_signal(signal_params):\n",
    "    \"\"\"Analyze a synthetic signal.\"\"\"\n",
    "    freq, amplitude, noise_level, duration = signal_params\n",
    "    \n",
    "    # Generate time vector\n",
    "    t = np.linspace(0, duration, int(1000 * duration))\n",
    "    \n",
    "    # Generate signal with noise\n",
    "    signal = amplitude * np.sin(2 * np.pi * freq * t)\n",
    "    noise = np.random.normal(0, noise_level, len(t))\n",
    "    noisy_signal = signal + noise\n",
    "    \n",
    "    # Compute statistics\n",
    "    mean_power = np.mean(noisy_signal**2)\n",
    "    peak_amplitude = np.max(np.abs(noisy_signal))\n",
    "    snr = 20 * np.log10(amplitude / noise_level) if noise_level > 0 else np.inf\n",
    "    \n",
    "    # Simple frequency analysis (find dominant frequency)\n",
    "    fft = np.fft.fft(noisy_signal)\n",
    "    freqs = np.fft.fftfreq(len(t), t[1] - t[0])\n",
    "    dominant_freq = freqs[np.argmax(np.abs(fft[:len(fft)//2]))]\n",
    "    \n",
    "    return {\n",
    "        'frequency': freq,\n",
    "        'amplitude': amplitude, \n",
    "        'noise_level': noise_level,\n",
    "        'mean_power': mean_power,\n",
    "        'peak_amplitude': peak_amplitude,\n",
    "        'snr_db': snr,\n",
    "        'detected_freq': dominant_freq,\n",
    "        'freq_error': abs(dominant_freq - freq)\n",
    "    }\n",
    "\n",
    "# Generate signal parameters for analysis\n",
    "signal_configs = []\n",
    "for freq in [1, 2, 5, 10, 15]:  # Different frequencies\n",
    "    for amp in [1.0, 2.0]:      # Different amplitudes\n",
    "        for noise in [0.1, 0.3, 0.5]:  # Different noise levels\n",
    "            signal_configs.append((freq, amp, noise, 2.0))  # 2 second duration\n",
    "\n",
    "print(f\"Analyzing {len(signal_configs)} signal configurations\")\n",
    "print(f\"Sample configuration: freq={signal_configs[0][0]}Hz, amp={signal_configs[0][1]}, noise={signal_configs[0][2]}\")\n",
    "\n",
    "# Run parallel signal analysis\n",
    "start_time = time.time()\n",
    "signal_results = stx_parallel.run(analyze_signal, [(config,) for config in signal_configs], \n",
    "                                 n_jobs=-1, desc=\"Analyzing signals\")\n",
    "analysis_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nSignal analysis completed in {analysis_time:.3f} seconds\")\n",
    "print(f\"Average frequency detection error: {np.mean([r['freq_error'] for r in signal_results]):.3f} Hz\")\n",
    "\n",
    "# Analyze results\n",
    "import pandas as pd\n",
    "\n",
    "# Convert results to DataFrame for analysis\n",
    "df = pd.DataFrame(signal_results)\n",
    "print(\"\\nSignal Analysis Summary:\")\n",
    "print(df.groupby(['frequency', 'noise_level']).agg({\n",
    "    'snr_db': 'mean',\n",
    "    'freq_error': 'mean',\n",
    "    'peak_amplitude': 'mean'\n",
    "}).round(3))\n",
    "\n",
    "# Visualize results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Parallel Signal Analysis Results')\n",
    "\n",
    "# SNR vs Noise Level\n",
    "for freq in df['frequency'].unique():\n",
    "    freq_data = df[df['frequency'] == freq]\n",
    "    axes[0, 0].plot(freq_data['noise_level'], freq_data['snr_db'], 'o-', label=f'{freq} Hz')\n",
    "axes[0, 0].set_xlabel('Noise Level')\n",
    "axes[0, 0].set_ylabel('SNR (dB)')\n",
    "axes[0, 0].set_title('SNR vs Noise Level')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Frequency Detection Error\n",
    "axes[0, 1].scatter(df['frequency'], df['freq_error'], c=df['noise_level'], \n",
    "                  cmap='viridis', alpha=0.7)\n",
    "axes[0, 1].set_xlabel('True Frequency (Hz)')\n",
    "axes[0, 1].set_ylabel('Frequency Error (Hz)')\n",
    "axes[0, 1].set_title('Frequency Detection Error')\n",
    "cbar = plt.colorbar(axes[0, 1].collections[0], ax=axes[0, 1])\n",
    "cbar.set_label('Noise Level')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Mean Power vs Amplitude\n",
    "axes[1, 0].scatter(df['amplitude'], df['mean_power'], c=df['frequency'], \n",
    "                  cmap='plasma', alpha=0.7)\n",
    "axes[1, 0].set_xlabel('Signal Amplitude')\n",
    "axes[1, 0].set_ylabel('Mean Power')\n",
    "axes[1, 0].set_title('Mean Power vs Amplitude')\n",
    "cbar = plt.colorbar(axes[1, 0].collections[0], ax=axes[1, 0])\n",
    "cbar.set_label('Frequency (Hz)')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Distribution of SNR values\n",
    "axes[1, 1].hist(df['snr_db'], bins=15, alpha=0.7, edgecolor='black')\n",
    "axes[1, 1].set_xlabel('SNR (dB)')\n",
    "axes[1, 1].set_ylabel('Count')\n",
    "axes[1, 1].set_title('Distribution of SNR Values')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Performance Analysis and Optimization\n",
    "\n",
    "### Worker Count Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cpu_intensive_task(n):\n",
    "    \"\"\"A CPU-intensive task for benchmarking.\"\"\"\n",
    "    # Compute some intensive calculation\n",
    "    result = 0\n",
    "    for i in range(n * 1000):\n",
    "        result += np.sin(i) * np.cos(i)\n",
    "    return result\n",
    "\n",
    "# Test different numbers of workers\n",
    "max_workers = multiprocessing.cpu_count()\n",
    "worker_counts = [1, 2, 4, max_workers//2, max_workers, max_workers*2]\n",
    "worker_counts = [w for w in worker_counts if w >= 1]  # Remove duplicates and invalid values\n",
    "worker_counts = sorted(list(set(worker_counts)))  # Remove duplicates\n",
    "\n",
    "# Create workload\n",
    "task_sizes = [500] * 20  # 20 tasks of moderate size\n",
    "args_list = [(size,) for size in task_sizes]\n",
    "\n",
    "print(f\"Benchmarking with {len(task_sizes)} tasks\")\n",
    "print(f\"Testing worker counts: {worker_counts}\")\n",
    "\n",
    "performance_results = []\n",
    "\n",
    "for n_workers in worker_counts:\n",
    "    print(f\"\\nTesting with {n_workers} workers...\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    results = stx_parallel.run(cpu_intensive_task, args_list, \n",
    "                              n_jobs=n_workers, desc=f\"Workers: {n_workers}\")\n",
    "    execution_time = time.time() - start_time\n",
    "    \n",
    "    performance_results.append({\n",
    "        'workers': n_workers,\n",
    "        'time': execution_time,\n",
    "        'speedup': performance_results[0]['time'] / execution_time if performance_results else 1.0,\n",
    "        'efficiency': (performance_results[0]['time'] / execution_time) / n_workers if performance_results else 1.0\n",
    "    })\n",
    "    \n",
    "    print(f\"  Execution time: {execution_time:.3f} seconds\")\n",
    "    if performance_results:\n",
    "        speedup = performance_results[0]['time'] / execution_time\n",
    "        efficiency = speedup / n_workers\n",
    "        print(f\"  Speedup: {speedup:.2f}x\")\n",
    "        print(f\"  Efficiency: {efficiency:.2f}\")\n",
    "\n",
    "# Visualize performance results\n",
    "perf_df = pd.DataFrame(performance_results)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Execution time vs workers\n",
    "axes[0].plot(perf_df['workers'], perf_df['time'], 'bo-', linewidth=2, markersize=8)\n",
    "axes[0].set_xlabel('Number of Workers')\n",
    "axes[0].set_ylabel('Execution Time (seconds)')\n",
    "axes[0].set_title('Execution Time vs Workers')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Speedup vs workers\n",
    "axes[1].plot(perf_df['workers'], perf_df['speedup'], 'ro-', linewidth=2, markersize=8, label='Actual')\n",
    "axes[1].plot(perf_df['workers'], perf_df['workers'], 'k--', alpha=0.5, label='Ideal Linear')\n",
    "axes[1].set_xlabel('Number of Workers')\n",
    "axes[1].set_ylabel('Speedup')\n",
    "axes[1].set_title('Speedup vs Workers')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Efficiency vs workers\n",
    "axes[2].plot(perf_df['workers'], perf_df['efficiency'], 'go-', linewidth=2, markersize=8)\n",
    "axes[2].axhline(y=1.0, color='k', linestyle='--', alpha=0.5, label='Perfect Efficiency')\n",
    "axes[2].set_xlabel('Number of Workers')\n",
    "axes[2].set_ylabel('Efficiency')\n",
    "axes[2].set_title('Efficiency vs Workers')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find optimal worker count\n",
    "optimal_idx = perf_df['time'].idxmin()\n",
    "optimal_workers = perf_df.loc[optimal_idx, 'workers']\n",
    "optimal_time = perf_df.loc[optimal_idx, 'time']\n",
    "\n",
    "print(f\"\\nOptimal configuration:\")\n",
    "print(f\"  Workers: {optimal_workers}\")\n",
    "print(f\"  Execution time: {optimal_time:.3f} seconds\")\n",
    "print(f\"  Speedup: {perf_df.loc[optimal_idx, 'speedup']:.2f}x\")\n",
    "print(f\"  Efficiency: {perf_df.loc[optimal_idx, 'efficiency']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Error Handling and Edge Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_with_errors(x):\n",
    "    \"\"\"Function that may raise errors for demonstration.\"\"\"\n",
    "    if x < 0:\n",
    "        raise ValueError(f\"Negative input not allowed: {x}\")\n",
    "    elif x == 0:\n",
    "        return float('inf')  # Special case\n",
    "    else:\n",
    "        return 1.0 / x\n",
    "\n",
    "def safe_function_wrapper(x):\n",
    "    \"\"\"Wrapper that handles errors gracefully.\"\"\"\n",
    "    try:\n",
    "        result = function_with_errors(x)\n",
    "        return result, None  # (result, error)\n",
    "    except Exception as e:\n",
    "        return None, str(e)  # (result, error)\n",
    "\n",
    "# Test with problematic inputs\n",
    "test_inputs = [-2, -1, 0, 0.5, 1, 2, 5]\n",
    "args_list = [(x,) for x in test_inputs]\n",
    "\n",
    "print(f\"Testing error handling with inputs: {test_inputs}\")\n",
    "\n",
    "# Run with safe wrapper\n",
    "results = stx_parallel.run(safe_function_wrapper, args_list, n_jobs=4, desc=\"Error handling test\")\n",
    "\n",
    "# Extract results and errors\n",
    "values, errors = results\n",
    "\n",
    "print(\"\\nResults:\")\n",
    "for i, (inp, val, err) in enumerate(zip(test_inputs, values, errors)):\n",
    "    if err is None:\n",
    "        print(f\"  Input {inp}: Result = {val}\")\n",
    "    else:\n",
    "        print(f\"  Input {inp}: Error = {err}\")\n",
    "\n",
    "# Test edge cases\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Testing Edge Cases\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Test with empty input\n",
    "try:\n",
    "    empty_result = stx_parallel.run(square_number, [], n_jobs=2)\n",
    "    print(\"Empty input test: Unexpected success\")\n",
    "except ValueError as e:\n",
    "    print(f\"Empty input test: Correctly caught error - {e}\")\n",
    "\n",
    "# Test with invalid function\n",
    "try:\n",
    "    invalid_result = stx_parallel.run(\"not_a_function\", [(1,)], n_jobs=2)\n",
    "    print(\"Invalid function test: Unexpected success\")\n",
    "except ValueError as e:\n",
    "    print(f\"Invalid function test: Correctly caught error - {e}\")\n",
    "\n",
    "# Test with too many workers\n",
    "import warnings\n",
    "with warnings.catch_warnings(record=True) as w:\n",
    "    warnings.simplefilter(\"always\")\n",
    "    result = stx_parallel.run(square_number, [(1,), (2,)], n_jobs=1000)\n",
    "    if w:\n",
    "        print(f\"Too many workers test: Warning correctly issued - {w[0].message}\")\n",
    "    else:\n",
    "        print(\"Too many workers test: No warning (unexpected)\")\n",
    "\n",
    "print(\"\\nEdge case testing completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Best Practices and Tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best Practices for SciTeX Parallel Processing\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "practices = [\n",
    "    {\n",
    "        \"title\": \"1. Choose Optimal Worker Count\",\n",
    "        \"description\": \"Use n_jobs=-1 for automatic CPU detection, or tune based on your workload\",\n",
    "        \"example\": \"n_jobs=-1  # Auto-detect CPUs\\nn_jobs=4   # Fixed worker count\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"2. Argument List Format\",\n",
    "        \"description\": \"Always use tuple format for arguments, even for single arguments\",\n",
    "        \"example\": \"args_list = [(arg1,), (arg2,)]  # Single argument\\nargs_list = [(a1, b1), (a2, b2)]  # Multiple arguments\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"3. Handle Multiple Return Values\",\n",
    "        \"description\": \"Functions returning tuples are automatically transposed\",\n",
    "        \"example\": \"results = run(func_returning_tuple, args)\\nval1_list, val2_list = results  # Automatic unpacking\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"4. Error Handling\",\n",
    "        \"description\": \"Wrap functions in try-catch for robust error handling\",\n",
    "        \"example\": \"def safe_func(x):\\n    try:\\n        return func(x), None\\n    except Exception as e:\\n        return None, str(e)\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"5. Progress Monitoring\",\n",
    "        \"description\": \"Use descriptive names for progress bars\",\n",
    "        \"example\": \"run(func, args, desc='Processing data batch 1/5')\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"6. Memory Considerations\",\n",
    "        \"description\": \"Be aware of memory usage with large datasets\",\n",
    "        \"example\": \"# Process in chunks for large datasets\\nfor chunk in chunks(large_dataset):\\n    results.extend(run(func, chunk))\"\n",
    "    }\n",
    "]\n",
    "\n",
    "for practice in practices:\n",
    "    print(f\"\\n{practice['title']}\")\n",
    "    print(\"-\" * len(practice['title']))\n",
    "    print(practice['description'])\n",
    "    print(f\"Example: {practice['example']}\")\n",
    "\n",
    "# Demonstrate chunking for large datasets\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Memory-Efficient Processing Example\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def chunk_processor(chunk_data):\n",
    "    \"\"\"Process a chunk of data.\"\"\"\n",
    "    chunk_id, data_size = chunk_data\n",
    "    # Simulate processing\n",
    "    processed_data = np.random.randn(data_size).sum()\n",
    "    return chunk_id, processed_data, data_size\n",
    "\n",
    "# Simulate large dataset processing\n",
    "total_data_size = 1000000  # 1M data points\n",
    "chunk_size = 50000         # 50K per chunk\n",
    "n_chunks = total_data_size // chunk_size\n",
    "\n",
    "# Create chunk specifications\n",
    "chunk_specs = [(i, chunk_size) for i in range(n_chunks)]\n",
    "\n",
    "print(f\"Processing {total_data_size:,} data points in {n_chunks} chunks\")\n",
    "print(f\"Chunk size: {chunk_size:,} points\")\n",
    "\n",
    "start_time = time.time()\n",
    "chunk_results = stx_parallel.run(chunk_processor, chunk_specs, \n",
    "                                n_jobs=-1, desc=\"Processing chunks\")\n",
    "chunk_time = time.time() - start_time\n",
    "\n",
    "chunk_ids, processed_values, sizes = chunk_results\n",
    "\n",
    "print(f\"\\nChunk processing completed in {chunk_time:.3f} seconds\")\n",
    "print(f\"Processed {sum(sizes):,} total data points\")\n",
    "print(f\"Processing rate: {sum(sizes)/chunk_time:,.0f} points/second\")\n",
    "print(f\"Sample processed values: {[f'{v:.2f}' for v in processed_values[:5]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary and Comparison\n",
    "\n",
    "### Performance Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"SciTeX Parallel Module Performance Summary\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Create a comprehensive benchmark\n",
    "def benchmark_task(task_info):\n",
    "    \"\"\"Benchmark task for comparison.\"\"\"\n",
    "    task_type, size = task_info\n",
    "    \n",
    "    if task_type == 'cpu':\n",
    "        # CPU-intensive task\n",
    "        result = sum(np.sin(i) for i in range(size))\n",
    "    elif task_type == 'memory':\n",
    "        # Memory-intensive task\n",
    "        arr = np.random.randn(size)\n",
    "        result = np.sum(arr ** 2)\n",
    "    elif task_type == 'io':\n",
    "        # I/O simulation\n",
    "        time.sleep(0.01)  # Simulate I/O wait\n",
    "        result = size * 2\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Test different task types\n",
    "task_types = ['cpu', 'memory', 'io']\n",
    "task_sizes = [1000, 5000, 10000]\n",
    "\n",
    "benchmark_results = {}\n",
    "\n",
    "for task_type in task_types:\n",
    "    print(f\"\\nBenchmarking {task_type.upper()} tasks...\")\n",
    "    \n",
    "    # Create task list\n",
    "    tasks = [(task_type, size) for size in task_sizes]\n",
    "    \n",
    "    # Sequential execution\n",
    "    start_time = time.time()\n",
    "    seq_results = [benchmark_task(task) for task in tasks]\n",
    "    seq_time = time.time() - start_time\n",
    "    \n",
    "    # Parallel execution\n",
    "    start_time = time.time()\n",
    "    par_results = stx_parallel.run(benchmark_task, tasks, n_jobs=-1, desc=f\"{task_type} tasks\")\n",
    "    par_time = time.time() - start_time\n",
    "    \n",
    "    speedup = seq_time / par_time\n",
    "    benchmark_results[task_type] = {\n",
    "        'sequential_time': seq_time,\n",
    "        'parallel_time': par_time,\n",
    "        'speedup': speedup,\n",
    "        'results_match': seq_results == par_results\n",
    "    }\n",
    "    \n",
    "    print(f\"  Sequential: {seq_time:.3f}s\")\n",
    "    print(f\"  Parallel:   {par_time:.3f}s\")\n",
    "    print(f\"  Speedup:    {speedup:.2f}x\")\n",
    "    print(f\"  Results match: {seq_results == par_results}\")\n",
    "\n",
    "# Create summary table\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Benchmark Summary\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"{'Task Type':<12} {'Sequential':<12} {'Parallel':<12} {'Speedup':<10} {'Correct':<8}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for task_type, results in benchmark_results.items():\n",
    "    print(f\"{task_type.upper():<12} {results['sequential_time']:<12.3f} {results['parallel_time']:<12.3f} {results['speedup']:<10.2f} {results['results_match']:<8}\")\n",
    "\n",
    "# Visualize benchmark results\n",
    "task_names = list(benchmark_results.keys())\n",
    "speedups = [benchmark_results[task]['speedup'] for task in task_names]\n",
    "seq_times = [benchmark_results[task]['sequential_time'] for task in task_names]\n",
    "par_times = [benchmark_results[task]['parallel_time'] for task in task_names]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Speedup comparison\n",
    "bars = ax1.bar(task_names, speedups, color=['skyblue', 'lightgreen', 'salmon'])\n",
    "ax1.set_ylabel('Speedup Factor')\n",
    "ax1.set_title('Parallel Speedup by Task Type')\n",
    "ax1.axhline(y=1, color='k', linestyle='--', alpha=0.5, label='No speedup')\n",
    "ax1.legend()\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, speedup in zip(bars, speedups):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height + 0.05,\n",
    "             f'{speedup:.1f}x', ha='center', va='bottom')\n",
    "\n",
    "# Execution time comparison\n",
    "x = np.arange(len(task_names))\n",
    "width = 0.35\n",
    "\n",
    "ax2.bar(x - width/2, seq_times, width, label='Sequential', color='lightcoral')\n",
    "ax2.bar(x + width/2, par_times, width, label='Parallel', color='lightblue')\n",
    "\n",
    "ax2.set_ylabel('Execution Time (seconds)')\n",
    "ax2.set_title('Execution Time Comparison')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels([t.upper() for t in task_names])\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nOverall Performance:\")\n",
    "avg_speedup = np.mean(speedups)\n",
    "print(f\"  Average speedup: {avg_speedup:.2f}x\")\n",
    "print(f\"  Best for: {task_names[np.argmax(speedups)].upper()} tasks ({max(speedups):.2f}x)\")\n",
    "print(f\"  System specs: {multiprocessing.cpu_count()} CPU cores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary\n",
    "\n",
    "The `scitex.parallel` module provides efficient parallel processing capabilities with the following key features:\n",
    "\n",
    "### Core Function: `run(func, args_list, n_jobs=-1, desc=\"Processing\")`\n",
    "\n",
    "**Parameters:**\n",
    "- `func`: Function to execute in parallel\n",
    "- `args_list`: List of tuples, each containing arguments for one function call\n",
    "- `n_jobs`: Number of workers (-1 for auto-detection)\n",
    "- `desc`: Description for progress bar\n",
    "\n",
    "**Key Features:**\n",
    "1. **Automatic CPU Detection**: Uses all available cores when `n_jobs=-1`\n",
    "2. **Progress Tracking**: Built-in tqdm progress bars\n",
    "3. **Multiple Return Values**: Automatically handles functions returning tuples\n",
    "4. **Error Handling**: Graceful handling of exceptions and edge cases\n",
    "5. **ThreadPoolExecutor**: Uses efficient thread-based parallelism\n",
    "\n",
    "### Best Use Cases:\n",
    "- **I/O-bound tasks**: File processing, network requests\n",
    "- **CPU-bound tasks**: Mathematical computations, data analysis\n",
    "- **Batch processing**: Large datasets, simulation runs\n",
    "- **Scientific computing**: Monte Carlo simulations, signal analysis\n",
    "\n",
    "### Performance Characteristics:\n",
    "- Optimal speedup typically achieved with 2-4x CPU core count\n",
    "- I/O-bound tasks show highest speedup ratios\n",
    "- Memory-intensive tasks benefit from parallel processing\n",
    "- CPU-bound tasks show good scaling up to core count\n",
    "\n",
    "### Integration Benefits:\n",
    "- Seamless integration with other SciTeX modules\n",
    "- Consistent API design with other SciTeX tools\n",
    "- Built-in progress feedback for scientific workflows\n",
    "- Robust error handling for production use\n",
    "\n",
    "The module is designed to make parallel processing accessible and efficient for scientific computing workflows, with minimal code changes required to parallelize existing sequential operations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}