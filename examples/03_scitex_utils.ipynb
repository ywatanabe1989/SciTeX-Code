{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SciTeX General Utilities\n",
    "\n",
    "This comprehensive notebook demonstrates the SciTeX utils module capabilities, covering general utilities for scientific computing, system operations, and data management.\n",
    "\n",
    "## Features Covered\n",
    "\n",
    "### Data Compression\n",
    "* HDF5 compression utilities\n",
    "* Storage optimization\n",
    "* File size management\n",
    "\n",
    "### Communication\n",
    "* Email notifications\n",
    "* ANSI escape handling\n",
    "* System notifications\n",
    "\n",
    "### Grid Operations\n",
    "* Grid counting and generation\n",
    "* Parameter space exploration\n",
    "* Combinatorial utilities\n",
    "\n",
    "### System Information\n",
    "* Git branch detection\n",
    "* Hostname and user information\n",
    "* Environment detection\n",
    "\n",
    "### Search and Analysis\n",
    "* Advanced search capabilities\n",
    "* Content analysis\n",
    "* Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect notebook name for output directory\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Get notebook name (for papermill compatibility)\n",
    "notebook_name = \"03_scitex_utils\"\n",
    "if 'PAPERMILL_NOTEBOOK_NAME' in os.environ:\n",
    "    notebook_name = Path(os.environ['PAPERMILL_NOTEBOOK_NAME']).stem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "import scitex\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import tempfile\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Set up example data directory\n",
    "data_dir = Path(\"./utils_examples\")\n",
    "data_dir.mkdir(exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: System Information and Environment\n",
    "\n",
    "### 1.1 System Information Gathering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System information gathering\n",
    "\n",
    "try:\n",
    "    # Get hostname\n",
    "    hostname = scitex.utils.get_hostname()\n",
    "except Exception as e:\n",
    "\n",
    "try:\n",
    "    # Get username\n",
    "    username = scitex.utils.get_username()\n",
    "except Exception as e:\n",
    "\n",
    "try:\n",
    "    # Get git branch\n",
    "    git_branch = scitex.utils.get_git_branch(scitex)\n",
    "except Exception as e:\n",
    "\n",
    "# Generate footer with system info\n",
    "try:\n",
    "    footer = scitex.utils.gen_footer(\"user@host\", \"notebook.ipynb\", scitex, \"main\")\n",
    "except Exception as e:\n",
    "\n",
    "# Additional system information\n",
    "\n",
    "# Environment variables (selected)\n",
    "env_vars = ['HOME', 'USER', 'PATH', 'SHELL', 'LANG']\n",
    "for var in env_vars:\n",
    "    value = os.environ.get(var, 'Not set')\n",
    "    # Truncate PATH for readability\n",
    "    if var == 'PATH' and len(value) > 100:\n",
    "        value = value[:100] + '...'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Notification System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# Notification system demonstration\n",
    "\n",
    "# Test notification with different messages\n",
    "notification_tests = [\n",
    "    {\n",
    "        'message': 'Test notification from SciTeX utils',\n",
    "        'level': 'info'\n",
    "    },\n",
    "    {\n",
    "        'message': 'Computation completed successfully',\n",
    "        'level': 'success'\n",
    "    },\n",
    "    {\n",
    "        'message': 'Warning: Low memory detected',\n",
    "        'level': 'warning'\n",
    "    },\n",
    "    {\n",
    "        'message': 'Error: Unable to process data',\n",
    "        'level': 'error'\n",
    "    }\n",
    "]\n",
    "\n",
    "for test in notification_tests:\n",
    "    try:\n",
    "        result = scitex.utils.notify(\n",
    "            message=test['message'],\n",
    "            level=test['level']\n",
    "        )\n",
    "    except Exception as e:\n",
    "\n",
    "# ANSI escape sequence handling\n",
    "\n",
    "ansi_test_strings = [\n",
    "    \"\\033[31mRed text\\033[0m\",\n",
    "    \"\\033[1;32mBold green text\\033[0m\",\n",
    "    \"\\033[4;34mUnderlined blue text\\033[0m\",\n",
    "    \"Normal text without ANSI\",\n",
    "    \"\\033[91mBright red\\033[0m mixed with \\033[92mgreen\\033[0m\"\n",
    "]\n",
    "\n",
    "for test_string in ansi_test_strings:\n",
    "    try:\n",
    "        cleaned = scitex.utils.ansi_escape.sub(\"\", test_string)\n",
    "    except Exception as e:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Grid Operations and Parameter Space\n",
    "\n",
    "### 2.1 Grid Generation and Counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid operations for parameter space exploration\n",
    "\n",
    "# Define parameter spaces for different scenarios\n",
    "parameter_spaces = {\n",
    "    'machine_learning': {\n",
    "        'learning_rate': [0.001, 0.01, 0.1],\n",
    "        'batch_size': [16, 32, 64, 128],\n",
    "        'dropout': [0.2, 0.3, 0.5],\n",
    "        'epochs': [50, 100, 200]\n",
    "    },\n",
    "    'signal_processing': {\n",
    "        'window_size': [64, 128, 256, 512],\n",
    "        'overlap': [0.25, 0.5, 0.75],\n",
    "        'filter_type': ['lowpass', 'highpass', 'bandpass'],\n",
    "        'cutoff_freq': [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "    },\n",
    "    'optimization': {\n",
    "        'algorithm': ['gradient_descent', 'adam', 'rmsprop'],\n",
    "        'momentum': [0.9, 0.95, 0.99],\n",
    "        'weight_decay': [0.0, 1e-4, 1e-3, 1e-2]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Count grids for each parameter space\n",
    "for space_name, params in parameter_spaces.items():\n",
    "    try:\n",
    "        grid_count = scitex.utils.count_grids(params)\n",
    "        \n",
    "        # Show size of each parameter\n",
    "        for param_name, param_values in params.items():\n",
    "            \n",
    "    except Exception as e:\n",
    "\n",
    "# Demonstrate memory estimation for large parameter spaces\n",
    "\n",
    "large_spaces = {\n",
    "    'image_processing': {\n",
    "        'kernel_size': list(range(3, 16, 2)),  # 3, 5, 7, 9, 11, 13, 15\n",
    "        'stride': [1, 2, 3, 4],\n",
    "        'padding': [0, 1, 2],\n",
    "        'dilation': [1, 2, 3],\n",
    "        'activation': ['relu', 'tanh', 'sigmoid', 'leaky_relu', 'elu']\n",
    "    },\n",
    "    'hyperparameter_search': {\n",
    "        'num_layers': list(range(1, 11)),  # 1 to 10\n",
    "        'hidden_units': [32, 64, 128, 256, 512, 1024],\n",
    "        'learning_rate': [10**(-i) for i in range(1, 7)],  # 0.1 to 0.000001\n",
    "        'regularization': [0.0, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1]\n",
    "    }\n",
    "}\n",
    "\n",
    "for space_name, params in large_spaces.items():\n",
    "    try:\n",
    "        grid_count = scitex.utils.count_grids(params)\n",
    "        \n",
    "        # Estimate memory usage (rough calculation)\n",
    "        # Assume each parameter combination takes ~100 bytes\n",
    "        estimated_memory = grid_count * 100\n",
    "        readable_memory = scitex.str.readable_bytes(estimated_memory)\n",
    "        \n",
    "        \n",
    "        if grid_count > 1000000:\n",
    "        elif grid_count > 100000:\n",
    "        else:\n",
    "            \n",
    "    except Exception as e:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Grid Generation and Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Grid generation for parameter exploration\n",
    "\n",
    "# Small parameter space for demonstration\n",
    "demo_params = {\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'batch_size': [32, 64],\n",
    "    'optimizer': ['adam', 'sgd']\n",
    "}\n",
    "\n",
    "\n",
    "try:\n",
    "    # Generate all combinations\n",
    "    combination_count = 0\n",
    "    for combination in scitex.utils.yield_grids(demo_params):\n",
    "        combination_count += 1\n",
    "    \n",
    "    \n",
    "except Exception as e:\n",
    "\n",
    "# Larger example with sampling\n",
    "\n",
    "large_params = {\n",
    "    'alpha': [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "    'beta': [0.01, 0.05, 0.1, 0.2],\n",
    "    'gamma': [0.9, 0.95, 0.99, 0.999],\n",
    "    'method': ['A', 'B', 'C']\n",
    "}\n",
    "\n",
    "total_combinations = scitex.utils.count_grids(large_params)\n",
    "\n",
    "try:\n",
    "    sample_count = 0\n",
    "    for combination in scitex.utils.yield_grids(large_params):\n",
    "        sample_count += 1\n",
    "        \n",
    "        if sample_count >= 10:\n",
    "            break\n",
    "            \n",
    "except Exception as e:\n",
    "\n",
    "# Practical application: Hyperparameter optimization simulation\n",
    "\n",
    "# Simulate evaluating different hyperparameter combinations\n",
    "optimization_params = {\n",
    "    'learning_rate': [0.001, 0.01, 0.1],\n",
    "    'hidden_units': [64, 128, 256],\n",
    "    'dropout': [0.2, 0.3, 0.5]\n",
    "}\n",
    "\n",
    "def simulate_model_performance(params):\n",
    "    \"\"\"Simulate model performance based on hyperparameters.\"\"\"\n",
    "    # Simulate some realistic performance based on parameters\n",
    "    base_score = 0.7\n",
    "    \n",
    "    # Learning rate effect\n",
    "    if params['learning_rate'] == 0.01:\n",
    "        base_score += 0.1\n",
    "    elif params['learning_rate'] == 0.1:\n",
    "        base_score -= 0.05\n",
    "    \n",
    "    # Hidden units effect\n",
    "    if params['hidden_units'] == 128:\n",
    "        base_score += 0.05\n",
    "    elif params['hidden_units'] == 256:\n",
    "        base_score += 0.02\n",
    "    \n",
    "    # Dropout effect\n",
    "    if params['dropout'] == 0.3:\n",
    "        base_score += 0.03\n",
    "    \n",
    "    # Add some random noise\n",
    "    import random\n",
    "    noise = random.uniform(-0.05, 0.05)\n",
    "    \n",
    "    return min(1.0, max(0.0, base_score + noise))\n",
    "\n",
    "# Run optimization simulation\n",
    "results = []\n",
    "total_combinations = scitex.utils.count_grids(optimization_params)\n",
    "\n",
    "\n",
    "for i, params in enumerate(scitex.utils.yield_grids(optimization_params), 1):\n",
    "    performance = simulate_model_performance(params)\n",
    "    results.append({\n",
    "        'combination': i,\n",
    "        'params': params.copy(),\n",
    "        'performance': performance\n",
    "    })\n",
    "\n",
    "# Find best combination\n",
    "best_result = max(results, key=lambda x: x['performance'])\n",
    "\n",
    "# Performance statistics\n",
    "performances = [r['performance'] for r in results]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Data Compression and Storage\n",
    "\n",
    "### 3.1 HDF5 Compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# HDF5 compression demonstration\n",
    "\n",
    "# Create test data for compression\n",
    "compression_test_dir = data_dir / \"compression_tests\"\n",
    "compression_test_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Generate different types of data\n",
    "test_datasets = {\n",
    "    'random_data': np.random.randn(1000, 100),\n",
    "    'structured_data': np.tile(np.arange(100), (1000, 1)),\n",
    "    'sparse_data': np.zeros((1000, 100)),\n",
    "    'time_series': np.sin(np.linspace(0, 100*np.pi, 10000)).reshape(100, 100),\n",
    "    'image_like': np.random.randint(0, 256, (100, 100, 3), dtype=np.uint8)\n",
    "}\n",
    "\n",
    "# Add some structure to sparse data\n",
    "test_datasets['sparse_data'][::10, ::10] = 1.0\n",
    "\n",
    "for name, data in test_datasets.items():\n",
    "\n",
    "# Test HDF5 compression if h5py is available\n",
    "try:\n",
    "    import h5py\n",
    "    \n",
    "    # Create uncompressed HDF5 files\n",
    "    uncompressed_files = {}\n",
    "    compressed_files = {}\n",
    "    \n",
    "    for name, data in test_datasets.items():\n",
    "        # Uncompressed file\n",
    "        uncompressed_file = compression_test_dir / f\"{name}_uncompressed.h5\"\n",
    "        with h5py.File(uncompressed_file, 'w') as f:\n",
    "            f.create_dataset('data', data=data)\n",
    "        uncompressed_files[name] = uncompressed_file\n",
    "        \n",
    "        # Compressed file\n",
    "        compressed_file = compression_test_dir / f\"{name}_compressed.h5\"\n",
    "        with h5py.File(compressed_file, 'w') as f:\n",
    "            f.create_dataset('data', data=data, compression='gzip', compression_opts=9)\n",
    "        compressed_files[name] = compressed_file\n",
    "    \n",
    "    # Test scitex compression utility\n",
    "    for name, uncompressed_file in uncompressed_files.items():\n",
    "        try:\n",
    "            scitex_compressed_file = compression_test_dir / f\"{name}_scitex_compressed.h5\"\n",
    "            \n",
    "            # Use scitex compression\n",
    "            result = scitex.utils.compress_hdf5(\n",
    "                str(uncompressed_file), \n",
    "                str(scitex_compressed_file)\n",
    "            )\n",
    "            \n",
    "            \n",
    "        except Exception as e:\n",
    "    \n",
    "    # Compare file sizes\n",
    "    \n",
    "    for name in test_datasets.keys():\n",
    "        \n",
    "        # Uncompressed size\n",
    "        if name in uncompressed_files:\n",
    "            uncompressed_size = uncompressed_files[name].stat().st_size\n",
    "        \n",
    "        # Compressed size\n",
    "        if name in compressed_files:\n",
    "            compressed_size = compressed_files[name].stat().st_size\n",
    "            \n",
    "            if name in uncompressed_files:\n",
    "                compression_ratio = uncompressed_size / compressed_size if compressed_size > 0 else 1.0\n",
    "                space_saved = (1 - compressed_size / uncompressed_size) * 100\n",
    "        \n",
    "        # SciTeX compressed size\n",
    "        scitex_file = compression_test_dir / f\"{name}_scitex_compressed.h5\"\n",
    "        if scitex_file.exists():\n",
    "            scitex_size = scitex_file.stat().st_size\n",
    "    \n",
    "except ImportError:\n",
    "except Exception as e:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Search and Analysis Utilities\n",
    "\n",
    "### 4.1 Advanced Search Capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced search utilities\n",
    "\n",
    "# Create test data for searching\n",
    "search_test_dir = data_dir / \"search_tests\"\n",
    "search_test_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Create test files with different content types\n",
    "test_files_content = {\n",
    "    'experiment_log.txt': '''\n",
    "Experiment Log - Neural Network Training\n",
    "Date: 2024-01-15\n",
    "Model: ResNet-50\n",
    "Dataset: ImageNet subset\n",
    "Hyperparameters:\n",
    "  learning_rate: 0.001\n",
    "  batch_size: 32\n",
    "  epochs: 100\n",
    "  optimizer: Adam\n",
    "Results:\n",
    "  Training accuracy: 0.945\n",
    "  Validation accuracy: 0.892\n",
    "  Test accuracy: 0.885\n",
    "  Training time: 2.5 hours\n",
    "Notes: Model converged successfully\n",
    "''',\n",
    "    'data_analysis.py': '''\n",
    "#!/usr/bin/env python3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_data(filename):\n",
    "    \"\"\"Load data from CSV file.\"\"\"\n",
    "    return pd.read_csv(filename)\n",
    "\n",
    "def analyze_performance(data):\n",
    "    \"\"\"Analyze model performance metrics.\"\"\"\n",
    "    accuracy = data['accuracy'].mean()\n",
    "    precision = data['precision'].mean()\n",
    "    recall = data['recall'].mean()\n",
    "    return accuracy, precision, recall\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data = load_data(\"results.csv\")\n",
    "    metrics = analyze_performance(data)\n",
    "''',\n",
    "    'config.json': '''\n",
    "{\n",
    "  \"model\": {\n",
    "    \"type\": \"neural_network\",\n",
    "    \"architecture\": \"resnet50\",\n",
    "    \"input_size\": [224, 224, 3],\n",
    "    \"num_classes\": 1000\n",
    "  },\n",
    "  \"training\": {\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"batch_size\": 32,\n",
    "    \"epochs\": 100,\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"loss_function\": \"categorical_crossentropy\"\n",
    "  },\n",
    "  \"data\": {\n",
    "    \"train_path\": \"/data/train\",\n",
    "    \"val_path\": \"/data/validation\",\n",
    "    \"test_path\": \"/data/test\",\n",
    "    \"augmentation\": true\n",
    "  }\n",
    "}\n",
    "''',\n",
    "    'results.csv': '''\n",
    "epoch,accuracy,precision,recall,loss,val_accuracy,val_loss\n",
    "1,0.234,0.245,0.223,2.345,0.245,2.234\n",
    "2,0.345,0.356,0.334,1.876,0.356,1.765\n",
    "3,0.456,0.467,0.445,1.543,0.467,1.432\n",
    "4,0.567,0.578,0.556,1.234,0.578,1.123\n",
    "5,0.678,0.689,0.667,0.987,0.689,0.876\n",
    "''',\n",
    "    'readme.md': '''\n",
    "# Machine Learning Project\n",
    "\n",
    "This project implements a deep learning model for image classification.\n",
    "\n",
    "## Features\n",
    "- ResNet-50 architecture\n",
    "- Data augmentation\n",
    "- Transfer learning\n",
    "- Performance monitoring\n",
    "\n",
    "## Usage\n",
    "```python\n",
    "python train.py --config config.json\n",
    "```\n",
    "\n",
    "## Results\n",
    "- Accuracy: 88.5%\n",
    "- Precision: 87.2%\n",
    "- Recall: 86.8%\n",
    "'''\n",
    "}\n",
    "\n",
    "# Create test files\n",
    "for filename, content in test_files_content.items():\n",
    "    filepath = search_test_dir / filename\n",
    "    filepath.write_text(content.strip())\n",
    "\n",
    "for filename in test_files_content.keys():\n",
    "    filepath = search_test_dir / filename\n",
    "    size = filepath.stat().st_size\n",
    "\n",
    "# Test search functionality\n",
    "\n",
    "search_queries = [\n",
    "    {\n",
    "        'query': 'accuracy',\n",
    "        'description': 'Find files containing \"accuracy\"'\n",
    "    },\n",
    "    {\n",
    "        'query': 'learning_rate',\n",
    "        'description': 'Find files containing \"learning_rate\"'\n",
    "    },\n",
    "    {\n",
    "        'query': 'ResNet',\n",
    "        'description': 'Find files containing \"ResNet\"'\n",
    "    },\n",
    "    {\n",
    "        'query': 'import.*pandas',\n",
    "        'description': 'Find files with pandas import (regex)'\n",
    "    },\n",
    "    {\n",
    "        'query': '0\\.[0-9]+',\n",
    "        'description': 'Find files with decimal numbers (regex)'\n",
    "    }\n",
    "]\n",
    "\n",
    "for search_test in search_queries:\n",
    "    try:\n",
    "        \n",
    "        # Search using scitex.utils.search\n",
    "        search_results = scitex.utils.search(\n",
    "            patterns=search_test['query'],\n",
    "            directory=str(search_test_dir)\n",
    "        )\n",
    "        \n",
    "        if search_results:\n",
    "            for result in search_results:\n",
    "        else:\n",
    "            \n",
    "    except Exception as e:\n",
    "\n",
    "# File content analysis\n",
    "\n",
    "for filename, content in test_files_content.items():\n",
    "    \n",
    "    # Basic statistics\n",
    "    lines = content.split('\\n')\n",
    "    words = content.split()\n",
    "    chars = len(content)\n",
    "    \n",
    "    \n",
    "    # Find numbers in content\n",
    "    import re\n",
    "    numbers = re.findall(r'\\b\\d+\\.\\d+\\b|\\b\\d+\\b', content)\n",
    "    if numbers:\n",
    "    \n",
    "    # Find common keywords\n",
    "    keywords = ['accuracy', 'model', 'data', 'training', 'learning', 'neural', 'network']\n",
    "    found_keywords = [kw for kw in keywords if kw.lower() in content.lower()]\n",
    "    if found_keywords:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Email and Communication\n",
    "\n",
    "### 5.1 Email Utilities (Demonstration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Email utilities demonstration (without actually sending emails)\n",
    "\n",
    "# Note: We won't actually send emails in this demo\n",
    "\n",
    "# Email configuration examples\n",
    "email_configs = {\n",
    "    'experiment_completion': {\n",
    "        'subject': 'Experiment Completed Successfully',\n",
    "        'body': '''\n",
    "Dear Researcher,\n",
    "\n",
    "Your machine learning experiment has completed successfully.\n",
    "\n",
    "Results Summary:\n",
    "- Training Accuracy: 94.5%\n",
    "- Validation Accuracy: 89.2%\n",
    "- Training Time: 2.5 hours\n",
    "- Model Size: 45.2 MB\n",
    "\n",
    "The trained model has been saved to:\n",
    "/models/experiment_20240115_142330.pkl\n",
    "\n",
    "Detailed logs are available in:\n",
    "/logs/training_20240115.log\n",
    "\n",
    "Best regards,\n",
    "Automated Training System\n",
    "''',\n",
    "        'priority': 'normal'\n",
    "    },\n",
    "    'error_notification': {\n",
    "        'subject': 'URGENT: Training Failed - Action Required',\n",
    "        'body': '''\n",
    "ATTENTION: Your machine learning experiment has failed.\n",
    "\n",
    "Error Details:\n",
    "- Error Type: OutOfMemoryError\n",
    "- Error Message: CUDA out of memory\n",
    "- Batch Size: 128 (consider reducing)\n",
    "- Model: ResNet-50\n",
    "- Epoch: 15/100\n",
    "\n",
    "Suggested Actions:\n",
    "1. Reduce batch size to 64 or 32\n",
    "2. Use gradient accumulation\n",
    "3. Enable mixed precision training\n",
    "4. Use a smaller model variant\n",
    "\n",
    "Error log: /logs/error_20240115_143045.log\n",
    "\n",
    "Please restart the training with adjusted parameters.\n",
    "\n",
    "Automated Training System\n",
    "''',\n",
    "        'priority': 'high'\n",
    "    },\n",
    "    'weekly_summary': {\n",
    "        'subject': 'Weekly Training Summary',\n",
    "        'body': '''\n",
    "Weekly Training Summary (Week of Jan 15-21, 2024)\n",
    "\n",
    "Experiments Completed: 12\n",
    "Success Rate: 91.7% (11/12)\n",
    "Total Training Time: 18.5 hours\n",
    "Best Model Accuracy: 96.2%\n",
    "\n",
    "Top Performing Models:\n",
    "1. ResNet-101: 96.2% accuracy\n",
    "2. EfficientNet-B4: 95.8% accuracy\n",
    "3. DenseNet-121: 94.9% accuracy\n",
    "\n",
    "Resource Usage:\n",
    "- GPU Hours: 18.5\n",
    "- Storage Used: 2.3 GB\n",
    "- Models Saved: 11\n",
    "\n",
    "Upcoming Experiments:\n",
    "- Vision Transformer evaluation\n",
    "- Hyperparameter optimization\n",
    "- Cross-validation study\n",
    "\n",
    "Have a great week!\n",
    "Training Management System\n",
    "''',\n",
    "        'priority': 'low'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Demonstrate email preparation\n",
    "for email_type, config in email_configs.items():\n",
    "    \n",
    "    # Show email preview (first few lines)\n",
    "    body_lines = config['body'].strip().split('\\n')\n",
    "    preview_lines = body_lines[:3]\n",
    "\n",
    "# Email sending function demonstration (mock)\n",
    "def mock_send_email(subject, body, recipient, priority='normal'):\n",
    "    \"\"\"Mock email sending function for demonstration.\"\"\"\n",
    "    return True\n",
    "\n",
    "# Demonstrate email sending workflow\n",
    "\n",
    "recipients = [\n",
    "    'researcher@university.edu',\n",
    "    'admin@lab.org',\n",
    "    'team@company.com'\n",
    "]\n",
    "\n",
    "# Simulate sending different types of notifications\n",
    "notification_scenarios = [\n",
    "    {\n",
    "        'trigger': 'experiment_completed',\n",
    "        'email_type': 'experiment_completion',\n",
    "        'recipient': 'researcher@university.edu'\n",
    "    },\n",
    "    {\n",
    "        'trigger': 'training_failed',\n",
    "        'email_type': 'error_notification',\n",
    "        'recipient': 'admin@lab.org'\n",
    "    },\n",
    "    {\n",
    "        'trigger': 'weekly_report',\n",
    "        'email_type': 'weekly_summary',\n",
    "        'recipient': 'team@company.com'\n",
    "    }\n",
    "]\n",
    "\n",
    "for scenario in notification_scenarios:\n",
    "    email_config = email_configs[scenario['email_type']]\n",
    "    \n",
    "    \n",
    "    # Simulate email sending\n",
    "    try:\n",
    "        # In a real implementation, you would use scitex.utils.send_gmail here\n",
    "        result = mock_send_email(\n",
    "            subject=email_config['subject'],\n",
    "            body=email_config['body'],\n",
    "            recipient=scenario['recipient'],\n",
    "            priority=email_config['priority']\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Practical Applications\n",
    "\n",
    "### 6.1 Experiment Management System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Comprehensive experiment management system\n",
    "class ExperimentManager:\n",
    "    def __init__(self, base_dir):\n",
    "        self.base_dir = Path(base_dir)\n",
    "        self.base_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Create subdirectories\n",
    "        self.experiments_dir = self.base_dir / \"experiments\"\n",
    "        self.logs_dir = self.base_dir / \"logs\"\n",
    "        self.results_dir = self.base_dir / \"results\"\n",
    "        \n",
    "        for dir_path in [self.experiments_dir, self.logs_dir, self.results_dir]:\n",
    "            dir_path.mkdir(exist_ok=True)\n",
    "        \n",
    "        self.experiment_history = []\n",
    "    \n",
    "    def create_experiment(self, name, parameters):\n",
    "        \"\"\"Create a new experiment configuration.\"\"\"\n",
    "        import datetime\n",
    "        \n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        experiment_id = f\"{name}_{timestamp}\"\n",
    "        \n",
    "        experiment = {\n",
    "            'id': experiment_id,\n",
    "            'name': name,\n",
    "            'parameters': parameters,\n",
    "            'created': timestamp,\n",
    "            'status': 'created',\n",
    "            'results': None,\n",
    "            'log_file': self.logs_dir / f\"{experiment_id}.log\",\n",
    "            'results_file': self.results_dir / f\"{experiment_id}_results.json\"\n",
    "        }\n",
    "        \n",
    "        # Save experiment configuration\n",
    "        config_file = self.experiments_dir / f\"{experiment_id}_config.json\"\n",
    "        import json\n",
    "        with open(config_file, 'w') as f:\n",
    "            json.dump(experiment, f, indent=2, default=str)\n",
    "        \n",
    "        self.experiment_history.append(experiment)\n",
    "        \n",
    "        return experiment\n",
    "    \n",
    "    def run_parameter_sweep(self, base_name, parameter_space):\n",
    "        \"\"\"Run experiments for all parameter combinations.\"\"\"\n",
    "        \n",
    "        # Count total combinations\n",
    "        total_combinations = scitex.utils.count_grids(parameter_space)\n",
    "        \n",
    "        # Generate experiments\n",
    "        experiments = []\n",
    "        for i, params in enumerate(scitex.utils.yield_grids(parameter_space), 1):\n",
    "            experiment_name = f\"{base_name}_run_{i:03d}\"\n",
    "            experiment = self.create_experiment(experiment_name, params)\n",
    "            experiments.append(experiment)\n",
    "            \n",
    "            # Simulate running the experiment\n",
    "            self.simulate_experiment_run(experiment)\n",
    "        \n",
    "        return experiments\n",
    "    \n",
    "    def simulate_experiment_run(self, experiment):\n",
    "        \"\"\"Simulate running an experiment.\"\"\"\n",
    "        import random\n",
    "        import time\n",
    "        \n",
    "        # Simulate some processing time\n",
    "        time.sleep(0.1)\n",
    "        \n",
    "        # Update status\n",
    "        experiment['status'] = 'running'\n",
    "        \n",
    "        # Write to log file\n",
    "        with open(experiment['log_file'], 'w') as f:\n",
    "            f.write(f\"Experiment: {experiment['id']}\\n\")\n",
    "            f.write(f\"Parameters: {experiment['parameters']}\\n\")\n",
    "            f.write(f\"Status: {experiment['status']}\\n\")\n",
    "            f.write(f\"Started: {time.strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        \n",
    "        # Simulate results based on parameters\n",
    "        base_performance = 0.7\n",
    "        \n",
    "        # Add parameter-based adjustments\n",
    "        params = experiment['parameters']\n",
    "        if 'learning_rate' in params:\n",
    "            if params['learning_rate'] == 0.01:\n",
    "                base_performance += 0.1\n",
    "            elif params['learning_rate'] == 0.1:\n",
    "                base_performance -= 0.05\n",
    "        \n",
    "        if 'batch_size' in params:\n",
    "            if params['batch_size'] == 64:\n",
    "                base_performance += 0.05\n",
    "        \n",
    "        # Add random variation\n",
    "        performance = base_performance + random.uniform(-0.1, 0.1)\n",
    "        performance = max(0.0, min(1.0, performance))\n",
    "        \n",
    "        # Create results\n",
    "        results = {\n",
    "            'accuracy': performance,\n",
    "            'precision': performance * random.uniform(0.9, 1.1),\n",
    "            'recall': performance * random.uniform(0.9, 1.1),\n",
    "            'training_time': random.uniform(30, 120),  # seconds\n",
    "            'memory_usage': random.uniform(500, 2000),  # MB\n",
    "            'converged': performance > 0.6\n",
    "        }\n",
    "        \n",
    "        # Clamp precision and recall\n",
    "        results['precision'] = max(0.0, min(1.0, results['precision']))\n",
    "        results['recall'] = max(0.0, min(1.0, results['recall']))\n",
    "        \n",
    "        experiment['results'] = results\n",
    "        experiment['status'] = 'completed' if results['converged'] else 'failed'\n",
    "        \n",
    "        # Save results\n",
    "        import json\n",
    "        with open(experiment['results_file'], 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "        \n",
    "        # Update log\n",
    "        with open(experiment['log_file'], 'a') as f:\n",
    "            f.write(f\"Completed: {time.strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "            f.write(f\"Results: {results}\\n\")\n",
    "            f.write(f\"Final status: {experiment['status']}\\n\")\n",
    "    \n",
    "    def analyze_results(self):\n",
    "        \"\"\"Analyze results across all experiments.\"\"\"\n",
    "        \n",
    "        if not self.experiment_history:\n",
    "            return\n",
    "        \n",
    "        # Filter completed experiments\n",
    "        completed = [e for e in self.experiment_history if e['status'] == 'completed']\n",
    "        failed = [e for e in self.experiment_history if e['status'] == 'failed']\n",
    "        \n",
    "        \n",
    "        if completed:\n",
    "            # Performance statistics\n",
    "            accuracies = [e['results']['accuracy'] for e in completed]\n",
    "            training_times = [e['results']['training_time'] for e in completed]\n",
    "            memory_usage = [e['results']['memory_usage'] for e in completed]\n",
    "            \n",
    "            \n",
    "            # Find best experiment\n",
    "            best_experiment = max(completed, key=lambda x: x['results']['accuracy'])\n",
    "            \n",
    "        # Storage analysis\n",
    "        total_log_size = sum(f.stat().st_size for f in self.logs_dir.rglob('*.log'))\n",
    "        total_results_size = sum(f.stat().st_size for f in self.results_dir.rglob('*.json'))\n",
    "        total_config_size = sum(f.stat().st_size for f in self.experiments_dir.rglob('*.json'))\n",
    "        \n",
    "    \n",
    "    def generate_summary_report(self):\n",
    "        \"\"\"Generate a comprehensive summary report.\"\"\"\n",
    "        report_file = self.base_dir / \"experiment_summary.txt\"\n",
    "        \n",
    "        with open(report_file, 'w') as f:\n",
    "            f.write(\"EXPERIMENT SUMMARY REPORT\\n\")\n",
    "            f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "            \n",
    "            # System information\n",
    "            f.write(\"System Information:\\n\")\n",
    "            f.write(\"-\" * 20 + \"\\n\")\n",
    "            f.write(f\"Hostname: {scitex.utils.get_hostname()}\\n\")\n",
    "            f.write(f\"Username: {scitex.utils.get_username()}\\n\")\n",
    "            f.write(f\"Git branch: {scitex.utils.get_git_branch(scitex)}\\n\")\n",
    "            f.write(f\"Generated: {time.strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "            \n",
    "            # Experiment statistics\n",
    "            completed = [e for e in self.experiment_history if e['status'] == 'completed']\n",
    "            \n",
    "            f.write(\"Experiment Statistics:\\n\")\n",
    "            f.write(\"-\" * 22 + \"\\n\")\n",
    "            f.write(f\"Total experiments: {len(self.experiment_history)}\\n\")\n",
    "            f.write(f\"Completed: {len(completed)}\\n\")\n",
    "            f.write(f\"Success rate: {len(completed)/len(self.experiment_history)*100:.1f}%\\n\\n\")\n",
    "            \n",
    "            if completed:\n",
    "                accuracies = [e['results']['accuracy'] for e in completed]\n",
    "                f.write(f\"Best accuracy: {np.max(accuracies):.4f}\\n\")\n",
    "                f.write(f\"Mean accuracy: {np.mean(accuracies):.4f}\\n\")\n",
    "                f.write(f\"Std accuracy: {np.std(accuracies):.4f}\\n\\n\")\n",
    "                \n",
    "                # Top experiments\n",
    "                top_experiments = sorted(completed, key=lambda x: x['results']['accuracy'], reverse=True)[:3]\n",
    "                f.write(\"Top 3 Experiments:\\n\")\n",
    "                f.write(\"-\" * 18 + \"\\n\")\n",
    "                for i, exp in enumerate(top_experiments, 1):\n",
    "                    f.write(f\"{i}. {exp['id']} - Accuracy: {exp['results']['accuracy']:.4f}\\n\")\n",
    "                    f.write(f\"   Parameters: {exp['parameters']}\\n\")\n",
    "            \n",
    "            # Footer\n",
    "            f.write(\"\\n\" + scitex.utils.gen_footer(\"user@host\", \"notebook.ipynb\", scitex, \"main\"))\n",
    "        \n",
    "        return report_file\n",
    "\n",
    "# Test the experiment management system\n",
    "experiment_manager = ExperimentManager(data_dir / \"experiment_management\")\n",
    "\n",
    "# Define parameter space for hyperparameter optimization\n",
    "hyperparameter_space = {\n",
    "    'learning_rate': [0.001, 0.01, 0.1],\n",
    "    'batch_size': [32, 64, 128],\n",
    "    'optimizer': ['adam', 'sgd']\n",
    "}\n",
    "\n",
    "# Run parameter sweep\n",
    "experiments = experiment_manager.run_parameter_sweep(\n",
    "    \"neural_network_optimization\",\n",
    "    hyperparameter_space\n",
    ")\n",
    "\n",
    "# Analyze results\n",
    "experiment_manager.analyze_results()\n",
    "\n",
    "# Generate summary report\n",
    "report_file = experiment_manager.generate_summary_report()\n",
    "\n",
    "# Show report content\n",
    "with open(report_file, 'r') as f:\n",
    "    report_content = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Best Practices\n",
    "\n",
    "This tutorial demonstrated the comprehensive utility capabilities of the SciTeX utils module:\n",
    "\n",
    "### Key Features Covered:\n",
    "1. **System Information**: `get_hostname()`, `get_username()`, `get_git_branch()`, `gen_footer()`\n",
    "2. **Grid Operations**: `count_grids()`, `yield_grids()` for parameter space exploration\n",
    "3. **Data Compression**: `compress_hdf5()` for storage optimization\n",
    "4. **Communication**: `send_gmail()`, `notify()`, `ansi_escape()` for notifications\n",
    "5. **Search Utilities**: `search()` for advanced content analysis\n",
    "6. **Experiment Management**: Comprehensive parameter sweep and result analysis\n",
    "7. **File Management**: Automated logging and result storage\n",
    "8. **Report Generation**: Summary reports with system information\n",
    "\n",
    "### Best Practices:\n",
    "- Use **grid operations** for systematic parameter space exploration\n",
    "- Apply **HDF5 compression** for large dataset storage\n",
    "- Implement **notification systems** for long-running experiments\n",
    "- Use **search utilities** for comprehensive data analysis\n",
    "- Create **experiment management** systems for reproducible research\n",
    "- Generate **automated reports** with system information\n",
    "- Use **system information** functions for environment tracking\n",
    "- Implement **proper logging** for experiment tracking\n",
    "\n",
    "### Recommended Workflows:\n",
    "1. **Hyperparameter Optimization**: Use grid operations with experiment management\n",
    "2. **Data Storage**: Apply compression utilities for large datasets\n",
    "3. **Result Analysis**: Use search utilities for pattern detection\n",
    "4. **Communication**: Set up notification systems for experiment completion\n",
    "5. **Documentation**: Generate automated reports with system context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanup = False  # Set to True to remove example files\n",
    "# Cleanup\n",
    "import shutil\n",
    "\n",
    "# cleanup = \"n\"  # input(\"Clean up example files? (y/n): \").lower().startswith('y')\n",
    "if cleanup:\n",
    "    shutil.rmtree(data_dir)\n",
    "else:\n",
    "    total_size = sum(f.stat().st_size for f in data_dir.rglob('*') if f.is_file())\n",
    "    \n",
    "    # Show structure\n",
    "    for item in sorted(data_dir.rglob('*')):\n",
    "        if item.is_dir():\n",
    "        else:\n",
    "            size = scitex.str.readable_bytes(item.stat().st_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}