{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SciTeX Path Module Tutorial\n",
    "\n",
    "This notebook demonstrates the path utilities in SciTeX for file system operations, versioning, and path management."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scitex as stx\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import tempfile\n",
    "import time\n",
    "\n",
    "# Create a temporary directory for examples\n",
    "temp_dir = tempfile.mkdtemp(prefix=\"scitex_path_demo_\")\n",
    "print(f\"Working directory: {temp_dir}\")\n",
    "\n",
    "# Change to temp directory\n",
    "original_dir = os.getcwd()\n",
    "os.chdir(temp_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic Path Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Getting Current Path Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the path of the current file\n",
    "current_path = stx.path.this_path()\n",
    "print(f\"Current file path: {current_path}\")\n",
    "\n",
    "# Note: In Jupyter notebooks, this_path() returns the notebook's path\n",
    "# In regular Python scripts, it returns the script's path\n",
    "\n",
    "# Get various path components\n",
    "print(\"\\nPath components:\")\n",
    "print(f\"  Directory: {os.path.dirname(current_path)}\")\n",
    "print(f\"  Filename: {os.path.basename(current_path)}\")\n",
    "print(f\"  Extension: {os.path.splitext(current_path)[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Path Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split paths into components\n",
    "test_paths = [\n",
    "    \"/home/user/data/experiment.csv\",\n",
    "    \"./results/model_v001.pkl\",\n",
    "    \"../figures/plot.png\",\n",
    "    \"data/raw/sample.txt.gz\"\n",
    "]\n",
    "\n",
    "print(\"Path splitting examples:\")\n",
    "for path in test_paths:\n",
    "    dir_path, filename, ext = stx.path.split(path)\n",
    "    print(f\"\\nPath: {path}\")\n",
    "    print(f\"  Directory: {dir_path}\")\n",
    "    print(f\"  Filename: {filename}\")\n",
    "    print(f\"  Extension: {ext}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Path Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean messy paths\n",
    "messy_paths = [\n",
    "    \"./data/../results//output.txt\",\n",
    "    \"figures/./plots///graph.png\",\n",
    "    \"my data/file with spaces.csv\",\n",
    "    \"~/documents/../downloads/./file.pdf\"\n",
    "]\n",
    "\n",
    "print(\"Path cleaning examples:\")\n",
    "for messy in messy_paths:\n",
    "    clean = stx.path.clean(messy)\n",
    "    print(f\"\\nOriginal: {messy}\")\n",
    "    print(f\"Cleaned:  {clean}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Safe Path Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Creating Safe Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get safe path for current script/notebook\n",
    "spath = stx.path.get_spath()\n",
    "print(f\"Safe path: {spath}\")\n",
    "\n",
    "# The safe path is unique to the current file\n",
    "# It's useful for creating output directories\n",
    "\n",
    "# Create the safe path directory\n",
    "stx.path.mk_spath()\n",
    "print(f\"\\nCreated directory: {os.path.exists(spath)}\")\n",
    "\n",
    "# Use safe path for outputs\n",
    "output_file = os.path.join(spath, \"results.txt\")\n",
    "with open(output_file, 'w') as f:\n",
    "    f.write(\"Results from notebook\\n\")\n",
    "print(f\"\\nCreated file: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. File and Directory Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Setting Up Test Directory Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test directory structure\n",
    "test_structure = {\n",
    "    \"data\": [\n",
    "        \"raw/sample1.csv\",\n",
    "        \"raw/sample2.csv\",\n",
    "        \"processed/cleaned_data.pkl\",\n",
    "        \"processed/features.npy\"\n",
    "    ],\n",
    "    \"models\": [\n",
    "        \"v1/model_001.pkl\",\n",
    "        \"v1/model_002.pkl\",\n",
    "        \"v2/model_001.pkl\",\n",
    "        \"best/final_model.pkl\"\n",
    "    ],\n",
    "    \"figures\": [\n",
    "        \"plots/accuracy.png\",\n",
    "        \"plots/loss.png\",\n",
    "        \"plots/confusion_matrix.pdf\",\n",
    "        \"report.tex\"\n",
    "    ],\n",
    "    \"logs\": [\n",
    "        \"experiment_001.log\",\n",
    "        \"experiment_002.log\",\n",
    "        \"debug.log\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create the structure\n",
    "for base_dir, files in test_structure.items():\n",
    "    for file_path in files:\n",
    "        full_path = os.path.join(base_dir, file_path)\n",
    "        os.makedirs(os.path.dirname(full_path), exist_ok=True)\n",
    "        # Create dummy file\n",
    "        with open(full_path, 'w') as f:\n",
    "            f.write(f\"Dummy content for {file_path}\\n\")\n",
    "\n",
    "print(\"Created test directory structure:\")\n",
    "for root, dirs, files in os.walk('.'):\n",
    "    level = root.replace('.', '').count(os.sep)\n",
    "    indent = ' ' * 2 * level\n",
    "    print(f\"{indent}{os.path.basename(root)}/\")\n",
    "    subindent = ' ' * 2 * (level + 1)\n",
    "    for file in files:\n",
    "        print(f\"{subindent}{file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Finding Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find files by pattern\n",
    "print(\"=== Finding Files ===\")\n",
    "\n",
    "# Find all CSV files\n",
    "csv_files = stx.path.find_file(\"*.csv\")\n",
    "print(\"\\nCSV files found:\")\n",
    "for f in csv_files:\n",
    "    print(f\"  {f}\")\n",
    "\n",
    "# Find all model files\n",
    "model_files = stx.path.find_file(\"model*.pkl\")\n",
    "print(\"\\nModel files found:\")\n",
    "for f in model_files:\n",
    "    print(f\"  {f}\")\n",
    "\n",
    "# Find files with specific pattern in name\n",
    "log_files = stx.path.find_file(\"*experiment*.log\")\n",
    "print(\"\\nExperiment log files:\")\n",
    "for f in log_files:\n",
    "    print(f\"  {f}\")\n",
    "\n",
    "# Find files in specific subdirectory\n",
    "processed_files = stx.path.find_file(\"*\", root=\"data/processed\")\n",
    "print(\"\\nFiles in data/processed:\")\n",
    "for f in processed_files:\n",
    "    print(f\"  {f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Finding Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find directories\n",
    "print(\"=== Finding Directories ===\")\n",
    "\n",
    "# Find all directories\n",
    "all_dirs = stx.path.find_dir(\"*\")\n",
    "print(\"\\nAll directories:\")\n",
    "for d in sorted(all_dirs):\n",
    "    print(f\"  {d}\")\n",
    "\n",
    "# Find version directories\n",
    "version_dirs = stx.path.find_dir(\"v*\")\n",
    "print(\"\\nVersion directories:\")\n",
    "for d in version_dirs:\n",
    "    print(f\"  {d}\")\n",
    "\n",
    "# Find specific named directories\n",
    "plot_dirs = stx.path.find_dir(\"plots\")\n",
    "print(\"\\nPlot directories:\")\n",
    "for d in plot_dirs:\n",
    "    print(f\"  {d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Version Control for Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Automatic Version Incrementing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create versioned files\n",
    "print(\"=== Version Control ===\")\n",
    "\n",
    "# Create initial file\n",
    "base_name = \"results/experiment\"\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "\n",
    "# Generate versioned filenames\n",
    "versions = []\n",
    "for i in range(5):\n",
    "    if i == 0:\n",
    "        # First version\n",
    "        filename = f\"{base_name}_v001.txt\"\n",
    "    else:\n",
    "        # Increment from previous\n",
    "        filename = stx.path.increment_version(versions[-1])\n",
    "    \n",
    "    versions.append(filename)\n",
    "    \n",
    "    # Create the file\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(f\"Experiment version {i+1}\\n\")\n",
    "        f.write(f\"Timestamp: {time.strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "    \n",
    "    print(f\"Created: {filename}\")\n",
    "    time.sleep(0.1)  # Small delay\n",
    "\n",
    "# Find latest version\n",
    "latest = stx.path.find_latest(\"results/experiment_v*.txt\")\n",
    "print(f\"\\nLatest version: {latest}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Custom Version Formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different version formats\n",
    "version_examples = [\n",
    "    \"model_v1.pkl\",\n",
    "    \"data_version_001.csv\",\n",
    "    \"checkpoint_iter_1000.pt\",\n",
    "    \"backup_2024_01_15_v1.zip\"\n",
    "]\n",
    "\n",
    "print(\"Custom version increments:\")\n",
    "for example in version_examples:\n",
    "    next_version = stx.path.increment_version(example)\n",
    "    print(f\"\\n{example}\")\n",
    "    print(f\"  -> {next_version}\")\n",
    "\n",
    "# Create sequence with custom prefix\n",
    "print(\"\\nCustom version sequence:\")\n",
    "custom_base = \"backup/daily_backup\"\n",
    "os.makedirs(\"backup\", exist_ok=True)\n",
    "\n",
    "for i in range(3):\n",
    "    if i == 0:\n",
    "        filename = f\"{custom_base}_rev001.tar.gz\"\n",
    "    else:\n",
    "        filename = stx.path.increment_version(filename, version_prefix=\"rev\")\n",
    "    \n",
    "    Path(filename).touch()\n",
    "    print(f\"  {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Working with Git Repositories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find git root (if in a git repository)\n",
    "try:\n",
    "    git_root = stx.path.find_git_root()\n",
    "    print(f\"Git root found: {git_root}\")\n",
    "    \n",
    "    # Show relative path from git root\n",
    "    current = os.getcwd()\n",
    "    rel_path = os.path.relpath(current, git_root)\n",
    "    print(f\"Current directory relative to git root: {rel_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Not in a git repository or git root not found: {e}\")\n",
    "    \n",
    "    # Simulate git repository for demo\n",
    "    print(\"\\nCreating demo git structure...\")\n",
    "    demo_git = \"demo_project\"\n",
    "    os.makedirs(os.path.join(demo_git, \".git\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(demo_git, \"src\", \"module\"), exist_ok=True)\n",
    "    \n",
    "    # Change to subdirectory\n",
    "    original_cwd = os.getcwd()\n",
    "    os.chdir(os.path.join(demo_git, \"src\", \"module\"))\n",
    "    \n",
    "    # Now find git root\n",
    "    try:\n",
    "        git_root = stx.path.find_git_root()\n",
    "        print(f\"Demo git root: {git_root}\")\n",
    "        print(f\"Current location: {os.getcwd()}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    finally:\n",
    "        os.chdir(original_cwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. File Size Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get file sizes\n",
    "print(\"=== File Sizes ===\")\n",
    "\n",
    "# Create files with different sizes\n",
    "size_test_dir = \"size_test\"\n",
    "os.makedirs(size_test_dir, exist_ok=True)\n",
    "\n",
    "test_files = {\n",
    "    \"small.txt\": b\"Small file content\",\n",
    "    \"medium.dat\": b\"x\" * 1024 * 10,  # 10 KB\n",
    "    \"large.bin\": b\"X\" * 1024 * 1024,  # 1 MB\n",
    "}\n",
    "\n",
    "for filename, content in test_files.items():\n",
    "    path = os.path.join(size_test_dir, filename)\n",
    "    with open(path, 'wb') as f:\n",
    "        f.write(content)\n",
    "    \n",
    "    # Get size in different formats\n",
    "    size_bytes = stx.path.getsize(path)\n",
    "    size_kb = stx.path.getsize(path, unit='KB')\n",
    "    size_mb = stx.path.getsize(path, unit='MB')\n",
    "    \n",
    "    print(f\"\\n{filename}:\")\n",
    "    print(f\"  Bytes: {size_bytes:,}\")\n",
    "    print(f\"  KB: {size_kb:.2f}\")\n",
    "    print(f\"  MB: {size_mb:.4f}\")\n",
    "\n",
    "# Get directory size\n",
    "dir_size = stx.path.getsize(size_test_dir)\n",
    "dir_size_kb = stx.path.getsize(size_test_dir, unit='KB')\n",
    "print(f\"\\nTotal directory size:\")\n",
    "print(f\"  Bytes: {dir_size:,}\")\n",
    "print(f\"  KB: {dir_size_kb:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Practical Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Experiment Output Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperimentManager:\n",
    "    \"\"\"Manage experiment outputs with versioning.\"\"\"\n",
    "    \n",
    "    def __init__(self, base_dir=\"experiments\"):\n",
    "        self.base_dir = base_dir\n",
    "        os.makedirs(base_dir, exist_ok=True)\n",
    "    \n",
    "    def new_experiment(self, name):\n",
    "        \"\"\"Create new experiment directory with version.\"\"\"\n",
    "        # Find existing experiments\n",
    "        pattern = os.path.join(self.base_dir, f\"{name}_v*.exp\")\n",
    "        latest = stx.path.find_latest(pattern)\n",
    "        \n",
    "        if latest:\n",
    "            # Increment version\n",
    "            new_dir = stx.path.increment_version(latest)\n",
    "        else:\n",
    "            # First version\n",
    "            new_dir = os.path.join(self.base_dir, f\"{name}_v001.exp\")\n",
    "        \n",
    "        # Create directory structure\n",
    "        os.makedirs(new_dir)\n",
    "        for subdir in ['data', 'models', 'figures', 'logs']:\n",
    "            os.makedirs(os.path.join(new_dir, subdir))\n",
    "        \n",
    "        return new_dir\n",
    "    \n",
    "    def save_results(self, exp_dir, results):\n",
    "        \"\"\"Save experiment results.\"\"\"\n",
    "        import json\n",
    "        \n",
    "        results_file = os.path.join(exp_dir, \"results.json\")\n",
    "        with open(results_file, 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "    \n",
    "    def list_experiments(self):\n",
    "        \"\"\"List all experiments.\"\"\"\n",
    "        exp_dirs = stx.path.find_dir(\"*.exp\", root=self.base_dir)\n",
    "        return sorted(exp_dirs)\n",
    "\n",
    "# Use the manager\n",
    "manager = ExperimentManager()\n",
    "\n",
    "# Run multiple experiments\n",
    "for i in range(3):\n",
    "    exp_dir = manager.new_experiment(\"neural_network\")\n",
    "    print(f\"\\nCreated experiment: {exp_dir}\")\n",
    "    \n",
    "    # Simulate saving results\n",
    "    results = {\n",
    "        \"accuracy\": 0.85 + i * 0.03,\n",
    "        \"loss\": 0.3 - i * 0.05,\n",
    "        \"parameters\": {\"lr\": 0.001, \"batch_size\": 32}\n",
    "    }\n",
    "    manager.save_results(exp_dir, results)\n",
    "\n",
    "# List all experiments\n",
    "print(\"\\nAll experiments:\")\n",
    "for exp in manager.list_experiments():\n",
    "    print(f\"  {exp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Data Pipeline with Auto-versioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPipeline:\n",
    "    \"\"\"Data processing pipeline with automatic versioning.\"\"\"\n",
    "    \n",
    "    def __init__(self, project_name):\n",
    "        self.project_name = project_name\n",
    "        self.base_dir = stx.path.clean(f\"./projects/{project_name}\")\n",
    "        self._setup_directories()\n",
    "    \n",
    "    def _setup_directories(self):\n",
    "        \"\"\"Setup project directory structure.\"\"\"\n",
    "        dirs = ['raw', 'processed', 'features', 'models', 'reports']\n",
    "        for d in dirs:\n",
    "            os.makedirs(os.path.join(self.base_dir, d), exist_ok=True)\n",
    "    \n",
    "    def save_data(self, data, stage, description=\"\"):\n",
    "        \"\"\"Save data with automatic versioning.\"\"\"\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        \n",
    "        # Determine file extension based on data type\n",
    "        if isinstance(data, pd.DataFrame):\n",
    "            ext = \".csv\"\n",
    "            save_func = lambda path: data.to_csv(path, index=False)\n",
    "        elif isinstance(data, np.ndarray):\n",
    "            ext = \".npy\"\n",
    "            save_func = lambda path: np.save(path, data)\n",
    "        else:\n",
    "            ext = \".pkl\"\n",
    "            import pickle\n",
    "            save_func = lambda path: pickle.dump(data, open(path, 'wb'))\n",
    "        \n",
    "        # Generate versioned filename\n",
    "        base_name = os.path.join(self.base_dir, stage, f\"{stage}_data\")\n",
    "        pattern = f\"{base_name}_v*{ext}\"\n",
    "        \n",
    "        latest = stx.path.find_latest(pattern)\n",
    "        if latest:\n",
    "            new_path = stx.path.increment_version(latest)\n",
    "        else:\n",
    "            new_path = f\"{base_name}_v001{ext}\"\n",
    "        \n",
    "        # Save data\n",
    "        save_func(new_path)\n",
    "        \n",
    "        # Save metadata\n",
    "        meta_path = new_path.replace(ext, \"_meta.json\")\n",
    "        import json\n",
    "        metadata = {\n",
    "            \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"description\": description,\n",
    "            \"shape\": getattr(data, 'shape', len(data)) if hasattr(data, '__len__') else None,\n",
    "            \"type\": type(data).__name__\n",
    "        }\n",
    "        with open(meta_path, 'w') as f:\n",
    "            json.dump(metadata, f, indent=2)\n",
    "        \n",
    "        return new_path\n",
    "    \n",
    "    def get_latest(self, stage):\n",
    "        \"\"\"Get latest version of data for a stage.\"\"\"\n",
    "        pattern = os.path.join(self.base_dir, stage, f\"{stage}_data_v*.*\")\n",
    "        files = stx.path.find_file(os.path.basename(pattern), \n",
    "                                  root=os.path.dirname(pattern))\n",
    "        \n",
    "        # Filter out metadata files\n",
    "        data_files = [f for f in files if not f.endswith('_meta.json')]\n",
    "        \n",
    "        if data_files:\n",
    "            return sorted(data_files)[-1]\n",
    "        return None\n",
    "\n",
    "# Use the pipeline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pipeline = DataPipeline(\"ml_project\")\n",
    "\n",
    "# Simulate data processing stages\n",
    "print(\"Processing data through pipeline...\\n\")\n",
    "\n",
    "# Stage 1: Raw data\n",
    "raw_data = pd.DataFrame({\n",
    "    'feature1': np.random.randn(100),\n",
    "    'feature2': np.random.randn(100),\n",
    "    'target': np.random.randint(0, 2, 100)\n",
    "})\n",
    "raw_path = pipeline.save_data(raw_data, 'raw', \"Initial dataset\")\n",
    "print(f\"Saved raw data: {raw_path}\")\n",
    "\n",
    "# Stage 2: Processed data (simulate multiple versions)\n",
    "for i in range(3):\n",
    "    processed_data = raw_data.copy()\n",
    "    processed_data['feature3'] = processed_data['feature1'] * 2 + i\n",
    "    proc_path = pipeline.save_data(processed_data, 'processed', \n",
    "                                  f\"Added feature3 (version {i+1})\")\n",
    "    print(f\"Saved processed data: {proc_path}\")\n",
    "    time.sleep(0.1)\n",
    "\n",
    "# Stage 3: Features\n",
    "features = processed_data[['feature1', 'feature2', 'feature3']].values\n",
    "feat_path = pipeline.save_data(features, 'features', \"Extracted feature matrix\")\n",
    "print(f\"Saved features: {feat_path}\")\n",
    "\n",
    "# Get latest versions\n",
    "print(\"\\nLatest versions:\")\n",
    "for stage in ['raw', 'processed', 'features']:\n",
    "    latest = pipeline.get_latest(stage)\n",
    "    if latest:\n",
    "        print(f\"  {stage}: {os.path.basename(latest)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Advanced Path Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data path from package\n",
    "try:\n",
    "    # This would work with installed packages\n",
    "    data_path = stx.path.get_data_path_from_a_package('scitex')\n",
    "    print(f\"SciTeX data path: {data_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not get package data path: {e}\")\n",
    "\n",
    "# Complex path operations\n",
    "print(\"\\nComplex path examples:\")\n",
    "\n",
    "# Clean and create nested path\n",
    "messy_path = \"./output//results/../final///report.pdf\"\n",
    "clean_path = stx.path.clean(messy_path)\n",
    "print(f\"Cleaned path: {clean_path}\")\n",
    "\n",
    "# Create directory if needed\n",
    "output_dir = os.path.dirname(clean_path)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Find all Python files (excluding certain directories)\n",
    "print(\"\\nFinding Python files...\")\n",
    "py_files = stx.path.find_file(\"*.py\", root=\".\")\n",
    "# Note: find_file automatically excludes /lib/, /env/, /build/ directories\n",
    "print(f\"Found {len(py_files)} Python files\")\n",
    "\n",
    "# Batch rename simulation\n",
    "print(\"\\nBatch versioning example:\")\n",
    "batch_dir = \"batch_test\"\n",
    "os.makedirs(batch_dir, exist_ok=True)\n",
    "\n",
    "# Create initial files\n",
    "for i in range(3):\n",
    "    old_name = os.path.join(batch_dir, f\"data_{i}.txt\")\n",
    "    Path(old_name).touch()\n",
    "    \n",
    "    # Convert to versioned name\n",
    "    new_name = os.path.join(batch_dir, f\"data_{i}_v001.txt\")\n",
    "    os.rename(old_name, new_name)\n",
    "    print(f\"  Renamed: {os.path.basename(old_name)} -> {os.path.basename(new_name)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return to original directory and cleanup\n",
    "os.chdir(original_dir)\n",
    "shutil.rmtree(temp_dir)\n",
    "print(f\"Cleaned up temporary directory: {temp_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Best Practices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Takeaways\n",
    "\n",
    "1. **Path Management**: Clean and normalize paths for cross-platform compatibility\n",
    "2. **File Versioning**: Automatic version incrementing for outputs\n",
    "3. **Smart Search**: Find files and directories with pattern matching\n",
    "4. **Safe Paths**: Generate unique output directories for scripts\n",
    "5. **Git Integration**: Find repository roots programmatically\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Always Clean Paths**:\n",
    "   ```python\n",
    "   path = stx.path.clean(\"./data/../results//output.txt\")\n",
    "   ```\n",
    "\n",
    "2. **Use Versioning for Outputs**:\n",
    "   ```python\n",
    "   latest = stx.path.find_latest(\"model_v*.pkl\")\n",
    "   next_version = stx.path.increment_version(latest)\n",
    "   ```\n",
    "\n",
    "3. **Organize with Safe Paths**:\n",
    "   ```python\n",
    "   output_dir = stx.path.mk_spath()\n",
    "   # Creates unique directory for current script\n",
    "   ```\n",
    "\n",
    "4. **Smart File Search**:\n",
    "   ```python\n",
    "   # Automatically excludes build/env directories\n",
    "   files = stx.path.find_file(\"*.py\")\n",
    "   ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nPath module tutorial completed!\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Use path.clean() for all file paths\")\n",
    "print(\"2. Implement versioning for experiment outputs\")\n",
    "print(\"3. Organize outputs with safe paths\")\n",
    "print(\"4. Use find_file/find_dir for dynamic file discovery\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}