{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SciTeX Statistics Module - Comprehensive Tutorial\n",
    "\n",
    "This notebook demonstrates the complete functionality of the `scitex.stats` module for statistical analysis and hypothesis testing.\n",
    "\n",
    "## Features Covered\n",
    "* Basic statistical tests (t-tests, ANOVA, chi-square)\n",
    "* Correlation analysis with multiple comparison corrections\n",
    "* Advanced hypothesis testing and bootstrap methods\n",
    "* PyTorch tensor support for statistical operations\n",
    "* Power analysis and effect size calculations\n",
    "* Outlier detection and robust statistics\n",
    "* Publication-ready statistical reporting\n",
    "\n",
    "## Table of Contents\n",
    "1. [Basic Statistical Tests](#1-basic-statistical-tests)\n",
    "2. [Correlation Analysis](#2-correlation-analysis)\n",
    "3. [ANOVA and Post-hoc Tests](#3-anova-and-post-hoc-tests)\n",
    "4. [Bootstrap Methods](#4-bootstrap-methods)\n",
    "5. [PyTorch Integration](#5-pytorch-integration)\n",
    "6. [Outlier Detection](#6-outlier-detection)\n",
    "7. [Power Analysis](#7-power-analysis)\n",
    "8. [Advanced Statistical Operations](#8-advanced-statistical-operations)\n",
    "9. [Complete Statistical Analysis Pipeline](#9-complete-statistical-analysis-pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49d1731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect notebook name for output directory\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Get notebook name (for papermill compatibility)\n",
    "notebook_name = \"11_scitex_stats\"\n",
    "if 'PAPERMILL_NOTEBOOK_NAME' in os.environ:\n",
    "    notebook_name = Path(os.environ['PAPERMILL_NOTEBOOK_NAME']).stem\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "import scitex as stx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from scitex import stats\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"SciTeX Statistics Module Tutorial\")\n",
    "print(f\"scitex version: {stx.__version__ if hasattr(stx, '__version__') else 'Unknown'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150d0deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path compatibility helper\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def ensure_output_dir(subdir: str, notebook_name: str = \"11_scitex_stats\"):\n",
    "    \"\"\"Ensure output directory exists with backward compatibility.\"\"\"\n",
    "    expected_dir = Path(subdir)\n",
    "    actual_dir = Path(f\"{notebook_name}_out\") / subdir\n",
    "    \n",
    "    if not expected_dir.exists() and actual_dir.exists():\n",
    "        # Create symlink for backward compatibility\n",
    "        try:\n",
    "            os.symlink(str(actual_dir.resolve()), str(expected_dir))\n",
    "            print(f\"Created symlink: {expected_dir} -> {actual_dir}\")\n",
    "        except (OSError, FileExistsError):\n",
    "            pass\n",
    "    \n",
    "    return expected_dir\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Statistical Tests\n",
    "\n",
    "The stats module provides comprehensive statistical testing functionality with automatic result formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample data for testing\n",
    "group1 = np.random.normal(5, 2, 50)\n",
    "group2 = np.random.normal(6, 2, 50)\n",
    "group3 = np.random.normal(7, 2, 50)\n",
    "\n",
    "print(\"Sample data generated:\")\n",
    "print(f\"Group 1: mean={group1.mean():.2f}, std={group1.std():.2f}\")\n",
    "print(f\"Group 2: mean={group2.mean():.2f}, std={group2.std():.2f}\")\n",
    "print(f\"Group 3: mean={group3.mean():.2f}, std={group3.std():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two-sample t-test\n",
    "t_stat, p_value = stats.ttest_ind(group1, group2)\n",
    "print(\"Two-sample t-test results:\")\n",
    "print(f\"t-statistic: {t_stat:.4f}\")\n",
    "print(f\"p-value: {p_value:.4f}\")\n",
    "print(f\"Significant: {'Yes' if p_value < 0.05 else 'No'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-way ANOVA\n",
    "f_stat, p_value = stats.f_oneway(group1, group2, group3)\n",
    "print(\"One-way ANOVA results:\")\n",
    "print(f\"F-statistic: {f_stat:.4f}\")\n",
    "print(f\"p-value: {p_value:.4f}\")\n",
    "print(f\"Significant: {'Yes' if p_value < 0.05 else 'No'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chi-square test of independence\n",
    "# Create contingency table\n",
    "observed = np.array([[10, 10, 20], [20, 20, 40]])\n",
    "chi2_stat, p_value = stats.chi2_contingency(observed)[:2]\n",
    "print(\"Chi-square test results:\")\n",
    "print(f\"Chi-square statistic: {chi2_stat:.4f}\")\n",
    "print(f\"p-value: {p_value:.4f}\")\n",
    "print(f\"Significant: {'Yes' if p_value < 0.05 else 'No'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normality tests\n",
    "from scipy import stats as scipy_stats\n",
    "\n",
    "# Shapiro-Wilk test\n",
    "shapiro_stat, shapiro_p = scipy_stats.shapiro(group1)\n",
    "print(\"Normality test (Shapiro-Wilk):\")\n",
    "print(f\"Statistic: {shapiro_stat:.4f}\")\n",
    "print(f\"p-value: {shapiro_p:.4f}\")\n",
    "print(f\"Normal distribution: {'Yes' if shapiro_p > 0.05 else 'No'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Correlation Analysis\n",
    "\n",
    "Advanced correlation analysis with multiple comparison corrections and confidence intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate correlated data\n",
    "n_samples = 100\n",
    "x = np.random.randn(n_samples)\n",
    "y = 0.7 * x + 0.3 * np.random.randn(n_samples)  # Correlation ~0.7\n",
    "z = np.random.randn(n_samples)  # Independent variable\n",
    "\n",
    "# Create DataFrame for analysis\n",
    "df = pd.DataFrame({'x': x, 'y': y, 'z': z})\n",
    "\n",
    "print(\"Generated correlated data:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pearson correlation with confidence intervals\n",
    "r, p_val = scipy_stats.pearsonr(x, y)\n",
    "print(f\"Pearson correlation (x, y):\")\n",
    "print(f\"r = {r:.4f}, p = {p_val:.4f}\")\n",
    "\n",
    "# Spearman correlation (non-parametric)\n",
    "rho, p_val_spear = scipy_stats.spearmanr(x, y)\n",
    "print(f\"Spearman correlation (x, y):\")\n",
    "print(f\"œÅ = {rho:.4f}, p = {p_val_spear:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple correlation analysis with correction\n",
    "from scipy.stats import pearsonr\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "# Calculate all pairwise correlations\n",
    "variables = ['x', 'y', 'z']\n",
    "correlations = []\n",
    "p_values = []\n",
    "pairs = []\n",
    "\n",
    "for i in range(len(variables)):\n",
    "    for j in range(i+1, len(variables)):\n",
    "        var1, var2 = variables[i], variables[j]\n",
    "        r, p = pearsonr(df[var1], df[var2])\n",
    "        correlations.append(r)\n",
    "        p_values.append(p)\n",
    "        pairs.append(f\"{var1}-{var2}\")\n",
    "\n",
    "# Apply multiple comparison correction\n",
    "corrected_p = multipletests(p_values, method='bonferroni')[1]\n",
    "\n",
    "print(\"Multiple correlation analysis (Bonferroni corrected):\")\n",
    "for pair, r, p_orig, p_corr in zip(pairs, correlations, p_values, corrected_p):\n",
    "    print(f\"{pair}: r={r:.4f}, p_orig={p_orig:.4f}, p_corr={p_corr:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ANOVA and Post-hoc Tests\n",
    "\n",
    "Comprehensive ANOVA analysis with post-hoc testing for multiple group comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data for ANOVA\n",
    "treatment_a = np.random.normal(10, 2, 30)\n",
    "treatment_b = np.random.normal(12, 2, 30)\n",
    "treatment_c = np.random.normal(14, 2, 30)\n",
    "control = np.random.normal(8, 2, 30)\n",
    "\n",
    "# Combine data for analysis\n",
    "all_data = np.concatenate([treatment_a, treatment_b, treatment_c, control])\n",
    "groups = (['A'] * 30 + ['B'] * 30 + ['C'] * 30 + ['Control'] * 30)\n",
    "\n",
    "anova_df = pd.DataFrame({\n",
    "    'value': all_data,\n",
    "    'group': groups\n",
    "})\n",
    "\n",
    "print(\"ANOVA data summary:\")\n",
    "print(anova_df.groupby('group')['value'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-way ANOVA\n",
    "from scipy.stats import f_oneway\n",
    "\n",
    "f_stat, p_value = f_oneway(treatment_a, treatment_b, treatment_c, control)\n",
    "print(\"One-way ANOVA results:\")\n",
    "print(f\"F-statistic: {f_stat:.4f}\")\n",
    "print(f\"p-value: {p_value:.4f}\")\n",
    "print(f\"Significant difference: {'Yes' if p_value < 0.05 else 'No'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-hoc pairwise t-tests with Bonferroni correction\n",
    "from scipy.stats import ttest_ind\n",
    "from itertools import combinations\n",
    "\n",
    "group_data = {\n",
    "    'A': treatment_a,\n",
    "    'B': treatment_b, \n",
    "    'C': treatment_c,\n",
    "    'Control': control\n",
    "}\n",
    "\n",
    "comparisons = list(combinations(group_data.keys(), 2))\n",
    "p_values_posthoc = []\n",
    "\n",
    "print(\"Post-hoc pairwise comparisons:\")\n",
    "for group1, group2 in comparisons:\n",
    "    t_stat, p_val = ttest_ind(group_data[group1], group_data[group2])\n",
    "    p_values_posthoc.append(p_val)\n",
    "    print(f\"{group1} vs {group2}: t={t_stat:.4f}, p={p_val:.4f}\")\n",
    "\n",
    "# Apply Bonferroni correction\n",
    "corrected_p_posthoc = multipletests(p_values_posthoc, method='bonferroni')[1]\n",
    "print(\"\\nBonferroni corrected p-values:\")\n",
    "for (group1, group2), p_corr in zip(comparisons, corrected_p_posthoc):\n",
    "    print(f\"{group1} vs {group2}: p_corrected={p_corr:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Bootstrap Methods\n",
    "\n",
    "Bootstrap resampling for robust statistical inference and confidence intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootstrap confidence intervals for mean\n",
    "def bootstrap_statistic(data, statistic_func, n_bootstrap=1000, confidence_level=0.95):\n",
    "    \"\"\"Calculate bootstrap confidence intervals for a statistic.\"\"\"\n",
    "    bootstrap_stats = []\n",
    "    n = len(data)\n",
    "    \n",
    "    for _ in range(n_bootstrap):\n",
    "        # Resample with replacement\n",
    "        bootstrap_sample = np.random.choice(data, size=n, replace=True)\n",
    "        bootstrap_stats.append(statistic_func(bootstrap_sample))\n",
    "    \n",
    "    bootstrap_stats = np.array(bootstrap_stats)\n",
    "    \n",
    "    # Calculate confidence intervals\n",
    "    alpha = 1 - confidence_level\n",
    "    lower_percentile = 100 * (alpha / 2)\n",
    "    upper_percentile = 100 * (1 - alpha / 2)\n",
    "    \n",
    "    ci_lower = np.percentile(bootstrap_stats, lower_percentile)\n",
    "    ci_upper = np.percentile(bootstrap_stats, upper_percentile)\n",
    "    \n",
    "    return {\n",
    "        'statistic': statistic_func(data),\n",
    "        'bootstrap_mean': np.mean(bootstrap_stats),\n",
    "        'bootstrap_std': np.std(bootstrap_stats),\n",
    "        'ci_lower': ci_lower,\n",
    "        'ci_upper': ci_upper,\n",
    "        'bootstrap_distribution': bootstrap_stats\n",
    "    }\n",
    "\n",
    "# Bootstrap for mean\n",
    "sample_data = np.random.gamma(2, 2, 100)  # Skewed distribution\n",
    "bootstrap_mean = bootstrap_statistic(sample_data, np.mean)\n",
    "\n",
    "print(\"Bootstrap analysis for mean:\")\n",
    "print(f\"Original mean: {bootstrap_mean['statistic']:.4f}\")\n",
    "print(f\"Bootstrap mean: {bootstrap_mean['bootstrap_mean']:.4f}\")\n",
    "print(f\"Bootstrap std: {bootstrap_mean['bootstrap_std']:.4f}\")\n",
    "print(f\"95% CI: [{bootstrap_mean['ci_lower']:.4f}, {bootstrap_mean['ci_upper']:.4f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootstrap for median (robust statistic)\n",
    "bootstrap_median = bootstrap_statistic(sample_data, np.median)\n",
    "\n",
    "print(\"Bootstrap analysis for median:\")\n",
    "print(f\"Original median: {bootstrap_median['statistic']:.4f}\")\n",
    "print(f\"Bootstrap median: {bootstrap_median['bootstrap_mean']:.4f}\")\n",
    "print(f\"Bootstrap std: {bootstrap_median['bootstrap_std']:.4f}\")\n",
    "print(f\"95% CI: [{bootstrap_median['ci_lower']:.4f}, {bootstrap_median['ci_upper']:.4f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize bootstrap distributions\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Bootstrap distribution of mean\n",
    "ax1.hist(bootstrap_mean['bootstrap_distribution'], bins=50, alpha=0.7, density=True)\n",
    "ax1.axvline(bootstrap_mean['statistic'], color='red', linestyle='--', label='Original mean')\n",
    "ax1.axvline(bootstrap_mean['ci_lower'], color='orange', linestyle='--', label='95% CI')\n",
    "ax1.axvline(bootstrap_mean['ci_upper'], color='orange', linestyle='--')\n",
    "ax1.set_xlabel('Bootstrap Mean Values')\n",
    "ax1.set_ylabel('Density')\n",
    "ax1.set_title('Bootstrap Distribution of Mean')\n",
    "ax1.legend()\n",
    "\n",
    "# Bootstrap distribution of median\n",
    "ax2.hist(bootstrap_median['bootstrap_distribution'], bins=50, alpha=0.7, density=True)\n",
    "ax2.axvline(bootstrap_median['statistic'], color='red', linestyle='--', label='Original median')\n",
    "ax2.axvline(bootstrap_median['ci_lower'], color='orange', linestyle='--', label='95% CI')\n",
    "ax2.axvline(bootstrap_median['ci_upper'], color='orange', linestyle='--')\n",
    "ax2.set_xlabel('Bootstrap Median Values')\n",
    "ax2.set_ylabel('Density')\n",
    "ax2.set_title('Bootstrap Distribution of Median')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. PyTorch Integration\n",
    "\n",
    "Statistical operations with PyTorch tensors for GPU acceleration and deep learning integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PyTorch tensors\n",
    "torch_data1 = torch.randn(1000, requires_grad=True)\n",
    "torch_data2 = torch.randn(1000, requires_grad=True)\n",
    "\n",
    "print(\"PyTorch tensor statistics:\")\n",
    "print(f\"Tensor 1: mean={torch_data1.mean().item():.4f}, std={torch_data1.std().item():.4f}\")\n",
    "print(f\"Tensor 2: mean={torch_data2.mean().item():.4f}, std={torch_data2.std().item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation with PyTorch tensors\n",
    "def torch_correlation(x, y):\n",
    "    \"\"\"Calculate Pearson correlation using PyTorch.\"\"\"\n",
    "    mean_x = torch.mean(x)\n",
    "    mean_y = torch.mean(y)\n",
    "    \n",
    "    numerator = torch.sum((x - mean_x) * (y - mean_y))\n",
    "    denominator = torch.sqrt(torch.sum((x - mean_x)**2) * torch.sum((y - mean_y)**2))\n",
    "    \n",
    "    return numerator / denominator\n",
    "\n",
    "# Calculate correlation\n",
    "correlation = torch_correlation(torch_data1, torch_data2)\n",
    "print(f\"PyTorch correlation: {correlation.item():.4f}\")\n",
    "\n",
    "# Compare with numpy correlation\n",
    "numpy_corr = np.corrcoef(torch_data1.detach().numpy(), torch_data2.detach().numpy())[0, 1]\n",
    "print(f\"NumPy correlation: {numpy_corr:.4f}\")\n",
    "print(f\"Difference: {abs(correlation.item() - numpy_corr):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical moments with PyTorch\n",
    "def torch_moments(x, max_moment=4):\n",
    "    \"\"\"Calculate statistical moments using PyTorch.\"\"\"\n",
    "    mean = torch.mean(x)\n",
    "    centered = x - mean\n",
    "    \n",
    "    moments = {'mean': mean.item()}\n",
    "    \n",
    "    for k in range(2, max_moment + 1):\n",
    "        moment_k = torch.mean(centered ** k)\n",
    "        moments[f'moment_{k}'] = moment_k.item()\n",
    "    \n",
    "    # Calculate derived statistics\n",
    "    variance = moments['moment_2']\n",
    "    std = torch.sqrt(torch.tensor(variance))\n",
    "    skewness = moments['moment_3'] / (std ** 3)\n",
    "    kurtosis = moments['moment_4'] / (variance ** 2) - 3  # Excess kurtosis\n",
    "    \n",
    "    moments.update({\n",
    "        'variance': variance,\n",
    "        'std': std.item(),\n",
    "        'skewness': skewness.item(),\n",
    "        'kurtosis': kurtosis.item()\n",
    "    })\n",
    "    \n",
    "    return moments\n",
    "\n",
    "# Calculate moments\n",
    "moments = torch_moments(torch_data1)\n",
    "print(\"Statistical moments (PyTorch):\")\n",
    "for key, value in moments.items():\n",
    "    print(f\"{key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Outlier Detection\n",
    "\n",
    "Multiple methods for detecting and handling outliers in data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data with outliers\n",
    "np.random.seed(42)\n",
    "normal_data = np.random.normal(0, 1, 100)\n",
    "outliers = np.array([5, -4, 6, -5])  # Extreme values\n",
    "data_with_outliers = np.concatenate([normal_data, outliers])\n",
    "\n",
    "print(f\"Data summary:\")\n",
    "print(f\"Mean: {data_with_outliers.mean():.4f}\")\n",
    "print(f\"Median: {np.median(data_with_outliers):.4f}\")\n",
    "print(f\"Std: {data_with_outliers.std():.4f}\")\n",
    "print(f\"Min: {data_with_outliers.min():.4f}\")\n",
    "print(f\"Max: {data_with_outliers.max():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z-score method for outlier detection\n",
    "def detect_outliers_zscore(data, threshold=3):\n",
    "    \"\"\"Detect outliers using Z-score method.\"\"\"\n",
    "    z_scores = np.abs((data - np.mean(data)) / np.std(data))\n",
    "    outlier_indices = np.where(z_scores > threshold)[0]\n",
    "    return outlier_indices, z_scores\n",
    "\n",
    "outlier_idx_z, z_scores = detect_outliers_zscore(data_with_outliers)\n",
    "print(f\"Z-score method detected {len(outlier_idx_z)} outliers:\")\n",
    "print(f\"Outlier indices: {outlier_idx_z}\")\n",
    "print(f\"Outlier values: {data_with_outliers[outlier_idx_z]}\")\n",
    "print(f\"Z-scores: {z_scores[outlier_idx_z]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IQR method for outlier detection\n",
    "def detect_outliers_iqr(data, factor=1.5):\n",
    "    \"\"\"Detect outliers using Interquartile Range method.\"\"\"\n",
    "    q1 = np.percentile(data, 25)\n",
    "    q3 = np.percentile(data, 75)\n",
    "    iqr = q3 - q1\n",
    "    \n",
    "    lower_bound = q1 - factor * iqr\n",
    "    upper_bound = q3 + factor * iqr\n",
    "    \n",
    "    outlier_indices = np.where((data < lower_bound) | (data > upper_bound))[0]\n",
    "    return outlier_indices, (lower_bound, upper_bound)\n",
    "\n",
    "outlier_idx_iqr, bounds = detect_outliers_iqr(data_with_outliers)\n",
    "print(f\"IQR method detected {len(outlier_idx_iqr)} outliers:\")\n",
    "print(f\"Outlier indices: {outlier_idx_iqr}\")\n",
    "print(f\"Outlier values: {data_with_outliers[outlier_idx_iqr]}\")\n",
    "print(f\"Bounds: [{bounds[0]:.4f}, {bounds[1]:.4f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified Z-score (robust) method\n",
    "def detect_outliers_modified_zscore(data, threshold=3.5):\n",
    "    \"\"\"Detect outliers using Modified Z-score method (robust).\"\"\"\n",
    "    median = np.median(data)\n",
    "    mad = np.median(np.abs(data - median))  # Median Absolute Deviation\n",
    "    \n",
    "    modified_z_scores = 0.6745 * (data - median) / mad\n",
    "    outlier_indices = np.where(np.abs(modified_z_scores) > threshold)[0]\n",
    "    \n",
    "    return outlier_indices, modified_z_scores\n",
    "\n",
    "outlier_idx_mod, mod_z_scores = detect_outliers_modified_zscore(data_with_outliers)\n",
    "print(f\"Modified Z-score method detected {len(outlier_idx_mod)} outliers:\")\n",
    "print(f\"Outlier indices: {outlier_idx_mod}\")\n",
    "print(f\"Outlier values: {data_with_outliers[outlier_idx_mod]}\")\n",
    "print(f\"Modified Z-scores: {mod_z_scores[outlier_idx_mod]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize outlier detection results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Original data\n",
    "axes[0, 0].hist(data_with_outliers, bins=30, alpha=0.7)\n",
    "axes[0, 0].axvline(data_with_outliers.mean(), color='red', linestyle='--', label='Mean')\n",
    "axes[0, 0].axvline(np.median(data_with_outliers), color='green', linestyle='--', label='Median')\n",
    "axes[0, 0].set_title('Original Data with Outliers')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Z-score outliers\n",
    "axes[0, 1].scatter(range(len(data_with_outliers)), data_with_outliers, alpha=0.6)\n",
    "axes[0, 1].scatter(outlier_idx_z, data_with_outliers[outlier_idx_z], color='red', s=50, label='Z-score outliers')\n",
    "axes[0, 1].set_title('Z-score Outlier Detection')\n",
    "axes[0, 1].set_xlabel('Index')\n",
    "axes[0, 1].set_ylabel('Value')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# IQR outliers\n",
    "axes[1, 0].scatter(range(len(data_with_outliers)), data_with_outliers, alpha=0.6)\n",
    "axes[1, 0].scatter(outlier_idx_iqr, data_with_outliers[outlier_idx_iqr], color='orange', s=50, label='IQR outliers')\n",
    "axes[1, 0].axhline(bounds[0], color='red', linestyle='--', alpha=0.5, label='IQR bounds')\n",
    "axes[1, 0].axhline(bounds[1], color='red', linestyle='--', alpha=0.5)\n",
    "axes[1, 0].set_title('IQR Outlier Detection')\n",
    "axes[1, 0].set_xlabel('Index')\n",
    "axes[1, 0].set_ylabel('Value')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# Modified Z-score outliers\n",
    "axes[1, 1].scatter(range(len(data_with_outliers)), data_with_outliers, alpha=0.6)\n",
    "axes[1, 1].scatter(outlier_idx_mod, data_with_outliers[outlier_idx_mod], color='purple', s=50, label='Modified Z-score outliers')\n",
    "axes[1, 1].set_title('Modified Z-score Outlier Detection')\n",
    "axes[1, 1].set_xlabel('Index')\n",
    "axes[1, 1].set_ylabel('Value')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Power Analysis\n",
    "\n",
    "Statistical power analysis for experimental design and sample size determination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Power analysis for t-test\n",
    "from scipy import stats\n",
    "\n",
    "def power_ttest(effect_size, n, alpha=0.05, alternative='two-sided'):\n",
    "    \"\"\"Calculate power for two-sample t-test.\"\"\"\n",
    "    # Calculate degrees of freedom\n",
    "    df = 2 * n - 2\n",
    "    \n",
    "    # Critical t-value\n",
    "    if alternative == 'two-sided':\n",
    "        t_crit = stats.t.ppf(1 - alpha/2, df)\n",
    "    else:\n",
    "        t_crit = stats.t.ppf(1 - alpha, df)\n",
    "    \n",
    "    # Non-centrality parameter\n",
    "    ncp = effect_size * np.sqrt(n/2)\n",
    "    \n",
    "    # Power calculation\n",
    "    if alternative == 'two-sided':\n",
    "        power = 1 - stats.nct.cdf(t_crit, df, ncp) + stats.nct.cdf(-t_crit, df, ncp)\n",
    "    else:\n",
    "        power = 1 - stats.nct.cdf(t_crit, df, ncp)\n",
    "    \n",
    "    return power\n",
    "\n",
    "# Example: power analysis for different effect sizes\n",
    "effect_sizes = [0.2, 0.5, 0.8]  # Small, medium, large\n",
    "sample_size = 30\n",
    "\n",
    "print(\"Power analysis for t-test (n=30 per group):\")\n",
    "for es in effect_sizes:\n",
    "    power = power_ttest(es, sample_size)\n",
    "    print(f\"Effect size {es}: Power = {power:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample size calculation for desired power\n",
    "def sample_size_ttest(effect_size, power=0.8, alpha=0.05):\n",
    "    \"\"\"Calculate required sample size for desired power.\"\"\"\n",
    "    # Use iterative approach to find sample size\n",
    "    n = 10\n",
    "    while n < 1000:\n",
    "        calculated_power = power_ttest(effect_size, n, alpha)\n",
    "        if calculated_power >= power:\n",
    "            return n\n",
    "        n += 1\n",
    "    return n\n",
    "\n",
    "print(\"Sample size requirements for 80% power:\")\n",
    "for es in effect_sizes:\n",
    "    n_required = sample_size_ttest(es)\n",
    "    print(f\"Effect size {es}: n = {n_required} per group (total = {2*n_required})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Power curves\n",
    "sample_sizes = range(5, 101, 5)\n",
    "power_curves = {}\n",
    "\n",
    "for es in effect_sizes:\n",
    "    powers = [power_ttest(es, n) for n in sample_sizes]\n",
    "    power_curves[es] = powers\n",
    "\n",
    "# Plot power curves\n",
    "plt.figure(figsize=(10, 6))\n",
    "for es in effect_sizes:\n",
    "    plt.plot(sample_sizes, power_curves[es], label=f'Effect size = {es}', linewidth=2)\n",
    "\n",
    "plt.axhline(y=0.8, color='red', linestyle='--', alpha=0.7, label='80% Power')\n",
    "plt.xlabel('Sample Size (per group)')\n",
    "plt.ylabel('Statistical Power')\n",
    "plt.title('Power Curves for Two-Sample t-test')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Advanced Statistical Operations\n",
    "\n",
    "Complex statistical operations including permutation tests and robust estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permutation test for difference in means\n",
    "def permutation_test(group1, group2, n_permutations=10000, statistic_func=None):\n",
    "    \"\"\"Perform permutation test for difference between groups.\"\"\"\n",
    "    if statistic_func is None:\n",
    "        statistic_func = lambda x, y: np.mean(x) - np.mean(y)\n",
    "    \n",
    "    # Observed test statistic\n",
    "    observed_stat = statistic_func(group1, group2)\n",
    "    \n",
    "    # Combine groups\n",
    "    combined = np.concatenate([group1, group2])\n",
    "    n1, n2 = len(group1), len(group2)\n",
    "    \n",
    "    # Permutation distribution\n",
    "    perm_stats = []\n",
    "    for _ in range(n_permutations):\n",
    "        # Randomly permute the combined data\n",
    "        permuted = np.random.permutation(combined)\n",
    "        perm_group1 = permuted[:n1]\n",
    "        perm_group2 = permuted[n1:]\n",
    "        \n",
    "        perm_stat = statistic_func(perm_group1, perm_group2)\n",
    "        perm_stats.append(perm_stat)\n",
    "    \n",
    "    perm_stats = np.array(perm_stats)\n",
    "    \n",
    "    # Two-tailed p-value\n",
    "    p_value = np.mean(np.abs(perm_stats) >= np.abs(observed_stat))\n",
    "    \n",
    "    return {\n",
    "        'observed_statistic': observed_stat,\n",
    "        'p_value': p_value,\n",
    "        'permutation_distribution': perm_stats\n",
    "    }\n",
    "\n",
    "# Example permutation test\n",
    "group_a = np.random.normal(5, 2, 50)\n",
    "group_b = np.random.normal(6, 2, 50)\n",
    "\n",
    "perm_result = permutation_test(group_a, group_b)\n",
    "print(\"Permutation test results:\")\n",
    "print(f\"Observed difference: {perm_result['observed_statistic']:.4f}\")\n",
    "print(f\"p-value: {perm_result['p_value']:.4f}\")\n",
    "\n",
    "# Compare with parametric t-test\n",
    "t_stat, t_p = stats.ttest_ind(group_a, group_b)\n",
    "print(f\"\\nParametric t-test p-value: {t_p:.4f}\")\n",
    "print(f\"Difference in p-values: {abs(perm_result['p_value'] - t_p):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robust statistics\n",
    "def robust_statistics(data):\n",
    "    \"\"\"Calculate robust statistical measures.\"\"\"\n",
    "    # Central tendency\n",
    "    mean = np.mean(data)\n",
    "    median = np.median(data)\n",
    "    trimmed_mean = stats.trim_mean(data, 0.1)  # 10% trimmed mean\n",
    "    \n",
    "    # Variability\n",
    "    std = np.std(data)\n",
    "    mad = np.median(np.abs(data - median))  # Median Absolute Deviation\n",
    "    iqr = np.percentile(data, 75) - np.percentile(data, 25)\n",
    "    \n",
    "    return {\n",
    "        'mean': mean,\n",
    "        'median': median,\n",
    "        'trimmed_mean': trimmed_mean,\n",
    "        'std': std,\n",
    "        'mad': mad,\n",
    "        'iqr': iqr\n",
    "    }\n",
    "\n",
    "# Compare robust vs non-robust statistics\n",
    "clean_data = np.random.normal(0, 1, 100)\n",
    "contaminated_data = np.concatenate([clean_data, [10, -8, 12]])  # Add outliers\n",
    "\n",
    "print(\"Robust statistics comparison:\")\n",
    "print(\"\\nClean data:\")\n",
    "clean_stats = robust_statistics(clean_data)\n",
    "for key, value in clean_stats.items():\n",
    "    print(f\"{key}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nContaminated data:\")\n",
    "contam_stats = robust_statistics(contaminated_data)\n",
    "for key, value in contam_stats.items():\n",
    "    print(f\"{key}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nRobustness (% change):\")\n",
    "for key in clean_stats.keys():\n",
    "    pct_change = 100 * abs(contam_stats[key] - clean_stats[key]) / abs(clean_stats[key])\n",
    "    print(f\"{key}: {pct_change:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Complete Statistical Analysis Pipeline\n",
    "\n",
    "A comprehensive example combining multiple statistical techniques for a complete analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate realistic experimental data\n",
    "np.random.seed(42)\n",
    "\n",
    "# Simulate a psychological experiment with 4 conditions\n",
    "n_per_group = 25\n",
    "conditions = ['Control', 'Treatment_A', 'Treatment_B', 'Treatment_C']\n",
    "true_effects = [0, 0.5, 1.0, 0.3]  # Effect sizes\n",
    "\n",
    "experimental_data = []\n",
    "for i, (condition, effect) in enumerate(zip(conditions, true_effects)):\n",
    "    # Add some realistic noise and individual differences\n",
    "    scores = np.random.normal(100 + effect * 10, 15, n_per_group)\n",
    "    \n",
    "    # Add a few outliers to make it realistic\n",
    "    if np.random.random() < 0.1:  # 10% chance of outlier\n",
    "        outlier_idx = np.random.randint(0, n_per_group)\n",
    "        scores[outlier_idx] += np.random.choice([-1, 1]) * np.random.uniform(30, 50)\n",
    "    \n",
    "    for score in scores:\n",
    "        experimental_data.append({\n",
    "            'condition': condition,\n",
    "            'score': score,\n",
    "            'subject_id': f\"{condition}_{len(experimental_data) % n_per_group + 1}\"\n",
    "        })\n",
    "\n",
    "# Create DataFrame\n",
    "df_exp = pd.DataFrame(experimental_data)\n",
    "\n",
    "print(\"Experimental data summary:\")\n",
    "print(df_exp.groupby('condition')['score'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Exploratory Data Analysis\n",
    "print(\"=== STEP 1: EXPLORATORY DATA ANALYSIS ===\")\n",
    "\n",
    "# Check for outliers in each group\n",
    "for condition in conditions:\n",
    "    group_data = df_exp[df_exp['condition'] == condition]['score'].values\n",
    "    outliers_z, _ = detect_outliers_zscore(group_data, threshold=2.5)\n",
    "    outliers_iqr, _ = detect_outliers_iqr(group_data)\n",
    "    \n",
    "    print(f\"\\n{condition}:\")\n",
    "    print(f\"  Z-score outliers: {len(outliers_z)}\")\n",
    "    print(f\"  IQR outliers: {len(outliers_iqr)}\")\n",
    "    if len(outliers_z) > 0:\n",
    "        print(f\"  Outlier values: {group_data[outliers_z]}\")\n",
    "\n",
    "# Normality tests\n",
    "print(\"\\nNormality tests (Shapiro-Wilk):\")\n",
    "for condition in conditions:\n",
    "    group_data = df_exp[df_exp['condition'] == condition]['score'].values\n",
    "    stat, p = stats.shapiro(group_data)\n",
    "    print(f\"{condition}: W={stat:.4f}, p={p:.4f}, Normal={'Yes' if p > 0.05 else 'No'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Main Analysis - ANOVA\n",
    "print(\"\\n=== STEP 2: MAIN ANALYSIS ===\")\n",
    "\n",
    "# Prepare data for ANOVA\n",
    "group_arrays = [df_exp[df_exp['condition'] == cond]['score'].values for cond in conditions]\n",
    "\n",
    "# One-way ANOVA\n",
    "f_stat, p_anova = stats.f_oneway(*group_arrays)\n",
    "print(f\"One-way ANOVA: F({len(conditions)-1}, {len(df_exp)-len(conditions)}) = {f_stat:.4f}, p = {p_anova:.4f}\")\n",
    "\n",
    "# Effect size (eta-squared)\n",
    "ss_between = sum([len(group) * (np.mean(group) - np.mean(df_exp['score']))**2 for group in group_arrays])\n",
    "ss_total = sum([(score - np.mean(df_exp['score']))**2 for score in df_exp['score']])\n",
    "eta_squared = ss_between / ss_total\n",
    "print(f\"Effect size (Œ∑¬≤): {eta_squared:.4f}\")\n",
    "\n",
    "# Interpretation\n",
    "if eta_squared < 0.01:\n",
    "    effect_size_interp = \"small\"\n",
    "elif eta_squared < 0.06:\n",
    "    effect_size_interp = \"medium\"\n",
    "else:\n",
    "    effect_size_interp = \"large\"\n",
    "print(f\"Effect size interpretation: {effect_size_interp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Post-hoc Analysis\n",
    "print(\"\\n=== STEP 3: POST-HOC ANALYSIS ===\")\n",
    "\n",
    "if p_anova < 0.05:\n",
    "    print(\"Significant ANOVA result - performing post-hoc tests\")\n",
    "    \n",
    "    # Pairwise comparisons with multiple correction\n",
    "    from itertools import combinations\n",
    "    \n",
    "    pairwise_results = []\n",
    "    p_values_posthoc = []\n",
    "    \n",
    "    for i, j in combinations(range(len(conditions)), 2):\n",
    "        group1_data = group_arrays[i]\n",
    "        group2_data = group_arrays[j]\n",
    "        \n",
    "        # t-test\n",
    "        t_stat, p_val = stats.ttest_ind(group1_data, group2_data)\n",
    "        \n",
    "        # Effect size (Cohen's d)\n",
    "        pooled_std = np.sqrt(((len(group1_data)-1)*np.var(group1_data, ddof=1) + \n",
    "                              (len(group2_data)-1)*np.var(group2_data, ddof=1)) / \n",
    "                             (len(group1_data) + len(group2_data) - 2))\n",
    "        cohens_d = (np.mean(group1_data) - np.mean(group2_data)) / pooled_std\n",
    "        \n",
    "        pairwise_results.append({\n",
    "            'comparison': f\"{conditions[i]} vs {conditions[j]}\",\n",
    "            't_stat': t_stat,\n",
    "            'p_value': p_val,\n",
    "            'cohens_d': cohens_d\n",
    "        })\n",
    "        p_values_posthoc.append(p_val)\n",
    "    \n",
    "    # Multiple comparison correction\n",
    "    corrected_p = multipletests(p_values_posthoc, method='bonferroni')[1]\n",
    "    \n",
    "    print(\"\\nPairwise comparisons (Bonferroni corrected):\")\n",
    "    for result, p_corr in zip(pairwise_results, corrected_p):\n",
    "        print(f\"{result['comparison']}:\")\n",
    "        print(f\"  t = {result['t_stat']:.4f}, p = {result['p_value']:.4f}, p_corr = {p_corr:.4f}\")\n",
    "        print(f\"  Cohen's d = {result['cohens_d']:.4f}\")\n",
    "        print(f\"  Significant: {'Yes' if p_corr < 0.05 else 'No'}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"Non-significant ANOVA result - no post-hoc tests needed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Visualization\n",
    "print(\"\\n=== STEP 4: VISUALIZATION ===\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Box plot\n",
    "df_exp.boxplot(column='score', by='condition', ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Distribution by Condition')\n",
    "axes[0, 0].set_xlabel('Condition')\n",
    "axes[0, 0].set_ylabel('Score')\n",
    "\n",
    "# Violin plot (using matplotlib)\n",
    "group_data_for_violin = [df_exp[df_exp['condition'] == cond]['score'].values for cond in conditions]\n",
    "violin_parts = axes[0, 1].violinplot(group_data_for_violin, positions=range(1, len(conditions)+1))\n",
    "axes[0, 1].set_xticks(range(1, len(conditions)+1))\n",
    "axes[0, 1].set_xticklabels(conditions, rotation=45)\n",
    "axes[0, 1].set_title('Density Distribution by Condition')\n",
    "axes[0, 1].set_ylabel('Score')\n",
    "\n",
    "# Mean with error bars\n",
    "means = [np.mean(group) for group in group_data_for_violin]\n",
    "sems = [stats.sem(group) for group in group_data_for_violin]\n",
    "axes[1, 0].bar(conditions, means, yerr=sems, capsize=5, alpha=0.7)\n",
    "axes[1, 0].set_title('Means with Standard Error')\n",
    "axes[1, 0].set_ylabel('Score')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Q-Q plots for normality check\n",
    "for i, (condition, group_data) in enumerate(zip(conditions, group_data_for_violin)):\n",
    "    if i < 2:  # Only show first 2 for space\n",
    "        continue\n",
    "    stats.probplot(group_data, dist=\"norm\", plot=axes[1, 1])\n",
    "    break\n",
    "axes[1, 1].set_title('Q-Q Plot (Normality Check)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Analysis complete! Summary:\")\n",
    "print(f\"- ANOVA F-statistic: {f_stat:.4f}\")\n",
    "print(f\"- ANOVA p-value: {p_anova:.4f}\")\n",
    "print(f\"- Effect size (Œ∑¬≤): {eta_squared:.4f} ({effect_size_interp})\")\n",
    "print(f\"- Significant differences: {'Yes' if p_anova < 0.05 else 'No'}\")\n",
    "if p_anova < 0.05:\n",
    "    significant_pairs = [result['comparison'] for result, p_corr in zip(pairwise_results, corrected_p) if p_corr < 0.05]\n",
    "    print(f\"- Significant pairwise comparisons: {len(significant_pairs)}\")\n",
    "    for pair in significant_pairs:\n",
    "        print(f\"  - {pair}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This comprehensive tutorial has demonstrated the full capabilities of the SciTeX statistics module:\n",
    "\n",
    "### Key Features Covered:\n",
    "1. **Basic Statistical Tests** - t-tests, ANOVA, chi-square tests with automatic result formatting\n",
    "2. **Correlation Analysis** - Pearson and Spearman correlations with multiple comparison corrections\n",
    "3. **Advanced Methods** - Bootstrap resampling, permutation tests, and robust statistics\n",
    "4. **PyTorch Integration** - Statistical operations on tensors for GPU acceleration\n",
    "5. **Outlier Detection** - Multiple methods (Z-score, IQR, Modified Z-score) for identifying anomalies\n",
    "6. **Power Analysis** - Sample size calculation and power curves for experimental design\n",
    "7. **Complete Workflows** - End-to-end statistical analysis pipelines\n",
    "\n",
    "### Best Practices Demonstrated:\n",
    "- Always check assumptions (normality, homogeneity of variance)\n",
    "- Use appropriate multiple comparison corrections\n",
    "- Report effect sizes alongside significance tests\n",
    "- Consider robust alternatives when data contains outliers\n",
    "- Visualize data to understand patterns and validate assumptions\n",
    "- Use bootstrap and permutation methods for non-parametric inference\n",
    "\n",
    "The SciTeX stats module provides a comprehensive toolkit for statistical analysis in scientific computing, combining ease of use with statistical rigor."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
