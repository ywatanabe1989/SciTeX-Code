{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprehensive SciTeX Parallel Processing Module Examples\n",
    "\n",
    "This notebook demonstrates the complete functionality of the `scitex.parallel` module, which provides parallel processing utilities for scientific computing tasks.\n",
    "\n",
    "## Module Overview\n",
    "\n",
    "The `scitex.parallel` module includes:\n",
    "- Parallel function execution using ThreadPoolExecutor\n",
    "- Automatic CPU core detection and utilization\n",
    "- Progress tracking with tqdm integration\n",
    "- Support for multiple return values and tuple handling\n",
    "\n",
    "## Import Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba21393d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect notebook name for output directory\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Get notebook name (for papermill compatibility)\n",
    "notebook_name = \"10_scitex_parallel\"\n",
    "if 'PAPERMILL_NOTEBOOK_NAME' in os.environ:\n",
    "    notebook_name = Path(os.environ['PAPERMILL_NOTEBOOK_NAME']).stem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import scitex parallel module\n",
    "import scitex.parallel as spar\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Available functions in scitex.parallel:\")\n",
    "parallel_attrs = [attr for attr in dir(spar) if not attr.startswith('_')]\n",
    "for i, attr in enumerate(parallel_attrs):\n",
    "    print(f\"{i+1:2d}. {attr}\")\n",
    "\n",
    "print(f\"\\nSystem Information:\")\n",
    "print(f\"CPU cores available: {multiprocessing.cpu_count()}\")\n",
    "print(f\"Default parallel jobs: {multiprocessing.cpu_count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Parallel Execution\n",
    "\n",
    "### Simple Mathematical Operations\n",
    "\n",
    "Let's start with basic parallel execution of mathematical operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Basic parallel mathematical operations\n",
    "print(\"Basic Parallel Mathematical Operations:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Define simple mathematical functions\n",
    "def square(x):\n",
    "    \"\"\"Compute square of a number.\"\"\"\n",
    "    time.sleep(0.01)  # Simulate some computation time\n",
    "    return x ** 2\n",
    "\n",
    "def add_numbers(x, y):\n",
    "    \"\"\"Add two numbers.\"\"\"\n",
    "    time.sleep(0.01)  # Simulate computation time\n",
    "    return x + y\n",
    "\n",
    "def compute_stats(x):\n",
    "    \"\"\"Compute multiple statistics for a number.\"\"\"\n",
    "    time.sleep(0.01)  # Simulate computation time\n",
    "    return x, x**2, x**3, np.sqrt(abs(x))\n",
    "\n",
    "# Test data\n",
    "test_numbers = list(range(1, 21))  # Numbers 1 to 20\n",
    "print(f\"Test data: {test_numbers[:10]}... (20 numbers total)\")\n",
    "\n",
    "# Example 1a: Single argument function\n",
    "print(\"\\n1a. Single argument function (square):\")\n",
    "args_list_single = [(x,) for x in test_numbers]  # Convert to tuple format\n",
    "\n",
    "# Sequential execution for comparison\n",
    "start_time = time.time()\n",
    "sequential_results = [square(x) for x in test_numbers]\n",
    "sequential_time = time.time() - start_time\n",
    "\n",
    "# Parallel execution\n",
    "start_time = time.time()\n",
    "parallel_results = spar.run(square, args_list_single, desc=\"Computing squares\")\n",
    "parallel_time = time.time() - start_time\n",
    "\n",
    "print(f\"Sequential results: {sequential_results[:5]}... (first 5)\")\n",
    "print(f\"Parallel results:   {parallel_results[:5]}... (first 5)\")\n",
    "print(f\"Results match: {sequential_results == parallel_results}\")\n",
    "print(f\"Sequential time: {sequential_time:.4f} seconds\")\n",
    "print(f\"Parallel time:   {parallel_time:.4f} seconds\")\n",
    "print(f\"Speedup: {sequential_time / parallel_time:.2f}x\")\n",
    "\n",
    "# Example 1b: Two argument function\n",
    "print(\"\\n1b. Two argument function (addition):\")\n",
    "args_list_double = [(i, i+10) for i in test_numbers]  # (1,11), (2,12), etc.\n",
    "\n",
    "start_time = time.time()\n",
    "add_results = spar.run(add_numbers, args_list_double, desc=\"Adding numbers\")\n",
    "add_time = time.time() - start_time\n",
    "\n",
    "print(f\"Addition arguments: {args_list_double[:5]}... (first 5)\")\n",
    "print(f\"Addition results:   {add_results[:5]}... (first 5)\")\n",
    "print(f\"Parallel time: {add_time:.4f} seconds\")\n",
    "\n",
    "# Verify results manually\n",
    "expected = [i + (i+10) for i in test_numbers]\n",
    "print(f\"Manual verification: {add_results == expected}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Return Values\n",
    "\n",
    "The parallel module handles functions that return multiple values (tuples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Functions with multiple return values\n",
    "print(\"Functions with Multiple Return Values:\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Test with smaller dataset for clarity\n",
    "test_data = [1, 2, 3, 4, 5]\n",
    "args_list_stats = [(x,) for x in test_data]\n",
    "\n",
    "print(f\"Test data: {test_data}\")\n",
    "print(f\"Function returns: (x, x^2, x^3, sqrt(|x|))\")\n",
    "\n",
    "# Run parallel computation\n",
    "start_time = time.time()\n",
    "multi_results = spar.run(compute_stats, args_list_stats, desc=\"Computing statistics\")\n",
    "multi_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nResults structure: {type(multi_results)}\")\n",
    "print(f\"Number of result arrays: {len(multi_results)}\")\n",
    "\n",
    "if isinstance(multi_results, tuple):\n",
    "    values, squares, cubes, sqrts = multi_results\n",
    "    \n",
    "    print(f\"\\nOriginal values: {values}\")\n",
    "    print(f\"Squares:         {squares}\")\n",
    "    print(f\"Cubes:           {cubes}\")\n",
    "    print(f\"Square roots:    {[f'{x:.3f}' for x in sqrts]}\")\n",
    "    \n",
    "    # Verify results\n",
    "    print(f\"\\nVerification:\")\n",
    "    print(f\"Values correct:  {values == test_data}\")\n",
    "    print(f\"Squares correct: {squares == [x**2 for x in test_data]}\")\n",
    "    print(f\"Cubes correct:   {cubes == [x**3 for x in test_data]}\")\n",
    "    \n",
    "else:\n",
    "    print(f\"Unexpected result format: {multi_results}\")\n",
    "\n",
    "print(f\"\\nExecution time: {multi_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Scientific Computing Applications\n",
    "\n",
    "### Numerical Integration\n",
    "\n",
    "Let's demonstrate parallel processing for numerical integration tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Parallel numerical integration\n",
    "print(\"Parallel Numerical Integration:\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "def monte_carlo_pi(n_samples):\n",
    "    \"\"\"Estimate π using Monte Carlo method.\"\"\"\n",
    "    # Generate random points in unit square\n",
    "    x = np.random.uniform(-1, 1, n_samples)\n",
    "    y = np.random.uniform(-1, 1, n_samples)\n",
    "    \n",
    "    # Count points inside unit circle\n",
    "    inside_circle = (x**2 + y**2) <= 1\n",
    "    pi_estimate = 4 * np.sum(inside_circle) / n_samples\n",
    "    \n",
    "    return pi_estimate, n_samples\n",
    "\n",
    "def integrate_function(a, b, n_points, func_name='sin'):\n",
    "    \"\"\"Integrate a function using trapezoidal rule.\"\"\"\n",
    "    x = np.linspace(a, b, n_points)\n",
    "    \n",
    "    if func_name == 'sin':\n",
    "        y = np.sin(x)\n",
    "    elif func_name == 'cos':\n",
    "        y = np.cos(x)\n",
    "    elif func_name == 'exp':\n",
    "        y = np.exp(-x**2)  # Gaussian\n",
    "    else:\n",
    "        y = x**2  # Parabola\n",
    "    \n",
    "    # Trapezoidal integration\n",
    "    integral = np.trapz(y, x)\n",
    "    return integral, func_name, (a, b)\n",
    "\n",
    "# Example 3a: Monte Carlo π estimation with different sample sizes\n",
    "print(\"3a. Monte Carlo π estimation:\")\n",
    "sample_sizes = [10000, 50000, 100000, 200000, 500000]\n",
    "mc_args = [(n,) for n in sample_sizes]\n",
    "\n",
    "print(f\"Sample sizes: {sample_sizes}\")\n",
    "\n",
    "start_time = time.time()\n",
    "pi_results = spar.run(monte_carlo_pi, mc_args, desc=\"Estimating π\")\n",
    "mc_time = time.time() - start_time\n",
    "\n",
    "if isinstance(pi_results, tuple):\n",
    "    pi_estimates, sample_counts = pi_results\n",
    "    \n",
    "    print(f\"\\nπ estimates: {[f'{pi:.6f}' for pi in pi_estimates]}\")\n",
    "    print(f\"True π:      {np.pi:.6f}\")\n",
    "    print(f\"Errors:      {[f'{abs(pi - np.pi):.6f}' for pi in pi_estimates]}\")\n",
    "    print(f\"Sample sizes: {sample_counts}\")\n",
    "    \n",
    "    # Show convergence\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.semilogx(sample_counts, pi_estimates, 'bo-', label='Estimates')\n",
    "    plt.axhline(y=np.pi, color='r', linestyle='--', label='True π')\n",
    "    plt.xlabel('Sample Size')\n",
    "    plt.ylabel('π Estimate')\n",
    "    plt.title('Monte Carlo Convergence')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    errors = [abs(pi - np.pi) for pi in pi_estimates]\n",
    "    plt.loglog(sample_counts, errors, 'ro-', label='Absolute Error')\n",
    "    plt.xlabel('Sample Size')\n",
    "    plt.ylabel('Absolute Error')\n",
    "    plt.title('Error vs Sample Size')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(f\"Monte Carlo execution time: {mc_time:.4f} seconds\")\n",
    "\n",
    "# Example 3b: Numerical integration of different functions\n",
    "print(\"\\n3b. Numerical integration of various functions:\")\n",
    "integration_tasks = [\n",
    "    (0, np.pi, 1000, 'sin'),      # ∫sin(x)dx from 0 to π = 2\n",
    "    (0, np.pi/2, 1000, 'cos'),    # ∫cos(x)dx from 0 to π/2 = 1\n",
    "    (-2, 2, 1000, 'exp'),         # ∫exp(-x²)dx from -2 to 2 ≈ √π\n",
    "    (0, 2, 1000, 'parabola'),     # ∫x²dx from 0 to 2 = 8/3\n",
    "]\n",
    "\n",
    "print(f\"Integration tasks: {len(integration_tasks)}\")\n",
    "for i, (a, b, n, func) in enumerate(integration_tasks):\n",
    "    print(f\"  {i+1}. ∫{func}(x)dx from {a} to {b} with {n} points\")\n",
    "\n",
    "start_time = time.time()\n",
    "int_results = spar.run(integrate_function, integration_tasks, desc=\"Integrating functions\")\n",
    "int_time = time.time() - start_time\n",
    "\n",
    "if isinstance(int_results, tuple):\n",
    "    integrals, func_names, intervals = int_results\n",
    "    \n",
    "    print(f\"\\nIntegration Results:\")\n",
    "    expected_values = [2.0, 1.0, np.sqrt(np.pi), 8/3]\n",
    "    \n",
    "    for i, (integral, func, interval, expected) in enumerate(zip(integrals, func_names, intervals, expected_values)):\n",
    "        error = abs(integral - expected)\n",
    "        print(f\"  {i+1}. {func}({interval}): {integral:.6f} (expected: {expected:.6f}, error: {error:.6f})\")\n",
    "\n",
    "print(f\"Integration execution time: {int_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing and Analysis\n",
    "\n",
    "Let's demonstrate parallel processing for data analysis tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 4: Parallel data processing and analysis\n",
    "print(\"Parallel Data Processing and Analysis:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "def analyze_dataset(data_id, n_samples, noise_level):\n",
    "    \"\"\"Analyze a synthetic dataset.\"\"\"\n",
    "    # Generate synthetic data\n",
    "    np.random.seed(data_id)  # Ensure reproducibility\n",
    "    \n",
    "    # Create synthetic time series with trend and noise\n",
    "    t = np.linspace(0, 10, n_samples)\n",
    "    signal = np.sin(2 * np.pi * t) + 0.5 * np.cos(4 * np.pi * t)\n",
    "    noise = noise_level * np.random.randn(n_samples)\n",
    "    data = signal + noise\n",
    "    \n",
    "    # Compute statistics\n",
    "    mean_val = np.mean(data)\n",
    "    std_val = np.std(data)\n",
    "    min_val = np.min(data)\n",
    "    max_val = np.max(data)\n",
    "    \n",
    "    # Compute frequency domain features\n",
    "    fft = np.fft.fft(data)\n",
    "    power_spectrum = np.abs(fft)**2\n",
    "    dominant_freq_idx = np.argmax(power_spectrum[1:len(power_spectrum)//2]) + 1\n",
    "    \n",
    "    return (data_id, mean_val, std_val, min_val, max_val, dominant_freq_idx, n_samples)\n",
    "\n",
    "def process_image_batch(batch_id, image_size, filter_type):\n",
    "    \"\"\"Process a batch of synthetic images.\"\"\"\n",
    "    np.random.seed(batch_id * 100)  # Ensure reproducibility\n",
    "    \n",
    "    # Generate synthetic image\n",
    "    image = np.random.randn(image_size, image_size)\n",
    "    \n",
    "    # Apply different filters\n",
    "    if filter_type == 'gaussian':\n",
    "        # Simple Gaussian-like filter (moving average)\n",
    "        from scipy import ndimage\n",
    "        try:\n",
    "            filtered = ndimage.gaussian_filter(image, sigma=1.0)\n",
    "        except ImportError:\n",
    "            # Fallback if scipy not available\n",
    "            filtered = image  # No filtering\n",
    "    elif filter_type == 'edge':\n",
    "        # Simple edge detection (gradient)\n",
    "        filtered = np.gradient(image)[0] + np.gradient(image)[1]\n",
    "    else:\n",
    "        # No filter\n",
    "        filtered = image\n",
    "    \n",
    "    # Compute image statistics\n",
    "    mean_intensity = np.mean(filtered)\n",
    "    std_intensity = np.std(filtered)\n",
    "    total_energy = np.sum(filtered**2)\n",
    "    \n",
    "    return (batch_id, filter_type, mean_intensity, std_intensity, total_energy, image_size)\n",
    "\n",
    "# Example 4a: Parallel time series analysis\n",
    "print(\"4a. Parallel time series analysis:\")\n",
    "\n",
    "# Create analysis tasks with different parameters\n",
    "analysis_tasks = [\n",
    "    (1, 1000, 0.1),   # Low noise\n",
    "    (2, 1000, 0.3),   # Medium noise\n",
    "    (3, 1000, 0.5),   # High noise\n",
    "    (4, 2000, 0.2),   # More samples\n",
    "    (5, 500, 0.2),    # Fewer samples\n",
    "    (6, 1000, 0.0),   # No noise\n",
    "]\n",
    "\n",
    "print(f\"Analysis tasks: {len(analysis_tasks)}\")\n",
    "for task in analysis_tasks:\n",
    "    data_id, n_samples, noise_level = task\n",
    "    print(f\"  Dataset {data_id}: {n_samples} samples, noise level {noise_level}\")\n",
    "\n",
    "start_time = time.time()\n",
    "analysis_results = spar.run(analyze_dataset, analysis_tasks, desc=\"Analyzing datasets\")\n",
    "analysis_time = time.time() - start_time\n",
    "\n",
    "if isinstance(analysis_results, tuple):\n",
    "    data_ids, means, stds, mins, maxs, dom_freqs, sample_counts = analysis_results\n",
    "    \n",
    "    print(f\"\\nAnalysis Results:\")\n",
    "    print(f\"Dataset | Samples | Mean    | Std     | Range          | Dom.Freq\")\n",
    "    print(\"-\" * 65)\n",
    "    \n",
    "    for i in range(len(data_ids)):\n",
    "        range_val = maxs[i] - mins[i]\n",
    "        print(f\"   {data_ids[i]:2d}   |  {sample_counts[i]:4d}   | {means[i]:6.3f}  | {stds[i]:6.3f}  | {range_val:6.3f}        |   {dom_freqs[i]:3d}\")\n",
    "    \n",
    "    # Visualize some results\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "    \n",
    "    # Plot means vs std\n",
    "    axes[0, 0].scatter(means, stds)\n",
    "    axes[0, 0].set_xlabel('Mean')\n",
    "    axes[0, 0].set_ylabel('Standard Deviation')\n",
    "    axes[0, 0].set_title('Mean vs Std Deviation')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot sample counts vs std\n",
    "    axes[0, 1].scatter(sample_counts, stds)\n",
    "    axes[0, 1].set_xlabel('Sample Count')\n",
    "    axes[0, 1].set_ylabel('Standard Deviation')\n",
    "    axes[0, 1].set_title('Sample Count vs Std Deviation')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot data ranges\n",
    "    ranges = [maxs[i] - mins[i] for i in range(len(data_ids))]\n",
    "    axes[1, 0].bar(data_ids, ranges)\n",
    "    axes[1, 0].set_xlabel('Dataset ID')\n",
    "    axes[1, 0].set_ylabel('Data Range')\n",
    "    axes[1, 0].set_title('Data Range by Dataset')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot dominant frequencies\n",
    "    axes[1, 1].bar(data_ids, dom_freqs)\n",
    "    axes[1, 1].set_xlabel('Dataset ID')\n",
    "    axes[1, 1].set_ylabel('Dominant Frequency Index')\n",
    "    axes[1, 1].set_title('Dominant Frequency by Dataset')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(f\"Time series analysis time: {analysis_time:.4f} seconds\")\n",
    "\n",
    "# Example 4b: Parallel image processing\n",
    "print(\"\\n4b. Parallel image processing:\")\n",
    "\n",
    "image_tasks = [\n",
    "    (1, 64, 'none'),      # Small image, no filter\n",
    "    (2, 64, 'gaussian'),  # Small image, Gaussian filter\n",
    "    (3, 64, 'edge'),      # Small image, edge detection\n",
    "    (4, 128, 'none'),     # Medium image, no filter\n",
    "    (5, 128, 'gaussian'), # Medium image, Gaussian filter\n",
    "    (6, 128, 'edge'),     # Medium image, edge detection\n",
    "]\n",
    "\n",
    "print(f\"Image processing tasks: {len(image_tasks)}\")\n",
    "for task in image_tasks:\n",
    "    batch_id, size, filter_type = task\n",
    "    print(f\"  Batch {batch_id}: {size}x{size} image, {filter_type} filter\")\n",
    "\n",
    "start_time = time.time()\n",
    "image_results = spar.run(process_image_batch, image_tasks, desc=\"Processing images\")\n",
    "image_time = time.time() - start_time\n",
    "\n",
    "if isinstance(image_results, tuple):\n",
    "    batch_ids, filter_types, mean_intensities, std_intensities, energies, sizes = image_results\n",
    "    \n",
    "    print(f\"\\nImage Processing Results:\")\n",
    "    print(f\"Batch | Size  | Filter    | Mean    | Std     | Energy\")\n",
    "    print(\"-\" * 55)\n",
    "    \n",
    "    for i in range(len(batch_ids)):\n",
    "        print(f\"  {batch_ids[i]:2d}  | {sizes[i]:3d}x{sizes[i]:3d} | {filter_types[i]:9s} | {mean_intensities[i]:6.3f}  | {std_intensities[i]:6.3f}  | {energies[i]:8.1f}\")\n",
    "\n",
    "print(f\"Image processing time: {image_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Performance Analysis and Optimization\n",
    "\n",
    "### Comparing Different Worker Counts\n",
    "\n",
    "Let's analyze how performance scales with the number of workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 5: Performance analysis with different worker counts\n",
    "print(\"Performance Analysis with Different Worker Counts:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "def cpu_intensive_task(task_id, n_iterations):\n",
    "    \"\"\"CPU-intensive task for performance testing.\"\"\"\n",
    "    # Simulate CPU-intensive computation\n",
    "    result = 0\n",
    "    for i in range(n_iterations):\n",
    "        result += np.sin(i) * np.cos(i) * np.exp(-i/1000)\n",
    "    \n",
    "    return task_id, result, n_iterations\n",
    "\n",
    "# Create test tasks\n",
    "n_tasks = 20\n",
    "iterations_per_task = 50000\n",
    "test_tasks = [(i, iterations_per_task) for i in range(n_tasks)]\n",
    "\n",
    "print(f\"Performance test setup:\")\n",
    "print(f\"  Number of tasks: {n_tasks}\")\n",
    "print(f\"  Iterations per task: {iterations_per_task:,}\")\n",
    "print(f\"  Total iterations: {n_tasks * iterations_per_task:,}\")\n",
    "\n",
    "# Test different numbers of workers\n",
    "max_workers = min(multiprocessing.cpu_count(), 8)  # Limit to reasonable number\n",
    "worker_counts = [1, 2, 4] + ([max_workers] if max_workers > 4 else [])\n",
    "worker_counts = list(set(worker_counts))  # Remove duplicates\n",
    "worker_counts.sort()\n",
    "\n",
    "print(f\"\\nTesting with worker counts: {worker_counts}\")\n",
    "\n",
    "performance_results = []\n",
    "\n",
    "for n_workers in worker_counts:\n",
    "    print(f\"\\nTesting with {n_workers} workers:\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    results = spar.run(cpu_intensive_task, test_tasks, n_jobs=n_workers, desc=f\"Workers: {n_workers}\")\n",
    "    execution_time = time.time() - start_time\n",
    "    \n",
    "    performance_results.append((n_workers, execution_time))\n",
    "    \n",
    "    print(f\"  Execution time: {execution_time:.4f} seconds\")\n",
    "    if len(performance_results) > 1:\n",
    "        baseline_time = performance_results[0][1]\n",
    "        speedup = baseline_time / execution_time\n",
    "        efficiency = speedup / n_workers * 100\n",
    "        print(f\"  Speedup vs 1 worker: {speedup:.2f}x\")\n",
    "        print(f\"  Parallel efficiency: {efficiency:.1f}%\")\n",
    "\n",
    "# Analyze and visualize performance results\n",
    "print(f\"\\nPerformance Summary:\")\n",
    "print(f\"Workers | Time (s) | Speedup | Efficiency\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "worker_nums = []\n",
    "times = []\n",
    "speedups = []\n",
    "efficiencies = []\n",
    "\n",
    "baseline_time = performance_results[0][1]\n",
    "\n",
    "for n_workers, exec_time in performance_results:\n",
    "    speedup = baseline_time / exec_time\n",
    "    efficiency = (speedup / n_workers) * 100\n",
    "    \n",
    "    worker_nums.append(n_workers)\n",
    "    times.append(exec_time)\n",
    "    speedups.append(speedup)\n",
    "    efficiencies.append(efficiency)\n",
    "    \n",
    "    print(f\"   {n_workers:2d}   | {exec_time:7.4f}  |  {speedup:5.2f}  |   {efficiency:5.1f}%\")\n",
    "\n",
    "# Plot performance results\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Execution time\n",
    "axes[0].plot(worker_nums, times, 'bo-', linewidth=2, markersize=8)\n",
    "axes[0].set_xlabel('Number of Workers')\n",
    "axes[0].set_ylabel('Execution Time (seconds)')\n",
    "axes[0].set_title('Execution Time vs Workers')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_xticks(worker_nums)\n",
    "\n",
    "# Speedup\n",
    "axes[1].plot(worker_nums, speedups, 'ro-', linewidth=2, markersize=8, label='Actual')\n",
    "axes[1].plot(worker_nums, worker_nums, 'k--', alpha=0.5, label='Ideal (linear)')\n",
    "axes[1].set_xlabel('Number of Workers')\n",
    "axes[1].set_ylabel('Speedup')\n",
    "axes[1].set_title('Speedup vs Workers')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_xticks(worker_nums)\n",
    "\n",
    "# Efficiency\n",
    "axes[2].plot(worker_nums, efficiencies, 'go-', linewidth=2, markersize=8)\n",
    "axes[2].axhline(y=100, color='k', linestyle='--', alpha=0.5, label='Ideal (100%)')\n",
    "axes[2].set_xlabel('Number of Workers')\n",
    "axes[2].set_ylabel('Parallel Efficiency (%)')\n",
    "axes[2].set_title('Parallel Efficiency vs Workers')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "axes[2].set_xticks(worker_nums)\n",
    "axes[2].set_ylim(0, 110)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find optimal number of workers\n",
    "optimal_idx = np.argmin(times)\n",
    "optimal_workers = worker_nums[optimal_idx]\n",
    "optimal_time = times[optimal_idx]\n",
    "\n",
    "print(f\"\\nOptimal configuration:\")\n",
    "print(f\"  Workers: {optimal_workers}\")\n",
    "print(f\"  Time: {optimal_time:.4f} seconds\")\n",
    "print(f\"  Speedup: {speedups[optimal_idx]:.2f}x\")\n",
    "print(f\"  Efficiency: {efficiencies[optimal_idx]:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task Granularity Analysis\n",
    "\n",
    "Let's analyze how task size affects parallel performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 6: Task granularity analysis\n",
    "print(\"Task Granularity Analysis:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "def variable_task(task_id, work_amount):\n",
    "    \"\"\"Task with variable amount of work.\"\"\"\n",
    "    # Simulate different amounts of computational work\n",
    "    start_time = time.time()\n",
    "    \n",
    "    result = 0\n",
    "    for i in range(work_amount):\n",
    "        result += np.sin(i * 0.001) + np.cos(i * 0.001)\n",
    "    \n",
    "    execution_time = time.time() - start_time\n",
    "    return task_id, result, execution_time, work_amount\n",
    "\n",
    "# Test different task granularities\n",
    "granularity_tests = {\n",
    "    'Very Fine': (100, 1000),    # 100 tasks, 1k iterations each\n",
    "    'Fine': (50, 2000),          # 50 tasks, 2k iterations each\n",
    "    'Medium': (20, 5000),        # 20 tasks, 5k iterations each\n",
    "    'Coarse': (10, 10000),       # 10 tasks, 10k iterations each\n",
    "    'Very Coarse': (5, 20000),   # 5 tasks, 20k iterations each\n",
    "}\n",
    "\n",
    "print(f\"Testing different task granularities:\")\n",
    "for name, (n_tasks, work_per_task) in granularity_tests.items():\n",
    "    total_work = n_tasks * work_per_task\n",
    "    print(f\"  {name:12s}: {n_tasks:3d} tasks × {work_per_task:5d} = {total_work:6d} total work\")\n",
    "\n",
    "granularity_results = {}\n",
    "n_workers = min(4, multiprocessing.cpu_count())  # Use reasonable number of workers\n",
    "\n",
    "print(f\"\\nUsing {n_workers} workers for all tests:\")\n",
    "\n",
    "for granularity_name, (n_tasks, work_per_task) in granularity_tests.items():\n",
    "    # Create tasks\n",
    "    tasks = [(i, work_per_task) for i in range(n_tasks)]\n",
    "    \n",
    "    # Run parallel execution\n",
    "    start_time = time.time()\n",
    "    results = spar.run(variable_task, tasks, n_jobs=n_workers, desc=f\"{granularity_name} tasks\")\n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    # Analyze results\n",
    "    if isinstance(results, tuple):\n",
    "        task_ids, task_results, task_times, work_amounts = results\n",
    "        \n",
    "        avg_task_time = np.mean(task_times)\n",
    "        total_task_time = np.sum(task_times)\n",
    "        overhead = total_time - avg_task_time  # Approximation\n",
    "        \n",
    "        granularity_results[granularity_name] = {\n",
    "            'n_tasks': n_tasks,\n",
    "            'work_per_task': work_per_task,\n",
    "            'total_time': total_time,\n",
    "            'avg_task_time': avg_task_time,\n",
    "            'total_task_time': total_task_time,\n",
    "            'overhead': overhead\n",
    "        }\n",
    "        \n",
    "        print(f\"  {granularity_name:12s}: {total_time:.4f}s total, {avg_task_time:.6f}s avg/task\")\n",
    "\n",
    "# Analyze and visualize granularity results\n",
    "print(f\"\\nGranularity Analysis Results:\")\n",
    "print(f\"Granularity  | Tasks | Work/Task | Total Time | Avg Task Time | Efficiency\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "granularity_names = []\n",
    "total_times = []\n",
    "n_tasks_list = []\n",
    "avg_task_times = []\n",
    "\n",
    "for name, results in granularity_results.items():\n",
    "    # Calculate efficiency as ratio of actual computation time to total time\n",
    "    efficiency = (results['avg_task_time'] / results['total_time']) * 100\n",
    "    \n",
    "    granularity_names.append(name)\n",
    "    total_times.append(results['total_time'])\n",
    "    n_tasks_list.append(results['n_tasks'])\n",
    "    avg_task_times.append(results['avg_task_time'])\n",
    "    \n",
    "    print(f\"{name:12s} | {results['n_tasks']:5d} | {results['work_per_task']:9d} | {results['total_time']:9.4f}s | {results['avg_task_time']:11.6f}s | {efficiency:7.1f}%\")\n",
    "\n",
    "# Plot granularity analysis\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Total execution time vs number of tasks\n",
    "axes[0].plot(n_tasks_list, total_times, 'bo-', linewidth=2, markersize=8)\n",
    "axes[0].set_xlabel('Number of Tasks')\n",
    "axes[0].set_ylabel('Total Execution Time (seconds)')\n",
    "axes[0].set_title('Total Time vs Task Count')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_xscale('log')\n",
    "\n",
    "# Average task time vs number of tasks\n",
    "axes[1].plot(n_tasks_list, avg_task_times, 'ro-', linewidth=2, markersize=8)\n",
    "axes[1].set_xlabel('Number of Tasks')\n",
    "axes[1].set_ylabel('Average Task Time (seconds)')\n",
    "axes[1].set_title('Avg Task Time vs Task Count')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_xscale('log')\n",
    "axes[1].set_yscale('log')\n",
    "\n",
    "# Throughput (tasks per second)\n",
    "throughputs = [n_tasks / total_time for n_tasks, total_time in zip(n_tasks_list, total_times)]\n",
    "axes[2].plot(n_tasks_list, throughputs, 'go-', linewidth=2, markersize=8)\n",
    "axes[2].set_xlabel('Number of Tasks')\n",
    "axes[2].set_ylabel('Throughput (tasks/second)')\n",
    "axes[2].set_title('Throughput vs Task Count')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "axes[2].set_xscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find optimal granularity\n",
    "optimal_idx = np.argmin(total_times)\n",
    "optimal_granularity = granularity_names[optimal_idx]\n",
    "optimal_n_tasks = n_tasks_list[optimal_idx]\n",
    "optimal_time = total_times[optimal_idx]\n",
    "\n",
    "print(f\"\\nOptimal task granularity: {optimal_granularity}\")\n",
    "print(f\"  Number of tasks: {optimal_n_tasks}\")\n",
    "print(f\"  Total time: {optimal_time:.4f} seconds\")\n",
    "print(f\"  Throughput: {optimal_n_tasks/optimal_time:.1f} tasks/second\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Error Handling and Edge Cases\n",
    "\n",
    "### Testing Error Conditions\n",
    "\n",
    "Let's test how the parallel module handles various error conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 7: Error handling and edge cases\n",
    "print(\"Error Handling and Edge Cases:\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "def failing_function(task_id, should_fail):\n",
    "    \"\"\"Function that may fail based on parameters.\"\"\"\n",
    "    if should_fail and task_id % 3 == 0:  # Fail every 3rd task\n",
    "        raise ValueError(f\"Intentional failure for task {task_id}\")\n",
    "    \n",
    "    # Simulate some work\n",
    "    result = task_id ** 2\n",
    "    return task_id, result\n",
    "\n",
    "def empty_function():\n",
    "    \"\"\"Function with no arguments.\"\"\"\n",
    "    return \"no args\"\n",
    "\n",
    "# Test Case 1: Empty argument list\n",
    "print(\"1. Testing empty argument list:\")\n",
    "try:\n",
    "    result = spar.run(lambda x: x * 2, [], desc=\"Empty test\")\n",
    "    print(f\"  Unexpected success: {result}\")\n",
    "except ValueError as e:\n",
    "    print(f\"  ✓ Expected error: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"  ✗ Unexpected error: {type(e).__name__}: {e}\")\n",
    "\n",
    "# Test Case 2: Non-callable function\n",
    "print(\"\\n2. Testing non-callable function:\")\n",
    "try:\n",
    "    result = spar.run(\"not a function\", [(1,), (2,)], desc=\"Non-callable test\")\n",
    "    print(f\"  Unexpected success: {result}\")\n",
    "except ValueError as e:\n",
    "    print(f\"  ✓ Expected error: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"  ✗ Unexpected error: {type(e).__name__}: {e}\")\n",
    "\n",
    "# Test Case 3: Invalid number of jobs\n",
    "print(\"\\n3. Testing invalid number of jobs:\")\n",
    "test_args = [(1,), (2,), (3,)]\n",
    "\n",
    "# Test n_jobs = 0\n",
    "try:\n",
    "    result = spar.run(lambda x: x * 2, test_args, n_jobs=0, desc=\"Zero jobs test\")\n",
    "    print(f\"  n_jobs=0: Unexpected success: {result}\")\n",
    "except ValueError as e:\n",
    "    print(f\"  n_jobs=0: ✓ Expected error: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"  n_jobs=0: ✗ Unexpected error: {type(e).__name__}: {e}\")\n",
    "\n",
    "# Test very high n_jobs (should warn but work)\n",
    "try:\n",
    "    import warnings\n",
    "    with warnings.catch_warnings(record=True) as w:\n",
    "        warnings.simplefilter(\"always\")\n",
    "        result = spar.run(lambda x: x * 2, test_args, n_jobs=100, desc=\"High jobs test\")\n",
    "        if w:\n",
    "            print(f\"  n_jobs=100: ✓ Warning issued: {w[0].message}\")\n",
    "        else:\n",
    "            print(f\"  n_jobs=100: No warning (might be expected on some systems)\")\n",
    "        print(f\"  n_jobs=100: Results: {result}\")\n",
    "except Exception as e:\n",
    "    print(f\"  n_jobs=100: ✗ Error: {type(e).__name__}: {e}\")\n",
    "\n",
    "# Test Case 4: Function with inconsistent return types\n",
    "print(\"\\n4. Testing function with inconsistent return types:\")\n",
    "def inconsistent_function(x):\n",
    "    if x % 2 == 0:\n",
    "        return x  # Single value\n",
    "    else:\n",
    "        return (x, x*2)  # Tuple\n",
    "\n",
    "inconsistent_args = [(i,) for i in range(1, 6)]\n",
    "try:\n",
    "    result = spar.run(inconsistent_function, inconsistent_args, desc=\"Inconsistent test\")\n",
    "    print(f\"  ✓ Results: {result}\")\n",
    "    print(f\"  Note: First result type determines tuple unpacking behavior\")\n",
    "except Exception as e:\n",
    "    print(f\"  ✗ Error: {type(e).__name__}: {e}\")\n",
    "\n",
    "# Test Case 5: Very small tasks (overhead analysis)\n",
    "print(\"\\n5. Testing very small tasks (overhead analysis):\")\n",
    "def tiny_task(x):\n",
    "    return x + 1\n",
    "\n",
    "tiny_args = [(i,) for i in range(1000)]  # 1000 tiny tasks\n",
    "\n",
    "# Sequential execution\n",
    "start_time = time.time()\n",
    "sequential_result = [tiny_task(i) for i in range(1000)]\n",
    "sequential_time = time.time() - start_time\n",
    "\n",
    "# Parallel execution\n",
    "start_time = time.time()\n",
    "parallel_result = spar.run(tiny_task, tiny_args, desc=\"Tiny tasks\")\n",
    "parallel_time = time.time() - start_time\n",
    "\n",
    "print(f\"  Sequential time: {sequential_time:.6f} seconds\")\n",
    "print(f\"  Parallel time:   {parallel_time:.6f} seconds\")\n",
    "print(f\"  Overhead factor: {parallel_time / sequential_time:.2f}x\")\n",
    "print(f\"  Results match: {sequential_result == parallel_result}\")\n",
    "\n",
    "if parallel_time > sequential_time:\n",
    "    print(f\"  ⚠ Parallel execution slower due to overhead (expected for tiny tasks)\")\n",
    "else:\n",
    "    print(f\"  ✓ Parallel execution faster despite overhead\")\n",
    "\n",
    "# Test Case 6: Memory-intensive tasks\n",
    "print(\"\\n6. Testing memory-intensive tasks:\")\n",
    "def memory_task(size_mb):\n",
    "    \"\"\"Create and process a large array.\"\"\"\n",
    "    # Create array of specified size in MB\n",
    "    n_elements = int(size_mb * 1024 * 1024 / 8)  # 8 bytes per float64\n",
    "    large_array = np.random.randn(n_elements)\n",
    "    \n",
    "    # Perform some computation\n",
    "    result = np.sum(large_array**2)\n",
    "    \n",
    "    # Clean up\n",
    "    del large_array\n",
    "    \n",
    "    return size_mb, result\n",
    "\n",
    "# Use smaller arrays to avoid memory issues\n",
    "memory_args = [(1,), (2,), (3,), (4,)]  # 1-4 MB arrays\n",
    "\n",
    "try:\n",
    "    start_time = time.time()\n",
    "    memory_results = spar.run(memory_task, memory_args, desc=\"Memory-intensive tasks\")\n",
    "    memory_time = time.time() - start_time\n",
    "    \n",
    "    if isinstance(memory_results, tuple):\n",
    "        sizes, results = memory_results\n",
    "        print(f\"  ✓ Memory tasks completed successfully\")\n",
    "        print(f\"  Array sizes: {sizes} MB\")\n",
    "        print(f\"  Results: {[f'{r:.2e}' for r in results]}\")\n",
    "        print(f\"  Execution time: {memory_time:.4f} seconds\")\n",
    "    else:\n",
    "        print(f\"  ✓ Results: {memory_results}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"  ✗ Error with memory-intensive tasks: {type(e).__name__}: {e}\")\n",
    "\n",
    "print(f\"\\nError handling and edge case testing complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook has demonstrated the comprehensive functionality of the `scitex.parallel` module:\n",
    "\n",
    "### Core Functionality\n",
    "- **`run`**: Execute functions in parallel using ThreadPoolExecutor\n",
    "  - Support for functions with multiple arguments via tuple unpacking\n",
    "  - Automatic CPU core detection and utilization\n",
    "  - Progress tracking with tqdm integration\n",
    "  - Intelligent handling of multiple return values\n",
    "\n",
    "### Key Features\n",
    "1. **Ease of Use**: Simple interface requiring only function and argument list\n",
    "2. **Flexibility**: Support for various function signatures and return types\n",
    "3. **Robustness**: Comprehensive error handling and validation\n",
    "4. **Performance**: Optimized for scientific computing workloads\n",
    "5. **Monitoring**: Built-in progress tracking for long-running tasks\n",
    "\n",
    "### Demonstrated Applications\n",
    "\n",
    "#### Basic Mathematical Operations\n",
    "- Simple arithmetic functions\n",
    "- Functions with multiple arguments\n",
    "- Functions returning multiple values (tuples)\n",
    "\n",
    "#### Scientific Computing\n",
    "- **Monte Carlo Methods**: Parallel π estimation with different sample sizes\n",
    "- **Numerical Integration**: Parallel integration of various mathematical functions\n",
    "- **Data Analysis**: Parallel processing of multiple datasets\n",
    "- **Image Processing**: Batch processing with different filters\n",
    "\n",
    "#### Performance Analysis\n",
    "- **Scalability Testing**: Performance vs number of workers\n",
    "- **Granularity Analysis**: Optimal task size determination\n",
    "- **Overhead Measurement**: Understanding parallel processing costs\n",
    "\n",
    "### Performance Insights\n",
    "1. **Worker Scaling**: Performance typically improves with more workers up to CPU count\n",
    "2. **Task Granularity**: Medium-sized tasks often provide optimal performance\n",
    "3. **Overhead Considerations**: Very small tasks may run slower in parallel\n",
    "4. **Memory Constraints**: Large memory tasks may limit effective parallelism\n",
    "\n",
    "### Best Practices Illustrated\n",
    "- **Task Design**: Create tasks with sufficient computational work\n",
    "- **Worker Selection**: Use automatic CPU detection or tune based on workload\n",
    "- **Error Handling**: Implement robust error handling in task functions\n",
    "- **Memory Management**: Consider memory usage in parallel contexts\n",
    "- **Progress Monitoring**: Use descriptive progress messages for user feedback\n",
    "\n",
    "### Common Use Cases\n",
    "- **Parameter Sweeps**: Running experiments with different parameter combinations\n",
    "- **Data Processing**: Parallel analysis of multiple datasets or files\n",
    "- **Simulation Studies**: Monte Carlo simulations and statistical sampling\n",
    "- **Image/Signal Processing**: Batch processing of multimedia data\n",
    "- **Model Training**: Parallel training of multiple model configurations\n",
    "- **Scientific Computing**: Numerical integration, optimization, and analysis\n",
    "\n",
    "### Integration Benefits\n",
    "- **Scientific Workflows**: Seamless integration with NumPy, SciPy, and pandas\n",
    "- **Research Reproducibility**: Consistent parallel execution across platforms\n",
    "- **Development Efficiency**: Simple API reduces parallel programming complexity\n",
    "- **Performance Optimization**: Built-in tools for performance analysis and tuning\n",
    "\n",
    "The `scitex.parallel` module provides essential parallel processing capabilities for scientific computing, with emphasis on simplicity, robustness, and performance optimization for research applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
