{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SciTeX Context Management\n",
    "\n",
    "This comprehensive notebook demonstrates the SciTeX context module capabilities, covering context management, output suppression, and environment control utilities.\n",
    "\n",
    "## Features Covered\n",
    "\n",
    "### Output Control\n",
    "* Output suppression utilities\n",
    "* Quiet operation modes\n",
    "* Context managers for clean execution\n",
    "\n",
    "### Environment Management\n",
    "* Temporary state changes\n",
    "* Clean execution contexts\n",
    "* Resource management\n",
    "\n",
    "### Integration Examples\n",
    "* Scientific computation workflows\n",
    "* Data processing pipelines\n",
    "* Automated analysis systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "import scitex\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Set up example data directory\n",
    "data_dir = Path(\"./context_examples\")\n",
    "data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"SciTeX Context Management Tutorial - Ready to begin!\")\n",
    "print(f\"Available context functions: {len(scitex.context.__all__)}\")\n",
    "print(f\"Functions: {scitex.context.__all__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Basic Output Suppression\n",
    "\n",
    "### 1.1 Suppress Output Context Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate basic output suppression\n",
    "print(\"Basic Output Suppression:\")\n",
    "print(\"=\" * 28)\n",
    "\n",
    "# Normal output (visible)\n",
    "print(\"\\n1. Normal output (visible):\")\n",
    "print(\"This message will be visible\")\n",
    "print(\"So will this one\")\n",
    "print(\"And this one too\")\n",
    "\n",
    "# Suppressed output (hidden)\n",
    "print(\"\\n2. Suppressed output (hidden):\")\n",
    "print(\"About to suppress output...\")\n",
    "\n",
    "with scitex.context.suppress_output():\n",
    "    print(\"This message will be hidden\")\n",
    "    print(\"This one too\")\n",
    "    print(\"And this one as well\")\n",
    "    \n",
    "    # Even function calls that produce output\n",
    "    for i in range(3):\n",
    "        print(f\"Hidden message {i+1}\")\n",
    "\n",
    "print(\"Output suppression ended - this message is visible again\")\n",
    "\n",
    "# Test with different types of output\n",
    "print(\"\\n3. Testing different output types:\")\n",
    "\n",
    "def noisy_function():\n",
    "    \"\"\"A function that produces lots of output.\"\"\"\n",
    "    print(\"Starting noisy function...\")\n",
    "    for i in range(5):\n",
    "        print(f\"Processing item {i+1}/5\")\n",
    "        # Simulate some work\n",
    "        time.sleep(0.01)\n",
    "    print(\"Noisy function completed!\")\n",
    "    return \"Function result\"\n",
    "\n",
    "# Run function normally (noisy)\n",
    "print(\"\\nRunning function normally (noisy):\")\n",
    "result1 = noisy_function()\n",
    "print(f\"Result: {result1}\")\n",
    "\n",
    "# Run function with suppressed output (quiet)\n",
    "print(\"\\nRunning function with suppressed output (quiet):\")\n",
    "with scitex.context.suppress_output():\n",
    "    result2 = noisy_function()\n",
    "print(f\"Result: {result2}\")\n",
    "\n",
    "print(\"\\nBoth results are identical:\", result1 == result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Quiet Operation Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate quiet operation mode\n",
    "print(\"Quiet Operation Mode:\")\n",
    "print(\"=\" * 22)\n",
    "\n",
    "# Define functions with verbose output\n",
    "def verbose_data_processing():\n",
    "    \"\"\"Simulate verbose data processing.\"\"\"\n",
    "    print(\"Loading data...\")\n",
    "    data = np.random.randn(1000, 50)\n",
    "    print(f\"Loaded data shape: {data.shape}\")\n",
    "    \n",
    "    print(\"Normalizing data...\")\n",
    "    normalized_data = (data - np.mean(data)) / np.std(data)\n",
    "    print(f\"Data normalized, mean: {np.mean(normalized_data):.6f}, std: {np.std(normalized_data):.6f}\")\n",
    "    \n",
    "    print(\"Computing correlations...\")\n",
    "    correlations = np.corrcoef(normalized_data.T)\n",
    "    print(f\"Correlation matrix shape: {correlations.shape}\")\n",
    "    \n",
    "    print(\"Finding principal components...\")\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(correlations)\n",
    "    print(f\"Found {len(eigenvalues)} eigenvalues\")\n",
    "    print(f\"Top 3 eigenvalues: {sorted(eigenvalues, reverse=True)[:3]}\")\n",
    "    \n",
    "    print(\"Data processing completed!\")\n",
    "    return {\n",
    "        'data': normalized_data,\n",
    "        'correlations': correlations,\n",
    "        'eigenvalues': eigenvalues,\n",
    "        'eigenvectors': eigenvectors\n",
    "    }\n",
    "\n",
    "def verbose_model_training():\n",
    "    \"\"\"Simulate verbose model training.\"\"\"\n",
    "    print(\"Initializing model...\")\n",
    "    print(\"Model architecture: 3-layer neural network\")\n",
    "    print(\"Input size: 50, Hidden: 32, Output: 10\")\n",
    "    \n",
    "    print(\"Starting training...\")\n",
    "    for epoch in range(10):\n",
    "        loss = 1.0 / (epoch + 1) + 0.1 * np.random.random()\n",
    "        accuracy = 1.0 - loss + 0.05 * np.random.random()\n",
    "        print(f\"Epoch {epoch+1}/10 - Loss: {loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "        time.sleep(0.01)  # Simulate training time\n",
    "    \n",
    "    print(\"Training completed!\")\n",
    "    print(\"Model saved to: model_weights.pkl\")\n",
    "    return {'final_loss': loss, 'final_accuracy': accuracy}\n",
    "\n",
    "# Test verbose operations\n",
    "print(\"\\n1. Verbose data processing:\")\n",
    "data_results = verbose_data_processing()\n",
    "print(f\"Data processing returned {len(data_results)} items\")\n",
    "\n",
    "print(\"\\n2. Verbose model training:\")\n",
    "training_results = verbose_model_training()\n",
    "print(f\"Training results: {training_results}\")\n",
    "\n",
    "# Test quiet operations using scitex.context.quiet\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"QUIET OPERATIONS (OUTPUT SUPPRESSED)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\\n3. Quiet data processing:\")\n",
    "with scitex.context.quiet():\n",
    "    quiet_data_results = verbose_data_processing()\n",
    "print(f\"Quiet data processing completed, returned {len(quiet_data_results)} items\")\n",
    "\n",
    "print(\"\\n4. Quiet model training:\")\n",
    "with scitex.context.quiet():\n",
    "    quiet_training_results = verbose_model_training()\n",
    "print(f\"Quiet training completed: {quiet_training_results}\")\n",
    "\n",
    "# Verify results are identical\n",
    "print(\"\\n5. Results comparison:\")\n",
    "print(f\"Data results keys match: {set(data_results.keys()) == set(quiet_data_results.keys())}\")\n",
    "print(f\"Training final accuracy: {training_results['final_accuracy']:.4f} vs {quiet_training_results['final_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Advanced Context Management\n",
    "\n",
    "### 2.1 Nested Context Managers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate nested context managers\n",
    "print(\"Nested Context Managers:\")\n",
    "print(\"=\" * 27)\n",
    "\n",
    "def multi_level_function():\n",
    "    \"\"\"Function with multiple levels of verbosity.\"\"\"\n",
    "    print(\"[LEVEL 1] Starting multi-level function\")\n",
    "    \n",
    "    def level_2_function():\n",
    "        print(\"[LEVEL 2] Inside level 2 function\")\n",
    "        for i in range(3):\n",
    "            print(f\"[LEVEL 2] Processing {i+1}/3\")\n",
    "        \n",
    "        def level_3_function():\n",
    "            print(\"[LEVEL 3] Deep processing\")\n",
    "            for j in range(5):\n",
    "                print(f\"[LEVEL 3] Deep step {j+1}/5\")\n",
    "            print(\"[LEVEL 3] Deep processing complete\")\n",
    "            return \"deep_result\"\n",
    "        \n",
    "        result = level_3_function()\n",
    "        print(f\"[LEVEL 2] Level 3 returned: {result}\")\n",
    "        return result\n",
    "    \n",
    "    result = level_2_function()\n",
    "    print(f\"[LEVEL 1] Level 2 returned: {result}\")\n",
    "    print(\"[LEVEL 1] Multi-level function complete\")\n",
    "    return result\n",
    "\n",
    "# Test normal execution\n",
    "print(\"\\n1. Normal execution (all output visible):\")\n",
    "result1 = multi_level_function()\n",
    "print(f\"Final result: {result1}\")\n",
    "\n",
    "# Test single-level suppression\n",
    "print(\"\\n2. Single-level suppression:\")\n",
    "with scitex.context.suppress_output():\n",
    "    result2 = multi_level_function()\n",
    "print(f\"Final result: {result2}\")\n",
    "\n",
    "# Test nested suppression contexts\n",
    "print(\"\\n3. Nested suppression contexts:\")\n",
    "\n",
    "def selective_suppression():\n",
    "    print(\"[OUTER] Starting selective suppression\")\n",
    "    \n",
    "    print(\"[OUTER] About to enter quiet zone...\")\n",
    "    with scitex.context.quiet():\n",
    "        print(\"[QUIET] This should be suppressed\")\n",
    "        \n",
    "        # Even more nested\n",
    "        with scitex.context.suppress_output():\n",
    "            print(\"[DOUBLE QUIET] This is doubly suppressed\")\n",
    "            for i in range(3):\n",
    "                print(f\"[DOUBLE QUIET] Iteration {i}\")\n",
    "        \n",
    "        print(\"[QUIET] Back to single suppression\")\n",
    "    \n",
    "    print(\"[OUTER] Exited quiet zone\")\n",
    "    return \"selective_result\"\n",
    "\n",
    "result3 = selective_suppression()\n",
    "print(f\"Selective suppression result: {result3}\")\n",
    "\n",
    "# Test context manager exception handling\n",
    "print(\"\\n4. Exception handling in context managers:\")\n",
    "\n",
    "def function_with_error():\n",
    "    print(\"Starting function that will raise an error\")\n",
    "    print(\"Doing some work...\")\n",
    "    raise ValueError(\"Intentional error for testing\")\n",
    "\n",
    "# Test that context manager properly handles exceptions\n",
    "try:\n",
    "    print(\"Testing exception handling with context manager:\")\n",
    "    with scitex.context.suppress_output():\n",
    "        function_with_error()\nexcept ValueError as e:\n",
    "    print(f\"Caught expected error: {e}\")\n",
    "    print(\"Context manager properly restored output after exception\")\n",
    "\n",
    "print(\"This message confirms output is working normally after exception\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Warning and Error Suppression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate warning and error suppression\n",
    "print(\"Warning and Error Suppression:\")\n",
    "print(\"=\" * 32)\n",
    "\n",
    "# Function that generates warnings\n",
    "def function_with_warnings():\n",
    "    \"\"\"Function that generates various warnings.\"\"\"\n",
    "    print(\"Function starting...\")\n",
    "    \n",
    "    # Generate numpy warnings\n",
    "    print(\"Creating arrays with potential warnings...\")\n",
    "    \n",
    "    # Division by zero warning\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"always\")\n",
    "        result1 = np.array([1, 2, 3, 0]) / np.array([2, 0, 1, 0])  # Will generate warnings\n",
    "    \n",
    "    # Invalid value warning\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"always\")\n",
    "        result2 = np.sqrt(np.array([-1, 4, -9, 16]))  # Will generate warnings\n",
    "    \n",
    "    # Overflow warning\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"always\")\n",
    "        result3 = np.exp(np.array([700, 800, 900]))  # Will generate warnings\n",
    "    \n",
    "    print(\"Arrays created with potential warnings\")\n",
    "    print(f\"Results: {len(result1)}, {len(result2)}, {len(result3)} arrays\")\n",
    "    \n",
    "    return result1, result2, result3\n",
    "\n",
    "# Test with warnings visible\n",
    "print(\"\\n1. Function with warnings visible:\")\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"always\")  # Show all warnings\n",
    "    results1 = function_with_warnings()\n",
    "\n",
    "# Test with both output and warnings suppressed\n",
    "print(\"\\n2. Function with output and warnings suppressed:\")\n",
    "with scitex.context.suppress_output():\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")  # Suppress warnings\n",
    "        results2 = function_with_warnings()\n",
    "\n",
    "print(\"Suppressed execution completed\")\n",
    "\n",
    "# Verify results are still computed correctly\n",
    "print(f\"Results computed correctly: {len(results1) == len(results2)}\")\n",
    "\n",
    "# Test stderr suppression\n",
    "print(\"\\n3. Testing stderr suppression:\")\n",
    "\n",
    "def function_with_stderr():\n",
    "    \"\"\"Function that writes to stderr.\"\"\"\n",
    "    print(\"Writing to stdout\")\n",
    "    sys.stderr.write(\"Writing to stderr\\n\")\n",
    "    sys.stderr.write(\"Another stderr message\\n\")\n",
    "    print(\"Back to stdout\")\n",
    "    return \"stderr_test_result\"\n",
    "\n",
    "# Normal execution (stderr visible)\n",
    "print(\"Normal execution (stderr may be visible):\")\n",
    "result_normal = function_with_stderr()\n",
    "\n",
    "# Suppressed execution\n",
    "print(\"\\nSuppressed execution:\")\n",
    "with scitex.context.suppress_output():\n",
    "    result_suppressed = function_with_stderr()\n",
    "\n",
    "print(f\"Results match: {result_normal == result_suppressed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Scientific Computing Applications\n",
    "\n",
    "### 3.1 Clean Data Processing Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean data processing pipelines\n",
    "print(\"Clean Data Processing Pipelines:\")\n",
    "print(\"=\" * 36)\n",
    "\n",
    "class DataProcessor:\n",
    "    \"\"\"A data processor with verbose and quiet modes.\"\"\"\n",
    "    \n",
    "    def __init__(self, verbose=True):\n",
    "        self.verbose = verbose\n",
    "        self.processing_log = []\n",
    "    \n",
    "    def log(self, message):\n",
    "        \"\"\"Log a message if verbose mode is enabled.\"\"\"\n",
    "        self.processing_log.append(message)\n",
    "        if self.verbose:\n",
    "            print(f\"[DataProcessor] {message}\")\n",
    "    \n",
    "    def load_data(self, shape=(1000, 50)):\n",
    "        \"\"\"Load synthetic data.\"\"\"\n",
    "        self.log(f\"Loading data with shape {shape}\")\n",
    "        data = np.random.randn(*shape)\n",
    "        \n",
    "        # Add some structure\n",
    "        data[:, :10] += np.sin(np.linspace(0, 2*np.pi, shape[0]))[:, np.newaxis]\n",
    "        data[:, 10:20] += np.cos(np.linspace(0, 4*np.pi, shape[0]))[:, np.newaxis]\n",
    "        \n",
    "        self.log(f\"Data loaded successfully\")\n",
    "        self.log(f\"Data statistics: mean={np.mean(data):.4f}, std={np.std(data):.4f}\")\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def preprocess_data(self, data):\n",
    "        \"\"\"Preprocess the data.\"\"\"\n",
    "        self.log(\"Starting data preprocessing\")\n",
    "        \n",
    "        # Step 1: Remove outliers\n",
    "        self.log(\"Removing outliers (>3 std)\")\n",
    "        outlier_mask = np.abs(data) > 3 * np.std(data)\n",
    "        data_cleaned = data.copy()\n",
    "        data_cleaned[outlier_mask] = np.nan\n",
    "        outliers_removed = np.sum(outlier_mask)\n",
    "        self.log(f\"Removed {outliers_removed} outliers\")\n",
    "        \n",
    "        # Step 2: Interpolate missing values\n",
    "        self.log(\"Interpolating missing values\")\n",
    "        for col in range(data_cleaned.shape[1]):\n",
    "            mask = ~np.isnan(data_cleaned[:, col])\n",
    "            if np.sum(mask) > 0:\n",
    "                data_cleaned[~mask, col] = np.mean(data_cleaned[mask, col])\n",
    "        \n",
    "        # Step 3: Normalize\n",
    "        self.log(\"Normalizing data (z-score)\")\n",
    "        data_normalized = (data_cleaned - np.mean(data_cleaned, axis=0)) / np.std(data_cleaned, axis=0)\n",
    "        \n",
    "        self.log(\"Preprocessing completed\")\n",
    "        self.log(f\"Final data: mean={np.mean(data_normalized):.6f}, std={np.std(data_normalized):.6f}\")\n",
    "        \n",
    "        return data_normalized\n",
    "    \n",
    "    def analyze_data(self, data):\n",
    "        \"\"\"Analyze the preprocessed data.\"\"\"\n",
    "        self.log(\"Starting data analysis\")\n",
    "        \n",
    "        # Correlation analysis\n",
    "        self.log(\"Computing correlation matrix\")\n",
    "        correlation_matrix = np.corrcoef(data.T)\n",
    "        \n",
    "        # Principal component analysis\n",
    "        self.log(\"Performing PCA\")\n",
    "        eigenvalues, eigenvectors = np.linalg.eig(correlation_matrix)\n",
    "        \n",
    "        # Sort by eigenvalue magnitude\n",
    "        idx = np.argsort(eigenvalues)[::-1]\n",
    "        eigenvalues = eigenvalues[idx]\n",
    "        eigenvectors = eigenvectors[:, idx]\n",
    "        \n",
    "        # Compute explained variance\n",
    "        explained_variance = eigenvalues / np.sum(eigenvalues)\n",
    "        cumulative_variance = np.cumsum(explained_variance)\n",
    "        \n",
    "        self.log(f\"First 5 eigenvalues: {eigenvalues[:5]}\")\n",
    "        self.log(f\"Variance explained by first 5 PCs: {explained_variance[:5]}\")\n",
    "        self.log(f\"Cumulative variance (first 10 PCs): {cumulative_variance[9]:.4f}\")\n",
    "        \n",
    "        # Cluster analysis\n",
    "        self.log(\"Performing simple clustering\")\n",
    "        # Simple k-means-like clustering\n",
    "        n_clusters = 3\n",
    "        centroids = data[np.random.choice(data.shape[0], n_clusters, replace=False)]\n",
    "        \n",
    "        distances = np.sqrt(((data[:, np.newaxis, :] - centroids[np.newaxis, :, :]) ** 2).sum(axis=2))\n",
    "        labels = np.argmin(distances, axis=1)\n",
    "        \n",
    "        cluster_sizes = [np.sum(labels == i) for i in range(n_clusters)]\n",
    "        self.log(f\"Cluster sizes: {cluster_sizes}\")\n",
    "        \n",
    "        self.log(\"Analysis completed\")\n",
    "        \n",
    "        return {\n",
    "            'correlation_matrix': correlation_matrix,\n",
    "            'eigenvalues': eigenvalues,\n",
    "            'eigenvectors': eigenvectors,\n",
    "            'explained_variance': explained_variance,\n",
    "            'cumulative_variance': cumulative_variance,\n",
    "            'cluster_labels': labels,\n",
    "            'cluster_sizes': cluster_sizes\n",
    "        }\n",
    "    \n",
    "    def run_pipeline(self, data_shape=(1000, 50)):\n",
    "        \"\"\"Run the complete data processing pipeline.\"\"\"\n",
    "        self.log(\"=\" * 50)\n",
    "        self.log(\"STARTING DATA PROCESSING PIPELINE\")\n",
    "        self.log(\"=\" * 50)\n",
    "        \n",
    "        # Load data\n",
    "        data = self.load_data(data_shape)\n",
    "        \n",
    "        # Preprocess\n",
    "        processed_data = self.preprocess_data(data)\n",
    "        \n",
    "        # Analyze\n",
    "        analysis_results = self.analyze_data(processed_data)\n",
    "        \n",
    "        self.log(\"=\" * 50)\n",
    "        self.log(\"PIPELINE COMPLETED SUCCESSFULLY\")\n",
    "        self.log(\"=\" * 50)\n",
    "        \n",
    "        return {\n",
    "            'raw_data': data,\n",
    "            'processed_data': processed_data,\n",
    "            'analysis': analysis_results,\n",
    "            'log': self.processing_log\n",
    "        }\n",
    "\n",
    "# Test verbose pipeline\n",
    "print(\"\\n1. Verbose data processing pipeline:\")\n",
    "verbose_processor = DataProcessor(verbose=True)\n",
    "verbose_results = verbose_processor.run_pipeline((500, 20))\n",
    "print(f\"\\nVerbose pipeline completed. Log entries: {len(verbose_results['log'])}\")\n",
    "\n",
    "# Test quiet pipeline using context manager\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"2. Quiet data processing pipeline:\")\n",
    "\n",
    "with scitex.context.quiet():\n",
    "    quiet_processor = DataProcessor(verbose=True)  # Still verbose, but output suppressed\n",
    "    quiet_results = quiet_processor.run_pipeline((500, 20))\n",
    "\n",
    "print(f\"Quiet pipeline completed. Log entries: {len(quiet_results['log'])}\")\n",
    "\n",
    "# Compare results\n",
    "print(\"\\n3. Results comparison:\")\n",
    "print(f\"Verbose analysis keys: {list(verbose_results['analysis'].keys())}\")\n",
    "print(f\"Quiet analysis keys: {list(quiet_results['analysis'].keys())}\")\n",
    "print(f\"Results structure identical: {set(verbose_results.keys()) == set(quiet_results.keys())}\")\n",
    "\n",
    "# Show log comparison\n",
    "print(f\"\\nLog comparison:\")\n",
    "print(f\"Verbose log entries: {len(verbose_results['log'])}\")\n",
    "print(f\"Quiet log entries: {len(quiet_results['log'])}\")\n",
    "print(f\"First verbose log entry: {verbose_results['log'][0]}\")\n",
    "print(f\"First quiet log entry: {quiet_results['log'][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Automated Analysis with Clean Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automated analysis with clean output\n",
    "print(\"Automated Analysis with Clean Output:\")\n",
    "print(\"=\" * 38)\n",
    "\n",
    "class AutomatedAnalyzer:\n",
    "    \"\"\"Automated analyzer that can run in quiet or verbose mode.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.analysis_history = []\n",
    "    \n",
    "    def analyze_dataset(self, dataset_name, data, quiet=False):\n",
    "        \"\"\"Analyze a dataset with optional quiet mode.\"\"\"\n",
    "        \n",
    "        def verbose_analysis():\n",
    "            print(f\"\\n{'='*50}\")\n",
    "            print(f\"ANALYZING DATASET: {dataset_name}\")\n",
    "            print(f\"{'='*50}\")\n",
    "            \n",
    "            print(f\"Dataset shape: {data.shape}\")\n",
    "            print(f\"Data type: {data.dtype}\")\n",
    "            \n",
    "            # Basic statistics\n",
    "            print(\"\\nBasic Statistics:\")\n",
    "            print(f\"  Mean: {np.mean(data):.6f}\")\n",
    "            print(f\"  Std:  {np.std(data):.6f}\")\n",
    "            print(f\"  Min:  {np.min(data):.6f}\")\n",
    "            print(f\"  Max:  {np.max(data):.6f}\")\n",
    "            \n",
    "            # Distribution analysis\n",
    "            print(\"\\nDistribution Analysis:\")\n",
    "            percentiles = [5, 25, 50, 75, 95]\n",
    "            perc_values = np.percentile(data, percentiles)\n",
    "            for p, v in zip(percentiles, perc_values):\n",
    "                print(f\"  {p}th percentile: {v:.6f}\")\n",
    "            \n",
    "            # Correlation analysis\n",
    "            if data.ndim > 1 and data.shape[1] > 1:\n",
    "                print(\"\\nCorrelation Analysis:\")\n",
    "                corr_matrix = np.corrcoef(data.T)\n",
    "                \n",
    "                # Find highest correlations\n",
    "                mask = np.triu(np.ones_like(corr_matrix, dtype=bool), k=1)\n",
    "                correlations = corr_matrix[mask]\n",
    "                high_corr = correlations[np.abs(correlations) > 0.5]\n",
    "                \n",
    "                print(f\"  Correlation matrix shape: {corr_matrix.shape}\")\n",
    "                print(f\"  High correlations (|r| > 0.5): {len(high_corr)}\")\n",
    "                if len(high_corr) > 0:\n",
    "                    print(f\"  Max correlation: {np.max(np.abs(high_corr)):.4f}\")\n",
    "            \n",
    "            # Outlier detection\n",
    "            print(\"\\nOutlier Detection:\")\n",
    "            z_scores = np.abs((data - np.mean(data)) / np.std(data))\n",
    "            outliers = z_scores > 3\n",
    "            n_outliers = np.sum(outliers)\n",
    "            outlier_percentage = (n_outliers / data.size) * 100\n",
    "            \n",
    "            print(f\"  Outliers (|z| > 3): {n_outliers} ({outlier_percentage:.2f}%)\")\n",
    "            \n",
    "            # Trend analysis\n",
    "            if data.ndim == 1 or (data.ndim == 2 and data.shape[1] == 1):\n",
    "                print(\"\\nTrend Analysis:\")\n",
    "                flat_data = data.flatten()\n",
    "                x = np.arange(len(flat_data))\n",
    "                slope, intercept = np.polyfit(x, flat_data, 1)\n",
    "                print(f\"  Linear trend slope: {slope:.8f}\")\n",
    "                print(f\"  Trend direction: {'Increasing' if slope > 0 else 'Decreasing' if slope < 0 else 'Flat'}\")\n",
    "            \n",
    "            print(f\"\\n{'='*50}\")\n",
    "            print(f\"ANALYSIS COMPLETED: {dataset_name}\")\n",
    "            print(f\"{'='*50}\\n\")\n",
    "            \n",
    "            # Return analysis results\n",
    "            results = {\n",
    "                'dataset_name': dataset_name,\n",
    "                'shape': data.shape,\n",
    "                'basic_stats': {\n",
    "                    'mean': np.mean(data),\n",
    "                    'std': np.std(data),\n",
    "                    'min': np.min(data),\n",
    "                    'max': np.max(data)\n",
    "                },\n",
    "                'percentiles': dict(zip(percentiles, perc_values)),\n",
    "                'outliers': {\n",
    "                    'count': n_outliers,\n",
    "                    'percentage': outlier_percentage\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            if data.ndim > 1 and data.shape[1] > 1:\n",
    "                results['correlations'] = {\n",
    "                    'matrix_shape': corr_matrix.shape,\n",
    "                    'high_correlations': len(high_corr),\n",
    "                    'max_correlation': np.max(np.abs(high_corr)) if len(high_corr) > 0 else 0\n",
    "                }\n",
    "            \n",
    "            if data.ndim == 1 or (data.ndim == 2 and data.shape[1] == 1):\n",
    "                results['trend'] = {\n",
    "                    'slope': slope,\n",
    "                    'direction': 'Increasing' if slope > 0 else 'Decreasing' if slope < 0 else 'Flat'\n",
    "                }\n",
    "            \n",
    "            return results\n",
    "        \n",
    "        # Run analysis with or without output suppression\n",
    "        if quiet:\n",
    "            with scitex.context.suppress_output():\n",
    "                results = verbose_analysis()\n",
    "        else:\n",
    "            results = verbose_analysis()\n",
    "        \n",
    "        # Store in history\n",
    "        self.analysis_history.append(results)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def batch_analysis(self, datasets, quiet=True):\n",
    "        \"\"\"Perform batch analysis on multiple datasets.\"\"\"\n",
    "        print(f\"Starting batch analysis of {len(datasets)} datasets...\")\n",
    "        \n",
    "        results = []\n",
    "        for name, data in datasets.items():\n",
    "            if not quiet:\n",
    "                print(f\"\\nProcessing dataset: {name}\")\n",
    "            \n",
    "            result = self.analyze_dataset(name, data, quiet=quiet)\n",
    "            results.append(result)\n",
    "            \n",
    "            if not quiet:\n",
    "                print(f\"Completed: {name}\")\n",
    "        \n",
    "        print(f\"\\nBatch analysis completed. Processed {len(results)} datasets.\")\n",
    "        return results\n",
    "    \n",
    "    def generate_summary(self):\n",
    "        \"\"\"Generate a summary of all analyses.\"\"\"\n",
    "        if not self.analysis_history:\n",
    "            print(\"No analyses performed yet.\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\nANALYSIS SUMMARY\")\n",
    "        print(f\"=\"*20)\n",
    "        print(f\"Total datasets analyzed: {len(self.analysis_history)}\")\n",
    "        \n",
    "        # Summary statistics\n",
    "        all_means = [r['basic_stats']['mean'] for r in self.analysis_history]\n",
    "        all_stds = [r['basic_stats']['std'] for r in self.analysis_history]\n",
    "        all_outlier_pcts = [r['outliers']['percentage'] for r in self.analysis_history]\n",
    "        \n",
    "        print(f\"\\nAcross all datasets:\")\n",
    "        print(f\"  Mean of means: {np.mean(all_means):.6f}\")\n",
    "        print(f\"  Mean of stds: {np.mean(all_stds):.6f}\")\n",
    "        print(f\"  Average outlier percentage: {np.mean(all_outlier_pcts):.2f}%\")\n",
    "        \n",
    "        # Dataset with highest/lowest variation\n",
    "        max_std_idx = np.argmax(all_stds)\n",
    "        min_std_idx = np.argmin(all_stds)\n",
    "        \n",
    "        print(f\"\\nDataset with highest variation: {self.analysis_history[max_std_idx]['dataset_name']} (std: {all_stds[max_std_idx]:.6f})\")\n",
    "        print(f\"Dataset with lowest variation: {self.analysis_history[min_std_idx]['dataset_name']} (std: {all_stds[min_std_idx]:.6f})\")\n",
    "\n",
    "# Create test datasets\n",
    "test_datasets = {\n",
    "    'random_normal': np.random.randn(1000, 10),\n",
    "    'random_uniform': np.random.uniform(-1, 1, (800, 15)),\n",
    "    'structured_sine': np.sin(np.linspace(0, 4*np.pi, 500)).reshape(-1, 1),\n",
    "    'noisy_trend': np.linspace(0, 10, 1000) + 0.5 * np.random.randn(1000),\n",
    "    'sparse_data': np.zeros((200, 20)),\n",
    "}\n",
    "\n",
    "# Add some structure to sparse data\n",
    "test_datasets['sparse_data'][::10, ::5] = np.random.randn(20, 4)\n",
    "\n",
    "# Create analyzer\n",
    "analyzer = AutomatedAnalyzer()\n",
    "\n",
    "# Test individual analysis (verbose)\n",
    "print(\"\\n1. Individual analysis (verbose):\")\n",
    "individual_result = analyzer.analyze_dataset('test_normal', np.random.randn(100, 5), quiet=False)\n",
    "\n",
    "# Test batch analysis (quiet)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"2. Batch analysis (quiet):\")\n",
    "batch_results = analyzer.batch_analysis(test_datasets, quiet=True)\n",
    "\n",
    "# Generate summary\n",
    "analyzer.generate_summary()\n",
    "\n",
    "# Test mixed mode\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"3. Mixed mode analysis:\")\n",
    "\n",
    "print(\"\\nAnalyzing with context manager:\")\n",
    "with scitex.context.quiet():\n",
    "    mixed_result = analyzer.analyze_dataset('mixed_mode', np.random.exponential(2, (300, 8)), quiet=False)\n",
    "\n",
    "print(f\"Mixed mode analysis completed for dataset: {mixed_result['dataset_name']}\")\n",
    "print(f\"Dataset shape: {mixed_result['shape']}\")\n",
    "print(f\"Mean: {mixed_result['basic_stats']['mean']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Performance and Resource Management\n",
    "\n",
    "### 4.1 Performance Comparison with Context Managers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance comparison with context managers\n",
    "print(\"Performance Comparison with Context Managers:\")\n",
    "print(\"=\" * 47)\n",
    "\n",
    "import time\n",
    "\n",
    "def performance_heavy_function(n_iterations=100):\n",
    "    \"\"\"A function that does heavy computation with lots of output.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "        print(f\"Iteration {i+1}/{n_iterations}: Starting computation\")\n",
    "        \n",
    "        # Heavy computation\n",
    "        data = np.random.randn(100, 100)\n",
    "        print(f\"  Generated {data.shape[0]}x{data.shape[1]} matrix\")\n",
    "        \n",
    "        # Matrix operations\n",
    "        eigenvals = np.linalg.eigvals(data @ data.T)\n",
    "        print(f\"  Computed {len(eigenvals)} eigenvalues\")\n",
    "        \n",
    "        result = np.sum(eigenvals)\n",
    "        results.append(result)\n",
    "        print(f\"  Eigenvalue sum: {result:.4f}\")\n",
    "        \n",
    "        if i % 10 == 9:\n",
    "            print(f\"Completed {i+1} iterations\")\n",
    "    \n",
    "    print(f\"All {n_iterations} iterations completed\")\n",
    "    return results\n",
    "\n",
    "# Test performance with output\n",
    "print(\"\\n1. Performance test with output (10 iterations):\")\n",
    "start_time = time.time()\n",
    "results_with_output = performance_heavy_function(10)\n",
    "time_with_output = time.time() - start_time\n",
    "print(f\"Time with output: {time_with_output:.4f} seconds\")\n",
    "\n",
    "# Test performance without output\n",
    "print(\"\\n2. Performance test without output (10 iterations):\")\n",
    "start_time = time.time()\n",
    "with scitex.context.suppress_output():\n",
    "    results_without_output = performance_heavy_function(10)\n",
    "time_without_output = time.time() - start_time\n",
    "print(f\"Time without output: {time_without_output:.4f} seconds\")\n",
    "\n",
    "# Compare performance\n",
    "print(f\"\\n3. Performance comparison:\")\n",
    "print(f\"With output:    {time_with_output:.4f} seconds\")\n",
    "print(f\"Without output: {time_without_output:.4f} seconds\")\n",
    "if time_with_output > time_without_output:\n",
    "    speedup = time_with_output / time_without_output\n",
    "    print(f\"Speedup from suppressing output: {speedup:.2f}x\")\n",
    "    print(f\"Time saved: {(time_with_output - time_without_output)*1000:.1f} ms\")\n",
    "else:\n",
    "    print(\"No significant performance difference detected\")\n",
    "\n",
    "# Verify results are identical\n",
    "results_match = np.allclose(results_with_output, results_without_output)\n",
    "print(f\"Results identical: {results_match}\")\n",
    "\n",
    "# Memory usage test\n",
    "print(\"\\n4. Memory usage test:\")\n",
    "\n",
    "def memory_intensive_function():\n",
    "    \"\"\"Function that creates large objects and prints about them.\"\"\"\n",
    "    arrays = []\n",
    "    \n",
    "    for i in range(20):\n",
    "        # Create progressively larger arrays\n",
    "        size = (i + 1) * 100\n",
    "        arr = np.random.randn(size, size)\n",
    "        arrays.append(arr)\n",
    "        \n",
    "        memory_usage = sum(a.nbytes for a in arrays) / (1024**2)  # MB\n",
    "        print(f\"Created array {i+1}: {arr.shape}, Memory usage: {memory_usage:.1f} MB\")\n",
    "        \n",
    "        if i % 5 == 4:\n",
    "            print(f\"Checkpoint: {i+1} arrays created, total memory: {memory_usage:.1f} MB\")\n",
    "    \n",
    "    total_memory = sum(a.nbytes for a in arrays) / (1024**2)\n",
    "    print(f\"Final memory usage: {total_memory:.1f} MB\")\n",
    "    \n",
    "    return arrays\n",
    "\n",
    "# Test memory function with output\n",
    "print(\"\\nMemory test with output:\")\n",
    "start_time = time.time()\n",
    "arrays_with_output = memory_intensive_function()\n",
    "time_memory_with = time.time() - start_time\n",
    "print(f\"Memory test with output completed in {time_memory_with:.4f} seconds\")\n",
    "\n",
    "# Clean up\n",
    "del arrays_with_output\n",
    "\n",
    "# Test memory function without output\n",
    "print(\"\\nMemory test without output:\")\n",
    "start_time = time.time()\n",
    "with scitex.context.suppress_output():\n",
    "    arrays_without_output = memory_intensive_function()\n",
    "time_memory_without = time.time() - start_time\n",
    "print(f\"Memory test without output completed in {time_memory_without:.4f} seconds\")\n",
    "\n",
    "# Compare memory test performance\n",
    "memory_speedup = time_memory_with / time_memory_without if time_memory_without > 0 else 1\n",
    "print(f\"Memory test speedup: {memory_speedup:.2f}x\")\n",
    "\n",
    "# Clean up\n",
    "del arrays_without_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Resource Management and Context Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resource management and context cleanup\n",
    "print(\"Resource Management and Context Cleanup:\")\n",
    "print(\"=\" * 42)\n",
    "\n",
    "class ResourceManager:\n",
    "    \"\"\"Demonstrate resource management with context managers.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.resources = []\n",
    "        self.resource_counter = 0\n",
    "    \n",
    "    def create_resource(self, name, size_mb=10):\n",
    "        \"\"\"Create a mock resource (large array).\"\"\"\n",
    "        self.resource_counter += 1\n",
    "        resource_id = f\"{name}_{self.resource_counter}\"\n",
    "        \n",
    "        # Create resource (large array)\n",
    "        elements = int(size_mb * 1024 * 1024 / 8)  # 8 bytes per float64\n",
    "        array_size = int(np.sqrt(elements))\n",
    "        resource_data = np.random.randn(array_size, array_size)\n",
    "        \n",
    "        resource = {\n",
    "            'id': resource_id,\n",
    "            'name': name,\n",
    "            'data': resource_data,\n",
    "            'size_mb': resource_data.nbytes / (1024**2),\n",
    "            'created_at': time.time()\n",
    "        }\n",
    "        \n",
    "        self.resources.append(resource)\n",
    "        \n",
    "        print(f\"Created resource: {resource_id} ({resource['size_mb']:.1f} MB)\")\n",
    "        return resource\n",
    "    \n",
    "    def cleanup_resources(self):\n",
    "        \"\"\"Clean up all resources.\"\"\"\n",
    "        total_memory = sum(r['size_mb'] for r in self.resources)\n",
    "        count = len(self.resources)\n",
    "        \n",
    "        print(f\"Cleaning up {count} resources ({total_memory:.1f} MB total)\")\n",
    "        \n",
    "        for resource in self.resources:\n",
    "            print(f\"  Cleaning resource: {resource['id']} ({resource['size_mb']:.1f} MB)\")\n",
    "            del resource['data']  # Free the large array\n",
    "        \n",
    "        self.resources.clear()\n",
    "        print(f\"All resources cleaned up\")\n",
    "    \n",
    "    def get_memory_usage(self):\n",
    "        \"\"\"Get current memory usage.\"\"\"\n",
    "        total_mb = sum(r['size_mb'] for r in self.resources)\n",
    "        return total_mb\n",
    "    \n",
    "    def resource_intensive_operation(self, n_resources=5):\n",
    "        \"\"\"Perform a resource-intensive operation.\"\"\"\n",
    "        print(f\"Starting resource-intensive operation with {n_resources} resources\")\n",
    "        \n",
    "        for i in range(n_resources):\n",
    "            resource = self.create_resource(f\"data_array\", size_mb=20)\n",
    "            \n",
    "            # Simulate processing\n",
    "            print(f\"Processing resource {resource['id']}...\")\n",
    "            \n",
    "            # Some computation\n",
    "            mean_value = np.mean(resource['data'])\n",
    "            std_value = np.std(resource['data'])\n",
    "            \n",
    "            print(f\"  Mean: {mean_value:.6f}, Std: {std_value:.6f}\")\n",
    "            print(f\"  Current memory usage: {self.get_memory_usage():.1f} MB\")\n",
    "            \n",
    "            if i % 2 == 1:\n",
    "                print(f\"  Checkpoint: {i+1} resources created\")\n",
    "        \n",
    "        final_memory = self.get_memory_usage()\n",
    "        print(f\"Operation completed. Final memory usage: {final_memory:.1f} MB\")\n",
    "        \n",
    "        return final_memory\n",
    "\n",
    "# Test resource management with output\n",
    "print(\"\\n1. Resource management with output:\")\n",
    "manager1 = ResourceManager()\n",
    "memory_used_1 = manager1.resource_intensive_operation(3)\n",
    "print(f\"Memory used: {memory_used_1:.1f} MB\")\n",
    "manager1.cleanup_resources()\n",
    "\n",
    "# Test resource management without output\n",
    "print(\"\\n2. Resource management without output:\")\n",
    "manager2 = ResourceManager()\n",
    "\n",
    "with scitex.context.suppress_output():\n",
    "    memory_used_2 = manager2.resource_intensive_operation(3)\n",
    "\n",
    "print(f\"Silent operation completed. Memory used: {memory_used_2:.1f} MB\")\n",
    "\n",
    "# Show current state\n",
    "print(f\"Resources still in memory: {len(manager2.resources)}\")\n",
    "print(f\"Current memory usage: {manager2.get_memory_usage():.1f} MB\")\n",
    "\n",
    "# Clean up with output\n",
    "manager2.cleanup_resources()\n",
    "\n",
    "# Test context manager exception handling with resources\n",
    "print(\"\\n3. Exception handling with resources:\")\n",
    "\n",
    "class SafeResourceManager(ResourceManager):\n",
    "    \"\"\"Resource manager with automatic cleanup on exceptions.\"\"\"\n",
    "    \n",
    "    def __enter__(self):\n",
    "        print(\"Entering SafeResourceManager context\")\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        if exc_type is not None:\n",
    "            print(f\"Exception occurred: {exc_type.__name__}: {exc_val}\")\n",
    "            print(\"Performing emergency cleanup...\")\n",
    "        else:\n",
    "            print(\"Normal exit, performing cleanup...\")\n",
    "        \n",
    "        self.cleanup_resources()\n",
    "        print(\"SafeResourceManager context exited\")\n",
    "        \n",
    "        # Don't suppress the exception\n",
    "        return False\n",
    "\n",
    "# Test normal operation\n",
    "print(\"\\nTesting normal operation with SafeResourceManager:\")\n",
    "with SafeResourceManager() as safe_manager:\n",
    "    safe_manager.create_resource(\"test_resource\", 5)\n",
    "    safe_manager.create_resource(\"another_resource\", 5)\n",
    "    print(f\"Created resources, memory usage: {safe_manager.get_memory_usage():.1f} MB\")\n",
    "\n",
    "# Test exception handling\n",
    "print(\"\\nTesting exception handling with SafeResourceManager:\")\n",
    "try:\n",
    "    with SafeResourceManager() as safe_manager:\n",
    "        safe_manager.create_resource(\"test_resource\", 5)\n",
    "        print(f\"Memory before exception: {safe_manager.get_memory_usage():.1f} MB\")\n",
    "        \n",
    "        # Cause an intentional exception\n",
    "        raise ValueError(\"Intentional error for testing\")\n",
    "        \nexcept ValueError as e:\n",
    "    print(f\"Caught exception outside context manager: {e}\")\n",
    "    print(\"Resources were properly cleaned up despite the exception\")\n",
    "\n",
    "# Test nested context managers\n",
    "print(\"\\n4. Nested context managers:\")\n",
    "\n",
    "with SafeResourceManager() as outer_manager:\n",
    "    outer_manager.create_resource(\"outer_resource\", 10)\n",
    "    \n",
    "    print(\"About to enter quiet zone...\")\n",
    "    with scitex.context.suppress_output():\n",
    "        outer_manager.create_resource(\"quiet_resource_1\", 10)\n",
    "        outer_manager.create_resource(\"quiet_resource_2\", 10)\n",
    "        \n",
    "        # This output will be suppressed\n",
    "        print(\"This message is suppressed\")\n",
    "        print(f\"Quiet zone memory: {outer_manager.get_memory_usage():.1f} MB\")\n",
    "    \n",
    "    print(\"Exited quiet zone\")\n",
    "    print(f\"Final memory before cleanup: {outer_manager.get_memory_usage():.1f} MB\")\n",
    "\n",
    "print(\"All nested context managers completed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Best Practices\n",
    "\n",
    "This tutorial demonstrated the comprehensive context management capabilities of the SciTeX context module:\n",
    "\n",
    "### Key Features Covered:\n",
    "1. **Output Suppression**: `suppress_output()` for clean execution\n",
    "2. **Quiet Operations**: `quiet()` for silent processing\n",
    "3. **Context Management**: Proper resource handling and cleanup\n",
    "4. **Exception Safety**: Robust error handling with context managers\n",
    "5. **Performance Optimization**: Reduced overhead from suppressed output\n",
    "6. **Nested Contexts**: Complex workflow management\n",
    "7. **Resource Management**: Memory and resource cleanup\n",
    "8. **Scientific Applications**: Clean data processing pipelines\n",
    "\n",
    "### Best Practices:\n",
    "- Use **output suppression** for batch processing and automated workflows\n",
    "- Apply **quiet operations** when running repetitive analyses\n",
    "- Implement **proper exception handling** in context managers\n",
    "- Use **nested contexts** for complex processing pipelines\n",
    "- Apply **resource management** for memory-intensive operations\n",
    "- Use **context managers** for temporary state changes\n",
    "- Implement **clean interfaces** that can operate in silent mode\n",
    "- Consider **performance benefits** of suppressing verbose output\n",
    "\n",
    "### Recommended Workflows:\n",
    "1. **Batch Processing**: Use quiet mode for multiple dataset analysis\n",
    "2. **Automated Pipelines**: Suppress output during production runs\n",
    "3. **Interactive Development**: Use normal mode for debugging, quiet for final runs\n",
    "4. **Resource Management**: Implement context managers for cleanup\n",
    "5. **Performance Optimization**: Profile with and without output suppression\n",
    "\n",
    "### Context Manager Patterns:\n",
    "```python\n",
    "# Basic suppression\n",
    "with scitex.context.suppress_output():\n",
    "    noisy_function()\n",
    "\n",
    "# Quiet operations\n",
    "with scitex.context.quiet():\n",
    "    batch_process_data()\n",
    "\n",
    "# Resource management\n",
    "with ResourceManager() as manager:\n",
    "    manager.process_data()\n",
    "    # Automatic cleanup on exit\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "import shutil\n",
    "\n",
    "cleanup = input(\"Clean up example files? (y/n): \").lower().startswith('y')\n",
    "if cleanup:\n",
    "    shutil.rmtree(data_dir)\n",
    "    print(\"✓ Example files cleaned up\")\n",
    "else:\n",
    "    print(f\"Example files preserved in: {data_dir}\")\n",
    "    if data_dir.exists():\n",
    "        files = list(data_dir.rglob('*'))\n",
    "        print(f\"Files created: {len([f for f in files if f.is_file()])}\")\n",
    "        print(f\"Directories created: {len([d for d in files if d.is_dir()])}\")\n",
    "        \n",
    "        if files:\n",
    "            total_size = sum(f.stat().st_size for f in files if f.is_file())\n",
    "            print(f\"Total size: {scitex.str.readable_bytes(total_size)}\")\n",
    "\n",
    "print(\"\\nSciTeX Context Management Tutorial Complete!\")\n",
    "print(\"\\nKey takeaways:\")\n",
    "print(\"- Use suppress_output() for clean automation\")\n",
    "print(\"- Apply quiet() for batch processing\")\n",
    "print(\"- Implement proper resource management\")\n",
    "print(\"- Consider performance benefits of output suppression\")\n",
    "print(\"- Use nested contexts for complex workflows\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}