{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SciTeX Decorators Module\n",
    "\n",
    "This notebook demonstrates the powerful decorator utilities provided by the SciTeX `decorators` module. These decorators enhance scientific computing functions with:\n",
    "\n",
    "- **Data Type Conversion**: Automatic conversion between numpy, pandas, torch formats\n",
    "- **Caching**: Memory and disk-based function result caching\n",
    "- **Batch Processing**: Efficient batch operations on arrays and dataframes\n",
    "- **Signal Processing**: Specialized decorators for signal analysis\n",
    "- **Performance**: Timeout controls and optimization utilities\n",
    "- **Development**: Deprecation warnings and implementation tracking\n",
    "\n",
    "These decorators provide clean, reusable enhancements for scientific computing workflows.\n",
    "\n",
    "## Installation and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scitex as stx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "# Configure SciTeX for this notebook\n",
    "stx.repro.fix_seeds(42)\n",
    "print(\"SciTeX Decorators Module Demonstration\")\n",
    "print(f\"SciTeX version: {stx.__version__}\")\n",
    "\n",
    "# Create working directory for cache demonstrations\n",
    "work_dir = Path('./temp_decorators_demo')\n",
    "work_dir.mkdir(exist_ok=True)\n",
    "print(f\"Working directory: {work_dir.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Type Conversion Decorators\n",
    "\n",
    "Automatically convert function inputs and outputs between different data formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate data type conversion decorators\n",
    "print(\"=== Data Type Conversion Decorators ===\")\n",
    "\n",
    "# Create sample data in different formats\n",
    "numpy_data = np.random.randn(100, 5)\n",
    "pandas_data = pd.DataFrame(numpy_data, columns=['A', 'B', 'C', 'D', 'E'])\n",
    "\n",
    "print(f\"Original numpy data shape: {numpy_data.shape}\")\n",
    "print(f\"Original pandas data shape: {pandas_data.shape}\")\n",
    "\n",
    "# NumPy decorator - ensures inputs/outputs are numpy arrays\n",
    "@stx.decorators.numpy_fn\n",
    "def numpy_statistical_analysis(data):\n",
    "    \"\"\"Perform statistical analysis expecting numpy input.\"\"\"\n",
    "    print(f\"   Processing numpy array with shape: {data.shape}\")\n",
    "    return {\n",
    "        'mean': np.mean(data, axis=0),\n",
    "        'std': np.std(data, axis=0),\n",
    "        'correlation': np.corrcoef(data.T)\n",
    "    }\n",
    "\n",
    "print(\"\\n1. NumPy function decorator:\")\n",
    "# Test with pandas input - should be automatically converted\n",
    "numpy_result = numpy_statistical_analysis(pandas_data)\n",
    "print(f\"   Result keys: {list(numpy_result.keys())}\")\n",
    "print(f\"   Mean shape: {numpy_result['mean'].shape}\")\n",
    "print(f\"   Correlation shape: {numpy_result['correlation'].shape}\")\n",
    "\n",
    "# Pandas decorator - ensures inputs/outputs work with DataFrames\n",
    "@stx.decorators.pandas_fn\n",
    "def pandas_data_processing(df):\n",
    "    \"\"\"Process data expecting pandas DataFrame.\"\"\"\n",
    "    print(f\"   Processing DataFrame with shape: {df.shape}\")\n",
    "    processed = df.copy()\n",
    "    processed['mean_row'] = df.mean(axis=1)\n",
    "    processed['sum_row'] = df.sum(axis=1)\n",
    "    return processed\n",
    "\n",
    "print(\"\\n2. Pandas function decorator:\")\n",
    "# Test with numpy input - should be automatically converted\n",
    "pandas_result = pandas_data_processing(numpy_data)\n",
    "print(f\"   Result type: {type(pandas_result)}\")\n",
    "print(f\"   Result columns: {list(pandas_result.columns)}\")\n",
    "print(f\"   Added columns: {pandas_result.columns[-2:].tolist()}\")\n",
    "\n",
    "# Demonstrate conversion utilities\n",
    "print(\"\\n3. Manual conversion utilities:\")\n",
    "# Check data types\n",
    "print(f\"   Is numpy array torch tensor? {stx.decorators.is_torch(numpy_data)}\")\n",
    "\n",
    "# Convert between formats\n",
    "numpy_converted = stx.decorators.to_numpy(pandas_data)\n",
    "print(f\"   Pandas to numpy conversion: {type(numpy_converted)} {numpy_converted.shape}\")\n",
    "\n",
    "# Show original vs converted data consistency\n",
    "conversion_match = np.allclose(numpy_data, numpy_converted)\n",
    "print(f\"   Conversion preserves data: {conversion_match}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Caching Decorators\n",
    "\n",
    "Implement memory and disk-based caching for expensive computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate caching decorators\n",
    "print(\"=== Caching Decorators ===\")\n",
    "\n",
    "# Memory caching decorator\n",
    "@stx.decorators.cache_mem\n",
    "def expensive_computation(n, complexity=1):\n",
    "    \"\"\"Simulate expensive computation with memory caching.\"\"\"\n",
    "    print(f\"   Computing expensive operation for n={n}, complexity={complexity}\")\n",
    "    time.sleep(0.1 * complexity)  # Simulate computation time\n",
    "    result = np.sum([i**2 for i in range(n)])\n",
    "    return result, np.random.randn(10)  # Return tuple with computed values\n",
    "\n",
    "print(\"1. Memory caching demonstration:\")\n",
    "# First call - should compute\n",
    "start_time = time.time()\n",
    "result1, array1 = expensive_computation(1000, complexity=2)\n",
    "first_call_time = time.time() - start_time\n",
    "\n",
    "# Second call with same parameters - should use cache\n",
    "start_time = time.time()\n",
    "result2, array2 = expensive_computation(1000, complexity=2)\n",
    "second_call_time = time.time() - start_time\n",
    "\n",
    "print(f\"   First call time: {first_call_time:.3f} seconds\")\n",
    "print(f\"   Second call time: {second_call_time:.6f} seconds\")\n",
    "print(f\"   Speedup: {first_call_time/second_call_time:.0f}x\")\n",
    "print(f\"   Results match: {result1 == result2}\")\n",
    "\n",
    "# Disk caching decorator\n",
    "cache_dir = work_dir / 'cache'\n",
    "cache_dir.mkdir(exist_ok=True)\n",
    "\n",
    "@stx.decorators.cache_disk(cache_dir=str(cache_dir))\n",
    "def data_intensive_analysis(data_size, analysis_type='correlation'):\n",
    "    \"\"\"Simulate data-intensive analysis with disk caching.\"\"\"\n",
    "    print(f\"   Performing {analysis_type} analysis on {data_size} data points\")\n",
    "    time.sleep(0.15)  # Simulate processing time\n",
    "    \n",
    "    # Generate sample data and perform analysis\n",
    "    data = np.random.randn(data_size, 10)\n",
    "    \n",
    "    if analysis_type == 'correlation':\n",
    "        result = np.corrcoef(data.T)\n",
    "    elif analysis_type == 'pca':\n",
    "        # Simplified PCA\n",
    "        cov_matrix = np.cov(data.T)\n",
    "        eigenvals, eigenvecs = np.linalg.eigh(cov_matrix)\n",
    "        result = {'eigenvalues': eigenvals, 'eigenvectors': eigenvecs}\n",
    "    else:\n",
    "        result = np.mean(data, axis=0)\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"\\n2. Disk caching demonstration:\")\n",
    "# First call - should compute and cache to disk\n",
    "start_time = time.time()\n",
    "correlation_result1 = data_intensive_analysis(5000, 'correlation')\n",
    "first_disk_time = time.time() - start_time\n",
    "\n",
    "# Second call - should load from disk cache\n",
    "start_time = time.time()\n",
    "correlation_result2 = data_intensive_analysis(5000, 'correlation')\n",
    "second_disk_time = time.time() - start_time\n",
    "\n",
    "print(f\"   First call time: {first_disk_time:.3f} seconds\")\n",
    "print(f\"   Second call time: {second_disk_time:.3f} seconds\")\n",
    "print(f\"   Results shape: {correlation_result1.shape}\")\n",
    "print(f\"   Results match: {np.allclose(correlation_result1, correlation_result2)}\")\n",
    "\n",
    "# Check cache files\n",
    "cache_files = list(cache_dir.glob('*'))\n",
    "print(f\"   Cache files created: {len(cache_files)}\")\n",
    "for cache_file in cache_files:\n",
    "    file_size = cache_file.stat().st_size\n",
    "    print(f\"     {cache_file.name}: {file_size} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Batch Processing Decorators\n",
    "\n",
    "Efficiently process data in batches with automatic chunking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate batch processing decorators\n",
    "print(\"=== Batch Processing Decorators ===\")\n",
    "\n",
    "# Create large dataset for batch processing\n",
    "large_dataset = np.random.randn(10000, 50)\n",
    "print(f\"Large dataset shape: {large_dataset.shape}\")\n",
    "\n",
    "# Batch function decorator for general batching\n",
    "@stx.decorators.batch_fn(batch_size=1000)\n",
    "def process_data_batch(data_batch):\n",
    "    \"\"\"Process data in batches to manage memory usage.\"\"\"\n",
    "    print(f\"   Processing batch with shape: {data_batch.shape}\")\n",
    "    # Simulate some computation\n",
    "    processed = np.mean(data_batch, axis=1)\n",
    "    return processed\n",
    "\n",
    "print(\"1. General batch processing:\")\n",
    "batch_results = process_data_batch(large_dataset)\n",
    "print(f\"   Batch results shape: {batch_results.shape}\")\n",
    "print(f\"   Expected shape: ({large_dataset.shape[0]},)\")\n",
    "\n",
    "# Batch NumPy function - combines batching with numpy conversion\n",
    "@stx.decorators.batch_numpy_fn(batch_size=2000)\n",
    "def numpy_batch_analysis(data_batch):\n",
    "    \"\"\"Perform numpy analysis in batches.\"\"\"\n",
    "    print(f\"   NumPy batch processing: {data_batch.shape}\")\n",
    "    # Compute statistical measures\n",
    "    stats = {\n",
    "        'mean': np.mean(data_batch, axis=1),\n",
    "        'std': np.std(data_batch, axis=1),\n",
    "        'max': np.max(data_batch, axis=1)\n",
    "    }\n",
    "    return stats\n",
    "\n",
    "print(\"\\n2. NumPy batch processing:\")\n",
    "numpy_batch_results = numpy_batch_analysis(large_dataset)\n",
    "print(f\"   Result keys: {list(numpy_batch_results.keys())}\")\n",
    "for key, values in numpy_batch_results.items():\n",
    "    print(f\"   {key} shape: {values.shape}\")\n",
    "\n",
    "# Batch pandas processing\n",
    "large_dataframe = pd.DataFrame(large_dataset, \n",
    "                               columns=[f'feature_{i}' for i in range(50)])\n",
    "\n",
    "@stx.decorators.batch_pandas_fn(batch_size=1500)\n",
    "def pandas_batch_processing(df_batch):\n",
    "    \"\"\"Process pandas DataFrame in batches.\"\"\"\n",
    "    print(f\"   Pandas batch processing: {df_batch.shape}\")\n",
    "    # Compute correlations and add derived features\n",
    "    processed = df_batch.copy()\n",
    "    processed['mean_features'] = df_batch.mean(axis=1)\n",
    "    processed['feature_range'] = df_batch.max(axis=1) - df_batch.min(axis=1)\n",
    "    return processed[['mean_features', 'feature_range']]  # Return only derived features\n",
    "\n",
    "print(\"\\n3. Pandas batch processing:\")\n",
    "pandas_batch_results = pandas_batch_processing(large_dataframe)\n",
    "print(f\"   Result type: {type(pandas_batch_results)}\")\n",
    "print(f\"   Result shape: {pandas_batch_results.shape}\")\n",
    "print(f\"   Result columns: {list(pandas_batch_results.columns)}\")\n",
    "print(f\"   Sample results:\")\n",
    "print(pandas_batch_results.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Signal Processing Decorators\n",
    "\n",
    "Specialized decorators for signal analysis and processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate signal processing decorators\n",
    "print(\"=== Signal Processing Decorators ===\")\n",
    "\n",
    "# Generate sample signals\n",
    "sampling_rate = 1000  # Hz\n",
    "duration = 2.0  # seconds\n",
    "t = np.linspace(0, duration, int(sampling_rate * duration))\n",
    "\n",
    "# Create complex signal with multiple components\n",
    "signal_clean = (np.sin(2 * np.pi * 10 * t) +  # 10 Hz sine wave\n",
    "                0.5 * np.sin(2 * np.pi * 50 * t) +  # 50 Hz sine wave\n",
    "                0.3 * np.sin(2 * np.pi * 100 * t))  # 100 Hz sine wave\n",
    "\n",
    "# Add noise\n",
    "noise = 0.2 * np.random.randn(len(t))\n",
    "signal_noisy = signal_clean + noise\n",
    "\n",
    "print(f\"Signal duration: {duration} seconds\")\n",
    "print(f\"Sampling rate: {sampling_rate} Hz\")\n",
    "print(f\"Signal length: {len(signal_noisy)} samples\")\n",
    "\n",
    "# Signal function decorator - ensures proper signal format\n",
    "@stx.decorators.signal_fn\n",
    "def signal_filtering(signal, cutoff_freq=30, sampling_rate=1000):\n",
    "    \"\"\"Apply simple filtering to signal data.\"\"\"\n",
    "    print(f\"   Filtering signal with {len(signal)} samples\")\n",
    "    print(f\"   Cutoff frequency: {cutoff_freq} Hz\")\n",
    "    \n",
    "    # Simple moving average filter (not ideal, but for demonstration)\n",
    "    window_size = int(sampling_rate / cutoff_freq)\n",
    "    if window_size % 2 == 0:\n",
    "        window_size += 1  # Ensure odd window size\n",
    "    \n",
    "    # Apply moving average\n",
    "    filtered = np.convolve(signal, np.ones(window_size)/window_size, mode='same')\n",
    "    \n",
    "    return {\n",
    "        'filtered_signal': filtered,\n",
    "        'original_length': len(signal),\n",
    "        'filter_window': window_size\n",
    "    }\n",
    "\n",
    "print(\"\\n1. Signal filtering with decorator:\")\n",
    "filter_result = signal_filtering(signal_noisy, cutoff_freq=25, sampling_rate=sampling_rate)\n",
    "print(f\"   Filtered signal length: {len(filter_result['filtered_signal'])}\")\n",
    "print(f\"   Filter window size: {filter_result['filter_window']}\")\n",
    "\n",
    "# Combined signal and batch processing\n",
    "# Create multiple signals for batch processing\n",
    "n_signals = 50\n",
    "signal_matrix = np.array([signal_noisy + 0.1 * np.random.randn(len(signal_noisy)) \n",
    "                         for _ in range(n_signals)])\n",
    "\n",
    "@stx.decorators.batch_fn(batch_size=10)\n",
    "@stx.decorators.signal_fn\n",
    "def batch_signal_analysis(signal_batch):\n",
    "    \"\"\"Analyze multiple signals in batches.\"\"\"\n",
    "    print(f\"   Analyzing signal batch: {signal_batch.shape}\")\n",
    "    \n",
    "    # Compute signal statistics\n",
    "    results = []\n",
    "    for signal in signal_batch:\n",
    "        # Basic signal metrics\n",
    "        power = np.mean(signal**2)\n",
    "        rms = np.sqrt(power)\n",
    "        peak_to_peak = np.max(signal) - np.min(signal)\n",
    "        \n",
    "        results.append({\n",
    "            'power': power,\n",
    "            'rms': rms,\n",
    "            'peak_to_peak': peak_to_peak\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"\\n2. Batch signal analysis:\")\n",
    "batch_signal_results = batch_signal_analysis(signal_matrix)\n",
    "print(f\"   Processed {len(batch_signal_results)} signals\")\n",
    "print(f\"   Sample result keys: {list(batch_signal_results[0].keys())}\")\n",
    "\n",
    "# Convert results to DataFrame for analysis\n",
    "signal_stats_df = pd.DataFrame(batch_signal_results)\n",
    "print(f\"\\n   Signal statistics summary:\")\n",
    "print(signal_stats_df.describe().round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Performance and Control Decorators\n",
    "\n",
    "Implement timeout controls and performance monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate performance and control decorators\n",
    "print(\"=== Performance and Control Decorators ===\")\n",
    "\n",
    "# Timeout decorator\n",
    "@stx.decorators.timeout(seconds=2)\n",
    "def fast_computation(n):\n",
    "    \"\"\"Fast computation that should complete within timeout.\"\"\"\n",
    "    print(f\"   Fast computation with n={n}\")\n",
    "    time.sleep(0.1)  # Quick computation\n",
    "    return sum(i**2 for i in range(n))\n",
    "\n",
    "@stx.decorators.timeout(seconds=1)\n",
    "def slow_computation(n):\n",
    "    \"\"\"Slow computation that might timeout.\"\"\"\n",
    "    print(f\"   Slow computation with n={n}\")\n",
    "    time.sleep(1.5)  # Slow computation - will timeout\n",
    "    return sum(i**3 for i in range(n))\n",
    "\n",
    "print(\"1. Timeout decorator demonstration:\")\n",
    "\n",
    "# Test fast computation\n",
    "try:\n",
    "    fast_result = fast_computation(1000)\n",
    "    print(f\"   Fast computation result: {fast_result}\")\n",
    "except Exception as e:\n",
    "    print(f\"   Fast computation failed: {e}\")\n",
    "\n",
    "# Test slow computation (should timeout)\n",
    "try:\n",
    "    slow_result = slow_computation(1000)\n",
    "    print(f\"   Slow computation result: {slow_result}\")\n",
    "except Exception as e:\n",
    "    print(f\"   Slow computation timed out: {type(e).__name__}\")\n",
    "\n",
    "# Wrap decorator for function enhancement\n",
    "def timing_wrapper(func):\n",
    "    \"\"\"Custom wrapper to measure execution time.\"\"\"\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        print(f\"   {func.__name__} executed in {end_time - start_time:.4f} seconds\")\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "@stx.decorators.wrap(timing_wrapper)\n",
    "def mathematical_computation(data_size, operation='mean'):\n",
    "    \"\"\"Perform mathematical computation with timing.\"\"\"\n",
    "    data = np.random.randn(data_size, data_size)\n",
    "    \n",
    "    if operation == 'mean':\n",
    "        result = np.mean(data)\n",
    "    elif operation == 'eigenvals':\n",
    "        result = np.linalg.eigvals(data)\n",
    "    elif operation == 'svd':\n",
    "        u, s, vh = np.linalg.svd(data)\n",
    "        result = s\n",
    "    else:\n",
    "        result = np.sum(data)\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"\\n2. Function wrapping with timing:\")\n",
    "mean_result = mathematical_computation(500, 'mean')\n",
    "eigenvals_result = mathematical_computation(200, 'eigenvals')\n",
    "svd_result = mathematical_computation(300, 'svd')\n",
    "\n",
    "print(f\"   Mean result: {mean_result:.4f}\")\n",
    "print(f\"   Eigenvalues count: {len(eigenvals_result)}\")\n",
    "print(f\"   SVD singular values count: {len(svd_result)}\")\n",
    "\n",
    "# Combined decorators for comprehensive function enhancement\n",
    "@stx.decorators.cache_mem\n",
    "@stx.decorators.timeout(seconds=5)\n",
    "@stx.decorators.numpy_fn\n",
    "def enhanced_data_analysis(data, analysis_type='full'):\n",
    "    \"\"\"Enhanced data analysis with multiple decorators.\"\"\"\n",
    "    print(f\"   Enhanced analysis on data shape: {data.shape}\")\n",
    "    time.sleep(0.2)  # Simulate computation\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    if analysis_type in ['full', 'stats']:\n",
    "        results['mean'] = np.mean(data, axis=0)\n",
    "        results['std'] = np.std(data, axis=0)\n",
    "        results['correlation'] = np.corrcoef(data.T)\n",
    "    \n",
    "    if analysis_type in ['full', 'decomp']:\n",
    "        # PCA-like analysis\n",
    "        cov_matrix = np.cov(data.T)\n",
    "        eigenvals, eigenvecs = np.linalg.eigh(cov_matrix)\n",
    "        results['eigenvalues'] = eigenvals\n",
    "        results['explained_variance'] = eigenvals / np.sum(eigenvals)\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"\\n3. Multiple decorator combination:\")\n",
    "test_data = np.random.randn(1000, 10)\n",
    "\n",
    "# First call - should compute\n",
    "start_time = time.time()\n",
    "enhanced_result1 = enhanced_data_analysis(test_data, 'full')\n",
    "first_enhanced_time = time.time() - start_time\n",
    "\n",
    "# Second call - should use cache\n",
    "start_time = time.time()\n",
    "enhanced_result2 = enhanced_data_analysis(test_data, 'full')\n",
    "second_enhanced_time = time.time() - start_time\n",
    "\n",
    "print(f\"   First call time: {first_enhanced_time:.3f} seconds\")\n",
    "print(f\"   Second call time: {second_enhanced_time:.6f} seconds\")\n",
    "print(f\"   Speedup from caching: {first_enhanced_time/second_enhanced_time:.0f}x\")\n",
    "print(f\"   Result keys: {list(enhanced_result1.keys())}\")\n",
    "print(f\"   Explained variance sum: {np.sum(enhanced_result1['explained_variance']):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Development and Documentation Decorators\n",
    "\n",
    "Manage deprecated functions and preserve documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate development and documentation decorators\n",
    "print(\"=== Development and Documentation Decorators ===\")\n",
    "\n",
    "# Deprecated function decorator\n",
    "@stx.decorators.deprecated(\"Use new_analysis_function instead\", version=\"2.0\")\n",
    "def old_analysis_function(data):\n",
    "    \"\"\"Old analysis function that is deprecated.\"\"\"\n",
    "    print(\"   Running deprecated analysis function\")\n",
    "    return np.mean(data)\n",
    "\n",
    "def new_analysis_function(data):\n",
    "    \"\"\"New improved analysis function.\"\"\"\n",
    "    print(\"   Running new analysis function\")\n",
    "    return {\n",
    "        'mean': np.mean(data),\n",
    "        'std': np.std(data),\n",
    "        'count': len(data)\n",
    "    }\n",
    "\n",
    "print(\"1. Deprecation warning demonstration:\")\n",
    "test_data = np.random.randn(100)\n",
    "\n",
    "# Use deprecated function - should show warning\n",
    "with warnings.catch_warnings(record=True) as w:\n",
    "    warnings.simplefilter(\"always\")\n",
    "    old_result = old_analysis_function(test_data)\n",
    "    \n",
    "    if w:\n",
    "        print(f\"   Deprecation warning captured: {w[0].message}\")\n",
    "    print(f\"   Old function result: {old_result:.4f}\")\n",
    "\n",
    "# Use new function\n",
    "new_result = new_analysis_function(test_data)\n",
    "print(f\"   New function result: {new_result}\")\n",
    "\n",
    "# Not implemented decorator\n",
    "@stx.decorators.not_implemented\n",
    "def future_analysis_feature(data, method='advanced'):\n",
    "    \"\"\"Advanced analysis feature to be implemented.\"\"\"\n",
    "    pass\n",
    "\n",
    "print(\"\\n2. Not implemented decorator:\")\n",
    "try:\n",
    "    future_result = future_analysis_feature(test_data)\n",
    "except NotImplementedError as e:\n",
    "    print(f\"   Not implemented error: {e}\")\n",
    "\n",
    "# Preserve documentation decorator\n",
    "def original_function(x, y):\n",
    "    \"\"\"Original function with detailed documentation.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    x : array_like\n",
    "        Input data array\n",
    "    y : array_like\n",
    "        Secondary data array\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    float\n",
    "        Computed correlation coefficient\n",
    "    \"\"\"\n",
    "    return np.corrcoef(x, y)[0, 1]\n",
    "\n",
    "@stx.decorators.preserve_doc(original_function)\n",
    "@stx.decorators.numpy_fn\n",
    "def enhanced_correlation_function(x, y):\n",
    "    \"\"\"This documentation will be replaced.\"\"\"\n",
    "    print(f\"   Computing enhanced correlation\")\n",
    "    correlation = np.corrcoef(x, y)[0, 1]\n",
    "    p_value = 0.05  # Simplified p-value calculation\n",
    "    return {\n",
    "        'correlation': correlation,\n",
    "        'p_value': p_value,\n",
    "        'significant': abs(correlation) > 0.3\n",
    "    }\n",
    "\n",
    "print(\"\\n3. Documentation preservation:\")\n",
    "x_data = np.random.randn(100)\n",
    "y_data = x_data + 0.5 * np.random.randn(100)  # Correlated data\n",
    "\n",
    "enhanced_corr_result = enhanced_correlation_function(x_data, y_data)\n",
    "print(f\"   Enhanced correlation result: {enhanced_corr_result}\")\n",
    "print(f\"   Function name: {enhanced_correlation_function.__name__}\")\n",
    "print(f\"   Function doc preserved: {enhanced_correlation_function.__doc__[:50]}...\")\n",
    "\n",
    "# Demonstrate auto-order decorator (if available)\n",
    "print(\"\\n4. Auto-order functionality:\")\n",
    "try:\n",
    "    stx.decorators.enable_auto_order()\n",
    "    print(\"   Auto-order enabled\")\n",
    "    \n",
    "    @stx.decorators.numpy_fn\n",
    "    def auto_ordered_function(data):\n",
    "        return np.mean(data, axis=0)\n",
    "    \n",
    "    auto_result = auto_ordered_function(np.random.randn(50, 5))\n",
    "    print(f\"   Auto-ordered result shape: {auto_result.shape}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   Auto-order feature: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Real-World Scientific Workflow Integration\n",
    "\n",
    "Demonstrate how decorators integrate into complete scientific workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate real-world scientific workflow with decorators\n",
    "print(\"=== Real-World Scientific Workflow Integration ===\")\n",
    "\n",
    "class ScientificDataProcessor:\n",
    "    \"\"\"Scientific data processing pipeline with decorators.\"\"\"\n",
    "    \n",
    "    def __init__(self, cache_dir=None):\n",
    "        self.cache_dir = cache_dir or work_dir / 'workflow_cache'\n",
    "        self.cache_dir.mkdir(exist_ok=True)\n",
    "        self.processing_stats = {}\n",
    "    \n",
    "    @stx.decorators.cache_disk(cache_dir=str(work_dir / 'workflow_cache'))\n",
    "    @stx.decorators.timeout(seconds=10)\n",
    "    @stx.decorators.numpy_fn\n",
    "    def preprocess_data(self, raw_data, normalize=True, remove_outliers=True):\n",
    "        \"\"\"Preprocess raw experimental data.\"\"\"\n",
    "        print(f\"   Preprocessing data with shape: {raw_data.shape}\")\n",
    "        time.sleep(0.1)  # Simulate preprocessing time\n",
    "        \n",
    "        processed = raw_data.copy()\n",
    "        \n",
    "        # Remove outliers\n",
    "        if remove_outliers:\n",
    "            q1, q3 = np.percentile(processed, [25, 75], axis=0)\n",
    "            iqr = q3 - q1\n",
    "            lower_bound = q1 - 1.5 * iqr\n",
    "            upper_bound = q3 + 1.5 * iqr\n",
    "            \n",
    "            # Clip outliers\n",
    "            processed = np.clip(processed, lower_bound, upper_bound)\n",
    "        \n",
    "        # Normalize\n",
    "        if normalize:\n",
    "            processed = (processed - np.mean(processed, axis=0)) / np.std(processed, axis=0)\n",
    "        \n",
    "        return {\n",
    "            'data': processed,\n",
    "            'original_shape': raw_data.shape,\n",
    "            'outliers_removed': remove_outliers,\n",
    "            'normalized': normalize\n",
    "        }\n",
    "    \n",
    "    @stx.decorators.batch_numpy_fn(batch_size=500)\n",
    "    @stx.decorators.cache_mem\n",
    "    def feature_extraction(self, data_batch):\n",
    "        \"\"\"Extract features from data batches.\"\"\"\n",
    "        print(f\"   Extracting features from batch: {data_batch.shape}\")\n",
    "        \n",
    "        features = []\n",
    "        for sample in data_batch:\n",
    "            # Extract various statistical features\n",
    "            feature_vector = {\n",
    "                'mean': np.mean(sample),\n",
    "                'std': np.std(sample),\n",
    "                'skewness': self._calculate_skewness(sample),\n",
    "                'kurtosis': self._calculate_kurtosis(sample),\n",
    "                'energy': np.sum(sample**2),\n",
    "                'peak_to_peak': np.max(sample) - np.min(sample)\n",
    "            }\n",
    "            features.append(list(feature_vector.values()))\n",
    "        \n",
    "        return np.array(features)\n",
    "    \n",
    "    @stx.decorators.timeout(seconds=15)\n",
    "    @stx.decorators.pandas_fn\n",
    "    def statistical_analysis(self, features_df):\n",
    "        \"\"\"Perform statistical analysis on extracted features.\"\"\"\n",
    "        print(f\"   Statistical analysis on DataFrame: {features_df.shape}\")\n",
    "        time.sleep(0.2)  # Simulate analysis time\n",
    "        \n",
    "        # Correlation analysis\n",
    "        correlation_matrix = features_df.corr()\n",
    "        \n",
    "        # Principal component analysis\n",
    "        data_standardized = (features_df - features_df.mean()) / features_df.std()\n",
    "        cov_matrix = np.cov(data_standardized.T)\n",
    "        eigenvals, eigenvecs = np.linalg.eigh(cov_matrix)\n",
    "        \n",
    "        # Sort by eigenvalues (descending)\n",
    "        idx = np.argsort(eigenvals)[::-1]\n",
    "        eigenvals = eigenvals[idx]\n",
    "        eigenvecs = eigenvecs[:, idx]\n",
    "        \n",
    "        explained_variance = eigenvals / np.sum(eigenvals)\n",
    "        \n",
    "        return {\n",
    "            'correlation_matrix': correlation_matrix,\n",
    "            'eigenvalues': eigenvals,\n",
    "            'explained_variance': explained_variance,\n",
    "            'cumulative_variance': np.cumsum(explained_variance),\n",
    "            'feature_stats': features_df.describe()\n",
    "        }\n",
    "    \n",
    "    def _calculate_skewness(self, data):\n",
    "        \"\"\"Calculate skewness of data.\"\"\"\n",
    "        mean = np.mean(data)\n",
    "        std = np.std(data)\n",
    "        return np.mean(((data - mean) / std) ** 3)\n",
    "    \n",
    "    def _calculate_kurtosis(self, data):\n",
    "        \"\"\"Calculate kurtosis of data.\"\"\"\n",
    "        mean = np.mean(data)\n",
    "        std = np.std(data)\n",
    "        return np.mean(((data - mean) / std) ** 4) - 3\n",
    "    \n",
    "    @stx.decorators.wrap(lambda f: lambda *args, **kwargs: print(f\"Generating report...\") or f(*args, **kwargs))\n",
    "    def generate_report(self, analysis_results):\n",
    "        \"\"\"Generate comprehensive analysis report.\"\"\"\n",
    "        report = {\n",
    "            'summary': {\n",
    "                'total_features': len(analysis_results['feature_stats'].columns),\n",
    "                'explained_variance_pc1': analysis_results['explained_variance'][0],\n",
    "                'explained_variance_pc2': analysis_results['explained_variance'][1],\n",
    "                'cumulative_variance_pc2': analysis_results['cumulative_variance'][1],\n",
    "                'max_correlation': analysis_results['correlation_matrix'].abs().max().max()\n",
    "            },\n",
    "            'recommendations': []\n",
    "        }\n",
    "        \n",
    "        # Add recommendations based on analysis\n",
    "        if report['summary']['explained_variance_pc1'] > 0.7:\n",
    "            report['recommendations'].append(\"First PC explains >70% variance - consider dimensionality reduction\")\n",
    "        \n",
    "        if report['summary']['max_correlation'] > 0.9:\n",
    "            report['recommendations'].append(\"High correlation detected - check for redundant features\")\n",
    "        \n",
    "        if report['summary']['cumulative_variance_pc2'] > 0.95:\n",
    "            report['recommendations'].append(\"First two PCs explain >95% variance - 2D representation viable\")\n",
    "        \n",
    "        return report\n",
    "\n",
    "# Demonstrate complete workflow\n",
    "print(\"1. Complete scientific data processing workflow:\")\n",
    "\n",
    "# Generate synthetic experimental data\n",
    "n_samples = 2000\n",
    "n_features = 20\n",
    "experimental_data = np.random.randn(n_samples, n_features)\n",
    "\n",
    "# Add some structure to the data\n",
    "experimental_data[:, :5] += np.random.randn(n_samples, 1) * 2  # Correlated features\n",
    "experimental_data[:500, 10:15] += 3  # Group differences\n",
    "\n",
    "# Initialize processor\n",
    "processor = ScientificDataProcessor()\n",
    "\n",
    "# Step 1: Preprocess data\n",
    "print(\"\\n2. Data preprocessing:\")\n",
    "preprocessed = processor.preprocess_data(experimental_data)\n",
    "print(f\"   Preprocessed data shape: {preprocessed['data'].shape}\")\n",
    "print(f\"   Normalization applied: {preprocessed['normalized']}\")\n",
    "\n",
    "# Step 2: Feature extraction\n",
    "print(\"\\n3. Feature extraction:\")\n",
    "features = processor.feature_extraction(preprocessed['data'])\n",
    "print(f\"   Extracted features shape: {features.shape}\")\n",
    "\n",
    "# Convert to DataFrame for analysis\n",
    "feature_names = ['mean', 'std', 'skewness', 'kurtosis', 'energy', 'peak_to_peak']\n",
    "features_df = pd.DataFrame(features, columns=feature_names)\n",
    "\n",
    "# Step 3: Statistical analysis\n",
    "print(\"\\n4. Statistical analysis:\")\n",
    "analysis_results = processor.statistical_analysis(features_df)\n",
    "print(f\"   Correlation matrix shape: {analysis_results['correlation_matrix'].shape}\")\n",
    "print(f\"   First PC explains {analysis_results['explained_variance'][0]:.1%} of variance\")\n",
    "print(f\"   First two PCs explain {analysis_results['cumulative_variance'][1]:.1%} of variance\")\n",
    "\n",
    "# Step 4: Generate report\n",
    "print(\"\\n5. Report generation:\")\n",
    "final_report = processor.generate_report(analysis_results)\n",
    "print(f\"   Report summary: {final_report['summary']}\")\n",
    "print(f\"   Recommendations ({len(final_report['recommendations'])}):\")\n",
    "for i, rec in enumerate(final_report['recommendations'], 1):\n",
    "    print(f\"     {i}. {rec}\")\n",
    "\n",
    "print(f\"\\n✓ Scientific workflow completed successfully!\")\n",
    "print(f\"   Cache files created: {len(list((work_dir / 'workflow_cache').glob('*')))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary and Best Practices\n",
    "\n",
    "The SciTeX decorators module provides powerful function enhancements for scientific computing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of key decorator utilities demonstrated\n",
    "summary = {\n",
    "    'Data Type Conversion': [\n",
    "        'stx.decorators.numpy_fn - Auto numpy conversion',\n",
    "        'stx.decorators.pandas_fn - Auto pandas conversion',\n",
    "        'stx.decorators.torch_fn - Auto torch conversion',\n",
    "        'Seamless format interoperability'\n",
    "    ],\n",
    "    'Caching': [\n",
    "        'stx.decorators.cache_mem - Memory caching',\n",
    "        'stx.decorators.cache_disk - Persistent disk caching',\n",
    "        'Automatic cache invalidation',\n",
    "        'Performance optimization'\n",
    "    ],\n",
    "    'Batch Processing': [\n",
    "        'stx.decorators.batch_fn - General batch processing',\n",
    "        'stx.decorators.batch_numpy_fn - NumPy batch ops',\n",
    "        'stx.decorators.batch_pandas_fn - Pandas batch ops',\n",
    "        'Memory-efficient large data handling'\n",
    "    ],\n",
    "    'Signal Processing': [\n",
    "        'stx.decorators.signal_fn - Signal format validation',\n",
    "        'Integration with batch processing',\n",
    "        'Signal-specific optimizations',\n",
    "        'Scientific signal analysis workflows'\n",
    "    ],\n",
    "    'Performance Control': [\n",
    "        'stx.decorators.timeout - Execution time limits',\n",
    "        'stx.decorators.wrap - Custom wrapper functions',\n",
    "        'Performance monitoring integration',\n",
    "        'Resource management'\n",
    "    ],\n",
    "    'Development Support': [\n",
    "        'stx.decorators.deprecated - Deprecation warnings',\n",
    "        'stx.decorators.not_implemented - Feature placeholders',\n",
    "        'stx.decorators.preserve_doc - Documentation preservation',\n",
    "        'Code maintenance and migration support'\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"SciTeX Decorators Module - Key Utilities Summary\")\n",
    "print(\"=\" * 58)\n",
    "\n",
    "for category, utilities in summary.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    for utility in utilities:\n",
    "        print(f\"  • {utility}\")\n",
    "\n",
    "print(f\"\\n{'='*58}\")\n",
    "print(\"Best Practices:\")\n",
    "print(\"  • Stack decorators in logical order (type conversion → caching → batching)\")\n",
    "print(\"  • Use caching for expensive computations that repeat\")\n",
    "print(\"  • Apply batch processing for memory-intensive operations\")\n",
    "print(\"  • Set appropriate timeouts for long-running functions\")\n",
    "print(\"  • Combine decorators to create powerful function enhancements\")\n",
    "print(\"  • Use deprecation warnings during code migration\")\n",
    "print(\"  • Preserve documentation when enhancing functions\")\n",
    "print(\"  • Test decorator combinations thoroughly\")\n",
    "\n",
    "print(f\"\\nDemo completed successfully! 🎉\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up temporary files and directories\n",
    "import shutil\n",
    "\n",
    "# Remove temporary directories\n",
    "temp_paths = ['./temp_decorators_demo']\n",
    "\n",
    "for temp_path in temp_paths:\n",
    "    if Path(temp_path).exists():\n",
    "        shutil.rmtree(temp_path)\n",
    "        print(f\"Cleaned up: {temp_path}\")\n",
    "\n",
    "print(\"\\nNotebook cleanup completed.\")"
   ]
  }\n ],\n \"metadata\": {\n  \"kernelspec\": {\n   \"display_name\": \"Python 3\",\n   \"language\": \"python\",\n   \"name\": \"python3\"\n  },\n  \"language_info\": {\n   \"codemirror_mode\": {\n    \"name\": \"ipython\",\n    \"version\": 3\n   },\n   \"file_extension\": \".py\",\n   \"mimetype\": \"text/x-python\",\n   \"name\": \"python\",\n   \"nbconvert_exporter\": \"python\",\n   \"pygments_lexer\": \"ipython3\",\n   \"version\": \"3.8.0\"\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 4\n}