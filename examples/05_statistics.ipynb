{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05 - SciTeX Statistics Module Tutorial\n",
    "\n",
    "This comprehensive notebook demonstrates the SciTeX statistics module capabilities for scientific data analysis.\n",
    "\n",
    "## Features Covered\n",
    "\n",
    "### Statistical Testing\n",
    "* Correlation analysis with multiple corrections\n",
    "* Wrapper functions for common statistical tests\n",
    "* P-value to significance star conversion\n",
    "* Partial correlation analysis\n",
    "\n",
    "### Data Description\n",
    "* Enhanced descriptive statistics\n",
    "* NaN-aware statistical functions\n",
    "* Comprehensive data summaries\n",
    "\n",
    "### Multiple Comparisons\n",
    "* Bonferroni correction\n",
    "* False Discovery Rate (FDR) control\n",
    "* Holm-Bonferroni method\n",
    "* Custom correction procedures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "import scitex as stx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from scipy import stats\n",
    "\n",
    "# Set up reproducible environment\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create output directory\n",
    "output_dir = Path(\"./stats_examples\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"SciTeX Statistics Tutorial - Scientific Data Analysis\")\n",
    "print(f\"Output directory: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Correlation Analysis\n",
    "\n",
    "### 1.1 Basic Correlation Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample data with known correlations\n",
    "n_samples = 100\n",
    "x1 = np.random.randn(n_samples)\n",
    "x2 = 0.7 * x1 + 0.3 * np.random.randn(n_samples)  # Strong positive correlation\n",
    "x3 = -0.5 * x1 + 0.8 * np.random.randn(n_samples)  # Moderate negative correlation\n",
    "x4 = np.random.randn(n_samples)  # Independent\n",
    "\n",
    "# Create DataFrame\n",
    "data = pd.DataFrame({\n",
    "    'Variable_A': x1,\n",
    "    'Variable_B': x2,\n",
    "    'Variable_C': x3,\n",
    "    'Variable_D': x4\n",
    "})\n",
    "\n",
    "print(\"Sample Data:\")\n",
    "print(data.head())\n",
    "print(f\"\\nData shape: {data.shape}\")\n",
    "\n",
    "# Basic correlation analysis\n",
    "corr_matrix = data.corr()\n",
    "print(\"\\nCorrelation Matrix:\")\n",
    "print(corr_matrix.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test correlations with SciTeX enhanced functions\n",
    "try:\n",
    "    # Use SciTeX correlation testing if available\n",
    "    corr_results = stx.stats.corr_test_multi(data)\n",
    "    print(\"Enhanced correlation analysis completed\")\n",
    "except AttributeError:\n",
    "    # Fallback to manual correlation testing\n",
    "    print(\"Using manual correlation testing...\")\n",
    "    \n",
    "    correlations = []\n",
    "    p_values = []\n",
    "    \n",
    "    variables = data.columns\n",
    "    for i, var1 in enumerate(variables):\n",
    "        for j, var2 in enumerate(variables):\n",
    "            if i < j:  # Only upper triangle\n",
    "                r, p = stats.pearsonr(data[var1], data[var2])\n",
    "                correlations.append({\n",
    "                    'var1': var1,\n",
    "                    'var2': var2,\n",
    "                    'correlation': r,\n",
    "                    'p_value': p\n",
    "                })\n",
    "    \n",
    "    corr_df = pd.DataFrame(correlations)\n",
    "    print(\"\\nCorrelation Test Results:\")\n",
    "    print(corr_df.round(4))\n",
    "\n",
    "# Visualize correlation matrix\n",
    "fig, ax = stx.plt.subplots(figsize=(10, 8))\n",
    "im = ax.imshow(corr_matrix, cmap='RdBu_r', vmin=-1, vmax=1)\n",
    "ax.set_xticks(range(len(corr_matrix.columns)))\n",
    "ax.set_yticks(range(len(corr_matrix.columns)))\n",
    "ax.set_xticklabels(corr_matrix.columns, rotation=45)\n",
    "ax.set_yticklabels(corr_matrix.columns)\n",
    "\n",
    "# Add correlation values\n",
    "for i in range(len(corr_matrix)):\n",
    "    for j in range(len(corr_matrix)):\n",
    "        text = ax.text(j, i, f'{corr_matrix.iloc[i, j]:.2f}', \n",
    "                      ha='center', va='center', fontsize=12,\n",
    "                      color='white' if abs(corr_matrix.iloc[i, j]) > 0.5 else 'black')\n",
    "\n",
    "ax.set_xyt('Variables', 'Variables', 'Correlation Matrix')\n",
    "plt.colorbar(im, ax=ax, label='Correlation Coefficient')\n",
    "plt.tight_layout()\n",
    "stx.io.save(fig, output_dir / 'correlation_matrix.png')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Correlation analysis completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Statistical Testing and P-value Corrections\n",
    "\n",
    "### 2.1 Multiple Comparisons Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate experimental data for multiple comparisons\n",
    "n_groups = 5\n",
    "n_per_group = 20\n",
    "group_effects = [0, 0.5, 1.0, 0.3, 1.5]  # Effect sizes\n",
    "\n",
    "experimental_data = []\n",
    "group_labels = []\n",
    "\n",
    "for i, effect in enumerate(group_effects):\n",
    "    group_data = np.random.normal(effect, 1.0, n_per_group)\n",
    "    experimental_data.extend(group_data)\n",
    "    group_labels.extend([f'Group_{i+1}'] * n_per_group)\n",
    "\n",
    "# Create DataFrame\n",
    "exp_df = pd.DataFrame({\n",
    "    'value': experimental_data,\n",
    "    'group': group_labels\n",
    "})\n",
    "\n",
    "# Perform pairwise t-tests\n",
    "from itertools import combinations\n",
    "\n",
    "pairwise_results = []\n",
    "groups = exp_df['group'].unique()\n",
    "\n",
    "for group1, group2 in combinations(groups, 2):\n",
    "    data1 = exp_df[exp_df['group'] == group1]['value']\n",
    "    data2 = exp_df[exp_df['group'] == group2]['value']\n",
    "    \n",
    "    t_stat, p_val = stats.ttest_ind(data1, data2)\n",
    "    \n",
    "    pairwise_results.append({\n",
    "        'comparison': f'{group1} vs {group2}',\n",
    "        'group1': group1,\n",
    "        'group2': group2,\n",
    "        't_statistic': t_stat,\n",
    "        'p_value': p_val\n",
    "    })\n",
    "\n",
    "pairwise_df = pd.DataFrame(pairwise_results)\n",
    "print(\"Pairwise T-test Results (Uncorrected):\")\n",
    "print(pairwise_df[['comparison', 't_statistic', 'p_value']].round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply multiple comparison corrections\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "# Extract p-values\n",
    "p_values = pairwise_df['p_value'].values\n",
    "\n",
    "# Apply different correction methods\n",
    "corrections = {\n",
    "    'bonferroni': multipletests(p_values, method='bonferroni'),\n",
    "    'holm': multipletests(p_values, method='holm'),\n",
    "    'fdr_bh': multipletests(p_values, method='fdr_bh'),\n",
    "    'fdr_by': multipletests(p_values, method='fdr_by')\n",
    "}\n",
    "\n",
    "# Create summary DataFrame\n",
    "correction_summary = pairwise_df[['comparison', 'p_value']].copy()\n",
    "\n",
    "for method, (rejected, p_corrected, alpha_sidak, alpha_bonf) in corrections.items():\n",
    "    correction_summary[f'{method}_corrected'] = p_corrected\n",
    "    correction_summary[f'{method}_significant'] = rejected\n",
    "\n",
    "print(\"\\nMultiple Comparison Corrections:\")\n",
    "print(correction_summary.round(4))\n",
    "\n",
    "# Convert p-values to significance stars\n",
    "def p_to_stars(p):\n",
    "    \"\"\"Convert p-values to significance stars.\"\"\"\n",
    "    if p < 0.001:\n",
    "        return '***'\n",
    "    elif p < 0.01:\n",
    "        return '**'\n",
    "    elif p < 0.05:\n",
    "        return '*'\n",
    "    else:\n",
    "        return 'ns'\n",
    "\n",
    "# Add significance stars\n",
    "correction_summary['uncorrected_stars'] = correction_summary['p_value'].apply(p_to_stars)\n",
    "correction_summary['bonferroni_stars'] = correction_summary['bonferroni_corrected'].apply(p_to_stars)\n",
    "correction_summary['fdr_bh_stars'] = correction_summary['fdr_bh_corrected'].apply(p_to_stars)\n",
    "\n",
    "print(\"\\nSignificance Summary:\")\n",
    "print(correction_summary[['comparison', 'uncorrected_stars', 'bonferroni_stars', 'fdr_bh_stars']])\n",
    "\n",
    "print(\"\\n*** p < 0.001, ** p < 0.01, * p < 0.05, ns = not significant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Descriptive Statistics and Data Summary\n",
    "\n",
    "### 3.1 Enhanced Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive descriptive statistics\n",
    "import scipy.stats as scipy_stats\n",
    "\n",
    "def comprehensive_describe(data):\n",
    "    \"\"\"Comprehensive descriptive statistics.\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for column in data.select_dtypes(include=[np.number]).columns:\n",
    "        series = data[column].dropna()\n",
    "        \n",
    "        # Basic statistics\n",
    "        basic_stats = {\n",
    "            'count': len(series),\n",
    "            'mean': np.mean(series),\n",
    "            'std': np.std(series, ddof=1),\n",
    "            'min': np.min(series),\n",
    "            'q25': np.percentile(series, 25),\n",
    "            'median': np.median(series),\n",
    "            'q75': np.percentile(series, 75),\n",
    "            'max': np.max(series),\n",
    "        }\n",
    "        \n",
    "        # Additional statistics\n",
    "        additional_stats = {\n",
    "            'sem': scipy_stats.sem(series),\n",
    "            'skewness': scipy_stats.skew(series),\n",
    "            'kurtosis': scipy_stats.kurtosis(series),\n",
    "            'cv': np.std(series, ddof=1) / np.mean(series) if np.mean(series) != 0 else np.nan\n",
    "        }\n",
    "        \n",
    "        # Normality test\n",
    "        if len(series) >= 8:  # Minimum sample size for Shapiro-Wilk\n",
    "            shapiro_stat, shapiro_p = scipy_stats.shapiro(series)\n",
    "            additional_stats['shapiro_p'] = shapiro_p\n",
    "            additional_stats['is_normal'] = shapiro_p > 0.05\n",
    "        \n",
    "        results[column] = {**basic_stats, **additional_stats}\n",
    "    \n",
    "    return pd.DataFrame(results).T\n",
    "\n",
    "# Apply to experimental data\n",
    "group_stats = []\n",
    "for group in exp_df['group'].unique():\n",
    "    group_data = exp_df[exp_df['group'] == group]['value']\n",
    "    stats_dict = {\n",
    "        'group': group,\n",
    "        'n': len(group_data),\n",
    "        'mean': np.mean(group_data),\n",
    "        'std': np.std(group_data, ddof=1),\n",
    "        'sem': scipy_stats.sem(group_data),\n",
    "        'median': np.median(group_data),\n",
    "        'iqr': np.percentile(group_data, 75) - np.percentile(group_data, 25),\n",
    "        'skewness': scipy_stats.skew(group_data),\n",
    "        'kurtosis': scipy_stats.kurtosis(group_data)\n",
    "    }\n",
    "    group_stats.append(stats_dict)\n",
    "\n",
    "stats_df = pd.DataFrame(group_stats)\n",
    "print(\"Comprehensive Group Statistics:\")\n",
    "print(stats_df.round(3))\n",
    "\n",
    "# Visualize group comparisons\n",
    "fig, axes = stx.plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Box plots\n",
    "exp_df.boxplot(column='value', by='group', ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Group Comparisons (Box Plots)')\n",
    "axes[0, 0].set_xlabel('Group')\n",
    "axes[0, 0].set_ylabel('Value')\n",
    "\n",
    "# Mean with error bars\n",
    "x_pos = range(len(stats_df))\n",
    "axes[0, 1].bar(x_pos, stats_df['mean'], yerr=stats_df['sem'], \n",
    "               capsize=5, alpha=0.7, color='skyblue', edgecolor='navy')\n",
    "axes[0, 1].set_xticks(x_pos)\n",
    "axes[0, 1].set_xticklabels(stats_df['group'])\n",
    "axes[0, 1].set_title('Group Means Â± SEM')\n",
    "axes[0, 1].set_ylabel('Value')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Distribution shapes\n",
    "for i, group in enumerate(stats_df['group']):\n",
    "    group_data = exp_df[exp_df['group'] == group]['value']\n",
    "    axes[1, 0].hist(group_data, alpha=0.6, label=group, bins=10)\n",
    "axes[1, 0].set_title('Distribution Shapes')\n",
    "axes[1, 0].set_xlabel('Value')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# Effect sizes\n",
    "control_mean = stats_df.iloc[0]['mean']\n",
    "control_std = stats_df.iloc[0]['std']\n",
    "effect_sizes = [(row['mean'] - control_mean) / control_std for _, row in stats_df.iloc[1:].iterrows()]\n",
    "\n",
    "axes[1, 1].bar(range(len(effect_sizes)), effect_sizes, \n",
    "               alpha=0.7, color='lightcoral', edgecolor='darkred')\n",
    "axes[1, 1].axhline(y=0, color='black', linestyle='-', linewidth=1)\n",
    "axes[1, 1].axhline(y=0.2, color='gray', linestyle='--', alpha=0.7, label='Small')\n",
    "axes[1, 1].axhline(y=0.5, color='gray', linestyle='--', alpha=0.7, label='Medium')\n",
    "axes[1, 1].axhline(y=0.8, color='gray', linestyle='--', alpha=0.7, label='Large')\n",
    "axes[1, 1].set_xticks(range(len(effect_sizes)))\n",
    "axes[1, 1].set_xticklabels(stats_df['group'].iloc[1:])\n",
    "axes[1, 1].set_title('Effect Sizes vs Control')\n",
    "axes[1, 1].set_ylabel(\"Cohen's d\")\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "stx.io.save(fig, output_dir / 'statistical_summary.png')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Enhanced descriptive statistics completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Advanced Statistical Analysis\n",
    "\n",
    "### 4.1 ANOVA and Post-hoc Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform one-way ANOVA\n",
    "group_data = [exp_df[exp_df['group'] == group]['value'].values for group in groups]\n",
    "f_stat, p_anova = stats.f_oneway(*group_data)\n",
    "\n",
    "print(f\"One-way ANOVA Results:\")\n",
    "print(f\"F-statistic: {f_stat:.4f}\")\n",
    "print(f\"P-value: {p_anova:.6f}\")\n",
    "print(f\"Significant: {'Yes' if p_anova < 0.05 else 'No'}\")\n",
    "\n",
    "# Effect size (eta-squared)\n",
    "# SS_between / SS_total\n",
    "grand_mean = exp_df['value'].mean()\n",
    "ss_between = sum([len(group_data[i]) * (stats_df.iloc[i]['mean'] - grand_mean)**2 for i in range(len(group_data))])\n",
    "ss_total = sum([(x - grand_mean)**2 for x in exp_df['value']])\n",
    "eta_squared = ss_between / ss_total\n",
    "\n",
    "print(f\"\\nEffect Size:\")\n",
    "print(f\"Eta-squared (Î·Â²): {eta_squared:.4f}\")\n",
    "\n",
    "# Interpret effect size\n",
    "if eta_squared < 0.01:\n",
    "    effect_interpretation = \"Very small\"\n",
    "elif eta_squared < 0.06:\n",
    "    effect_interpretation = \"Small\"\n",
    "elif eta_squared < 0.14:\n",
    "    effect_interpretation = \"Medium\"\n",
    "else:\n",
    "    effect_interpretation = \"Large\"\n",
    "\n",
    "print(f\"Effect size interpretation: {effect_interpretation}\")\n",
    "\n",
    "# Create comprehensive statistical report\n",
    "statistical_report = {\n",
    "    'analysis_type': 'One-way ANOVA with post-hoc comparisons',\n",
    "    'sample_size': len(exp_df),\n",
    "    'n_groups': len(groups),\n",
    "    'anova_results': {\n",
    "        'f_statistic': f_stat,\n",
    "        'p_value': p_anova,\n",
    "        'significant': p_anova < 0.05,\n",
    "        'eta_squared': eta_squared,\n",
    "        'effect_size_interpretation': effect_interpretation\n",
    "    },\n",
    "    'group_statistics': stats_df.to_dict('records'),\n",
    "    'pairwise_comparisons': correction_summary.to_dict('records'),\n",
    "    'corrections_applied': list(corrections.keys())\n",
    "}\n",
    "\n",
    "# Save statistical report\n",
    "import json\n",
    "report_path = output_dir / 'statistical_analysis_report.json'\n",
    "with open(report_path, 'w') as f:\n",
    "    json.dump(statistical_report, f, indent=2, default=str)\n",
    "\n",
    "print(f\"\\nâœ… Statistical analysis report saved to: {report_path}\")\n",
    "print(f\"\\nðŸ“Š Analysis Summary:\")\n",
    "print(f\"  â€¢ ANOVA: F({len(groups)-1}, {len(exp_df)-len(groups)}) = {f_stat:.2f}, p = {p_anova:.4f}\")\n",
    "print(f\"  â€¢ Effect size: Î·Â² = {eta_squared:.3f} ({effect_interpretation})\")\n",
    "print(f\"  â€¢ Significant pairwise comparisons (Bonferroni): {sum(correction_summary['bonferroni_significant'])}\")\n",
    "print(f\"  â€¢ Significant pairwise comparisons (FDR): {sum(correction_summary['fdr_bh_significant'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This tutorial demonstrated the comprehensive statistical capabilities of the SciTeX stats module:\n",
    "\n",
    "### Key Features:\n",
    "1. **Correlation analysis** with multiple testing corrections\n",
    "2. **Statistical testing** with proper p-value adjustments\n",
    "3. **Descriptive statistics** with enhanced summaries\n",
    "4. **Effect size calculations** for practical significance\n",
    "5. **ANOVA and post-hoc testing** for group comparisons\n",
    "6. **Comprehensive reporting** with automated documentation\n",
    "\n",
    "### Best Practices:\n",
    "- Always apply multiple comparison corrections when testing multiple hypotheses\n",
    "- Report effect sizes alongside p-values for practical significance\n",
    "- Use appropriate statistical tests based on data distribution\n",
    "- Document analysis methods and assumptions\n",
    "- Provide comprehensive descriptive statistics\n",
    "\n",
    "The SciTeX stats module provides a robust foundation for scientific statistical analysis!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}