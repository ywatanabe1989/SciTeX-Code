{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SciTeX IO Module - Advanced Features\n",
    "\n",
    "This notebook demonstrates advanced features of the `scitex.io` module for unified file operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scitex as stx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import json\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Format Auto-Detection\n",
    "\n",
    "SciTeX automatically detects file formats based on extensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test data\n",
    "test_data = {\n",
    "    'numbers': [1, 2, 3, 4, 5],\n",
    "    'letters': ['a', 'b', 'c', 'd', 'e'],\n",
    "    'values': [10.1, 20.2, 30.3, 40.4, 50.5]\n",
    "}\n",
    "df = pd.DataFrame(test_data)\n",
    "\n",
    "# Save in different formats\n",
    "formats = {\n",
    "    'csv': './io_demo/data.csv',\n",
    "    'json': './io_demo/data.json',\n",
    "    'yaml': './io_demo/data.yaml',\n",
    "    'pickle': './io_demo/data.pkl',\n",
    "    'parquet': './io_demo/data.parquet',\n",
    "    'excel': './io_demo/data.xlsx'\n",
    "}\n",
    "\n",
    "for fmt, path in formats.items():\n",
    "    try:\n",
    "        stx.io.save(df, path)\n",
    "        print(f\"✓ Saved as {fmt}: {path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Failed to save {fmt}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and verify\n",
    "for fmt, path in formats.items():\n",
    "    if Path(path).exists():\n",
    "        try:\n",
    "            loaded = stx.io.load(path)\n",
    "            print(f\"\\n{fmt.upper()}:\")\n",
    "            if isinstance(loaded, pd.DataFrame):\n",
    "                print(f\"  Shape: {loaded.shape}\")\n",
    "                print(f\"  Columns: {list(loaded.columns)}\")\n",
    "            else:\n",
    "                print(f\"  Type: {type(loaded)}\")\n",
    "                print(f\"  Content: {str(loaded)[:50]}...\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Failed to load: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Numpy Array Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different array types\n",
    "arrays = {\n",
    "    '1D array': np.array([1, 2, 3, 4, 5]),\n",
    "    '2D array': np.random.randn(10, 5),\n",
    "    '3D array': np.random.randn(4, 5, 6),\n",
    "    'Complex array': np.array([1+2j, 3+4j, 5+6j]),\n",
    "    'Structured array': np.array([(1, 'a', 2.5), (2, 'b', 3.5)], \n",
    "                                dtype=[('x', 'i4'), ('y', 'U1'), ('z', 'f4')])\n",
    "}\n",
    "\n",
    "# Save arrays\n",
    "for name, arr in arrays.items():\n",
    "    filename = f\"./io_demo/{name.replace(' ', '_')}.npy\"\n",
    "    stx.io.save(arr, filename)\n",
    "    print(f\"Saved {name}: shape={arr.shape if hasattr(arr, 'shape') else 'N/A'}, dtype={arr.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save multiple arrays in one file\n",
    "stx.io.save({\n",
    "    'array1': arrays['1D array'],\n",
    "    'array2': arrays['2D array'],\n",
    "    'metadata': {'created': '2024-01-01', 'version': 1.0}\n",
    "}, './io_demo/multiple_arrays.npz')\n",
    "\n",
    "# Load and inspect\n",
    "loaded_arrays = stx.io.load('./io_demo/multiple_arrays.npz')\n",
    "print(\"Loaded arrays:\")\n",
    "for key in loaded_arrays.files:\n",
    "    arr = loaded_arrays[key]\n",
    "    print(f\"  {key}: shape={arr.shape if hasattr(arr, 'shape') else 'scalar'}, dtype={arr.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compressed Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create large dataset\n",
    "large_data = np.random.randn(1000, 1000)\n",
    "large_df = pd.DataFrame(large_data)\n",
    "\n",
    "# Save with different compression methods\n",
    "compression_tests = [\n",
    "    ('./io_demo/large_uncompressed.npy', None),\n",
    "    ('./io_demo/large_compressed.npy.gz', 'gzip'),\n",
    "    ('./io_demo/large_compressed.csv.gz', 'gzip'),\n",
    "    ('./io_demo/large_compressed.pkl.bz2', 'bz2'),\n",
    "]\n",
    "\n",
    "for path, compression in compression_tests:\n",
    "    if path.endswith('.npy') or path.endswith('.npy.gz'):\n",
    "        data_to_save = large_data\n",
    "    else:\n",
    "        data_to_save = large_df\n",
    "    \n",
    "    stx.io.save(data_to_save, path)\n",
    "    size_mb = Path(path).stat().st_size / 1024 / 1024\n",
    "    print(f\"{path}: {size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare compression ratios\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sizes = []\n",
    "labels = []\n",
    "\n",
    "for path, _ in compression_tests:\n",
    "    if Path(path).exists():\n",
    "        size_mb = Path(path).stat().st_size / 1024 / 1024\n",
    "        sizes.append(size_mb)\n",
    "        labels.append(Path(path).name)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "bars = ax.bar(range(len(sizes)), sizes)\n",
    "ax.set_xticks(range(len(labels)))\n",
    "ax.set_xticklabels(labels, rotation=45, ha='right')\n",
    "ax.set_ylabel('File Size (MB)')\n",
    "ax.set_title('Compression Comparison')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, size in zip(bars, sizes):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, \n",
    "            f'{size:.1f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "stx.io.save(fig, './io_demo/compression_comparison.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Working with HDF5 Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create hierarchical data structure\n",
    "hdf5_data = {\n",
    "    'experiment1': {\n",
    "        'data': np.random.randn(100, 50),\n",
    "        'metadata': {\n",
    "            'date': '2024-01-01',\n",
    "            'parameters': {'alpha': 0.1, 'beta': 0.2}\n",
    "        }\n",
    "    },\n",
    "    'experiment2': {\n",
    "        'data': np.random.randn(200, 50),\n",
    "        'metadata': {\n",
    "            'date': '2024-01-02',\n",
    "            'parameters': {'alpha': 0.2, 'beta': 0.3}\n",
    "        }\n",
    "    },\n",
    "    'summary': pd.DataFrame({\n",
    "        'exp_id': [1, 2],\n",
    "        'mean_value': [0.05, -0.02],\n",
    "        'std_value': [0.98, 1.01]\n",
    "    })\n",
    "}\n",
    "\n",
    "# Save to HDF5\n",
    "stx.io.save(hdf5_data, './io_demo/experiments.h5')\n",
    "print(\"Saved hierarchical data to HDF5\")\n",
    "\n",
    "# Load and explore\n",
    "loaded_h5 = stx.io.load('./io_demo/experiments.h5')\n",
    "print(\"\\nHDF5 structure:\")\n",
    "for key in loaded_h5:\n",
    "    print(f\"  /{key}\")\n",
    "    if isinstance(loaded_h5[key], dict):\n",
    "        for subkey in loaded_h5[key]:\n",
    "            print(f\"    /{key}/{subkey}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Configuration Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex configuration\n",
    "config = {\n",
    "    'project': {\n",
    "        'name': 'SciTeX Demo',\n",
    "        'version': '1.0.0',\n",
    "        'authors': ['Alice', 'Bob'],\n",
    "        'created': '2024-01-01'\n",
    "    },\n",
    "    'experiment': {\n",
    "        'parameters': {\n",
    "            'learning_rate': 0.001,\n",
    "            'batch_size': 32,\n",
    "            'epochs': 100,\n",
    "            'optimizer': {\n",
    "                'type': 'adam',\n",
    "                'betas': [0.9, 0.999]\n",
    "            }\n",
    "        },\n",
    "        'data': {\n",
    "            'train_size': 0.8,\n",
    "            'validation_size': 0.1,\n",
    "            'test_size': 0.1,\n",
    "            'random_seed': 42\n",
    "        }\n",
    "    },\n",
    "    'paths': {\n",
    "        'data': './data/',\n",
    "        'models': './models/',\n",
    "        'results': './results/',\n",
    "        'figures': './figures/'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save as YAML (human-readable)\n",
    "stx.io.save(config, './io_demo/config.yaml')\n",
    "\n",
    "# Save as JSON (programmatic)\n",
    "stx.io.save(config, './io_demo/config.json')\n",
    "\n",
    "# Display YAML content\n",
    "print(\"YAML format:\")\n",
    "with open('./io_demo/config.yaml', 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and access nested configuration\n",
    "loaded_config = stx.io.load('./io_demo/config.yaml')\n",
    "\n",
    "print(f\"Project: {loaded_config['project']['name']} v{loaded_config['project']['version']}\")\n",
    "print(f\"Learning rate: {loaded_config['experiment']['parameters']['learning_rate']}\")\n",
    "print(f\"Optimizer: {loaded_config['experiment']['parameters']['optimizer']['type']}\")\n",
    "print(f\"\\nPaths:\")\n",
    "for key, path in loaded_config['paths'].items():\n",
    "    print(f\"  {key}: {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Symlinks and Output Organization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some results\n",
    "results = {\n",
    "    'accuracy': 0.95,\n",
    "    'loss': 0.05,\n",
    "    'confusion_matrix': [[95, 5], [3, 97]]\n",
    "}\n",
    "\n",
    "# Save with organized structure and symlink\n",
    "timestamp = pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')\n",
    "result_path = f'./io_demo/results/experiment_{timestamp}/metrics.json'\n",
    "\n",
    "stx.io.save(results, result_path, symlink_from_cwd=True)\n",
    "\n",
    "# Check if symlink was created\n",
    "symlink_path = Path('metrics.json')\n",
    "if symlink_path.exists() and symlink_path.is_symlink():\n",
    "    print(f\"✓ Symlink created: {symlink_path} -> {symlink_path.resolve()}\")\n",
    "else:\n",
    "    print(\"✗ No symlink found\")\n",
    "\n",
    "# List all results\n",
    "results_dir = Path('./io_demo/results')\n",
    "if results_dir.exists():\n",
    "    print(\"\\nAll results:\")\n",
    "    for exp_dir in sorted(results_dir.iterdir()):\n",
    "        print(f\"  {exp_dir.name}/\")\n",
    "        for file in exp_dir.iterdir():\n",
    "            print(f\"    {file.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Custom Save/Load Handlers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom class\n",
    "class ExperimentResult:\n",
    "    def __init__(self, name, data, metadata):\n",
    "        self.name = name\n",
    "        self.data = data\n",
    "        self.metadata = metadata\n",
    "        self.timestamp = pd.Timestamp.now()\n",
    "    \n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            'name': self.name,\n",
    "            'data': self.data.tolist() if isinstance(self.data, np.ndarray) else self.data,\n",
    "            'metadata': self.metadata,\n",
    "            'timestamp': self.timestamp.isoformat()\n",
    "        }\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dict(cls, d):\n",
    "        obj = cls(d['name'], np.array(d['data']), d['metadata'])\n",
    "        obj.timestamp = pd.Timestamp(d['timestamp'])\n",
    "        return obj\n",
    "\n",
    "# Create instance\n",
    "exp_result = ExperimentResult(\n",
    "    name='Test Experiment',\n",
    "    data=np.random.randn(10, 5),\n",
    "    metadata={'version': 1, 'author': 'SciTeX User'}\n",
    ")\n",
    "\n",
    "# Save as JSON (using to_dict)\n",
    "stx.io.save(exp_result.to_dict(), './io_demo/experiment_result.json')\n",
    "\n",
    "# Load and reconstruct\n",
    "loaded_dict = stx.io.load('./io_demo/experiment_result.json')\n",
    "reconstructed = ExperimentResult.from_dict(loaded_dict)\n",
    "\n",
    "print(f\"Original: {exp_result.name}, shape: {exp_result.data.shape}\")\n",
    "print(f\"Loaded: {reconstructed.name}, shape: {reconstructed.data.shape}\")\n",
    "print(f\"Timestamp preserved: {exp_result.timestamp == reconstructed.timestamp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Batch Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate multiple datasets\n",
    "datasets = {}\n",
    "for i in range(5):\n",
    "    datasets[f'dataset_{i}'] = pd.DataFrame({\n",
    "        'x': np.random.randn(100),\n",
    "        'y': np.random.randn(100),\n",
    "        'group': np.random.choice(['A', 'B', 'C'], 100)\n",
    "    })\n",
    "\n",
    "# Save all datasets\n",
    "for name, df in datasets.items():\n",
    "    stx.io.save(df, f'./io_demo/batch/{name}.csv')\n",
    "\n",
    "# Load all CSVs from directory\n",
    "batch_dir = Path('./io_demo/batch')\n",
    "loaded_datasets = {}\n",
    "\n",
    "for csv_file in batch_dir.glob('*.csv'):\n",
    "    name = csv_file.stem\n",
    "    loaded_datasets[name] = stx.io.load(csv_file)\n",
    "\n",
    "print(f\"Loaded {len(loaded_datasets)} datasets:\")\n",
    "for name, df in loaded_datasets.items():\n",
    "    print(f\"  {name}: shape={df.shape}, columns={list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Error Handling and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test various edge cases\n",
    "test_cases = [\n",
    "    ('Empty DataFrame', pd.DataFrame()),\n",
    "    ('Single value', 42),\n",
    "    ('List of mixed types', [1, 'two', 3.0, None]),\n",
    "    ('Nested structure', {'a': {'b': {'c': [1, 2, 3]}}}) \n",
    "]\n",
    "\n",
    "for name, data in test_cases:\n",
    "    print(f\"\\nTesting: {name}\")\n",
    "    \n",
    "    # Try different formats\n",
    "    for ext in ['.json', '.yaml', '.pkl']:\n",
    "        path = f'./io_demo/edge_cases/{name.replace(\" \", \"_\")}{ext}'\n",
    "        try:\n",
    "            stx.io.save(data, path)\n",
    "            loaded = stx.io.load(path)\n",
    "            \n",
    "            # Verify\n",
    "            if isinstance(data, pd.DataFrame):\n",
    "                success = data.equals(loaded)\n",
    "            elif isinstance(data, (list, dict)):\n",
    "                success = data == loaded\n",
    "            else:\n",
    "                success = data == loaded\n",
    "                \n",
    "            print(f\"  {ext}: {'✓' if success else '✗'}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  {ext}: ✗ ({type(e).__name__})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Create test data\n",
    "large_df = pd.DataFrame(np.random.randn(10000, 100))\n",
    "\n",
    "# Test different formats\n",
    "formats_to_test = {\n",
    "    'CSV': '.csv',\n",
    "    'Pickle': '.pkl',\n",
    "    'Parquet': '.parquet',\n",
    "    'HDF5': '.h5'\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, ext in formats_to_test.items():\n",
    "    path = f'./io_demo/perf_test{ext}'\n",
    "    \n",
    "    # Time save\n",
    "    start = time.time()\n",
    "    try:\n",
    "        stx.io.save(large_df, path)\n",
    "        save_time = time.time() - start\n",
    "        \n",
    "        # Time load\n",
    "        start = time.time()\n",
    "        loaded = stx.io.load(path)\n",
    "        load_time = time.time() - start\n",
    "        \n",
    "        # File size\n",
    "        size_mb = Path(path).stat().st_size / 1024 / 1024\n",
    "        \n",
    "        results.append({\n",
    "            'Format': name,\n",
    "            'Save Time (s)': round(save_time, 3),\n",
    "            'Load Time (s)': round(load_time, 3),\n",
    "            'File Size (MB)': round(size_mb, 2)\n",
    "        })\n",
    "    except Exception as e:\n",
    "        results.append({\n",
    "            'Format': name,\n",
    "            'Save Time (s)': 'Error',\n",
    "            'Load Time (s)': 'Error',\n",
    "            'File Size (MB)': 'Error'\n",
    "        })\n",
    "\n",
    "# Display results\n",
    "perf_df = pd.DataFrame(results)\n",
    "print(\"\\nPerformance Comparison:\")\n",
    "print(perf_df.to_string(index=False))\n",
    "\n",
    "# Visualize\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Only plot numeric values\n",
    "valid_results = [r for r in results if r['Save Time (s)'] != 'Error']\n",
    "formats = [r['Format'] for r in valid_results]\n",
    "\n",
    "ax1.bar(formats, [r['Save Time (s)'] for r in valid_results])\n",
    "ax1.set_title('Save Time')\n",
    "ax1.set_ylabel('Time (seconds)')\n",
    "\n",
    "ax2.bar(formats, [r['Load Time (s)'] for r in valid_results])\n",
    "ax2.set_title('Load Time')\n",
    "ax2.set_ylabel('Time (seconds)')\n",
    "\n",
    "ax3.bar(formats, [r['File Size (MB)'] for r in valid_results])\n",
    "ax3.set_title('File Size')\n",
    "ax3.set_ylabel('Size (MB)')\n",
    "\n",
    "plt.tight_layout()\n",
    "stx.io.save(fig, './io_demo/format_performance_comparison.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The `scitex.io` module provides:\n",
    "\n",
    "1. **Unified Interface**: Single `save()` and `load()` function for all formats\n",
    "2. **Format Auto-detection**: Based on file extensions\n",
    "3. **Compression Support**: Automatic handling of .gz, .bz2, etc.\n",
    "4. **Hierarchical Data**: Support for HDF5 and nested structures\n",
    "5. **Configuration Files**: YAML and JSON for settings\n",
    "6. **Symlink Creation**: For easy access to latest results\n",
    "7. **Batch Operations**: Process multiple files efficiently\n",
    "8. **Robust Error Handling**: Graceful handling of edge cases\n",
    "\n",
    "### Best Practices:\n",
    "\n",
    "- Use **relative paths** starting with `./`\n",
    "- **Organize outputs** by type (data/, results/, figures/)\n",
    "- Choose **appropriate formats**:\n",
    "  - CSV for data sharing\n",
    "  - Pickle for Python objects\n",
    "  - Parquet for large DataFrames\n",
    "  - HDF5 for hierarchical data\n",
    "  - YAML for human-readable configs\n",
    "- Use **compression** for large files\n",
    "- Create **symlinks** for important outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "import shutil\n",
    "\n",
    "# Remove demo directory\n",
    "if Path('./io_demo').exists():\n",
    "    # shutil.rmtree('./io_demo')  # Uncomment to clean up\n",
    "    print(\"Demo files kept in ./io_demo/\")\n",
    "\n",
    "# Remove symlink\n",
    "if Path('metrics.json').is_symlink():\n",
    "    # Path('metrics.json').unlink()  # Uncomment to remove\n",
    "    print(\"Symlink kept: metrics.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}