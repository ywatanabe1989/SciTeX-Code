{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SciTeX Pandas Utilities Tutorial\n",
    "\n",
    "This notebook demonstrates the pandas utilities in SciTeX, providing powerful extensions for DataFrame manipulation, analysis, and transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scitex as stx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Universal DataFrame Conversion with force_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Converting Various Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert different data types to DataFrame\n",
    "print(\"=== Converting various data types to DataFrame ===\")\n",
    "\n",
    "# Scalar value\n",
    "scalar = 42\n",
    "df_scalar = stx.pd.force_df(scalar)\n",
    "print(\"\\nScalar to DataFrame:\")\n",
    "print(df_scalar)\n",
    "\n",
    "# List\n",
    "list_data = [1, 2, 3, 4, 5]\n",
    "df_list = stx.pd.force_df(list_data)\n",
    "print(\"\\nList to DataFrame:\")\n",
    "print(df_list)\n",
    "\n",
    "# NumPy array\n",
    "array_1d = np.array([10, 20, 30, 40])\n",
    "df_array_1d = stx.pd.force_df(array_1d)\n",
    "print(\"\\n1D Array to DataFrame:\")\n",
    "print(df_array_1d)\n",
    "\n",
    "# 2D NumPy array\n",
    "array_2d = np.random.randn(3, 4)\n",
    "df_array_2d = stx.pd.force_df(array_2d)\n",
    "print(\"\\n2D Array to DataFrame:\")\n",
    "print(df_array_2d)\n",
    "\n",
    "# Dictionary with unequal lengths\n",
    "dict_unequal = {\n",
    "    'A': [1, 2, 3],\n",
    "    'B': [4, 5],\n",
    "    'C': [6, 7, 8, 9]\n",
    "}\n",
    "df_dict = stx.pd.force_df(dict_unequal)\n",
    "print(\"\\nDictionary with unequal lengths to DataFrame:\")\n",
    "print(df_dict)\n",
    "\n",
    "# Series\n",
    "series = pd.Series([100, 200, 300], index=['x', 'y', 'z'])\n",
    "df_series = stx.pd.force_df(series)\n",
    "print(\"\\nSeries to DataFrame:\")\n",
    "print(df_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Custom Filler Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using custom filler for unequal lengths\n",
    "data_unequal = {\n",
    "    'experiment_1': [23.5, 24.1, 23.8],\n",
    "    'experiment_2': [22.9, 23.5, 24.0, 23.7, 23.9],\n",
    "    'experiment_3': [24.2, 23.6]\n",
    "}\n",
    "\n",
    "# Default filler (NaN)\n",
    "df_nan = stx.pd.force_df(data_unequal)\n",
    "print(\"With NaN filler:\")\n",
    "print(df_nan)\n",
    "\n",
    "# Custom filler (0)\n",
    "df_zero = stx.pd.force_df(data_unequal, filler=0)\n",
    "print(\"\\nWith 0 filler:\")\n",
    "print(df_zero)\n",
    "\n",
    "# Custom filler (mean value)\n",
    "all_values = [v for vals in data_unequal.values() for v in vals]\n",
    "mean_value = np.mean(all_values)\n",
    "df_mean = stx.pd.force_df(data_unequal, filler=mean_value)\n",
    "print(f\"\\nWith mean filler ({mean_value:.2f}):\")\n",
    "print(df_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Finding P-value Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with statistical results\n",
    "stats_data = {\n",
    "    'feature': ['height', 'weight', 'age', 'blood_pressure'],\n",
    "    'mean_control': [170.5, 68.2, 35.4, 120.5],\n",
    "    'mean_treatment': [172.1, 66.8, 35.6, 118.2],\n",
    "    'p_value': [0.023, 0.156, 0.832, 0.041],\n",
    "    'pval_adjusted': [0.092, 0.312, 0.832, 0.123],\n",
    "    'significance': ['*', 'ns', 'ns', '*'],\n",
    "    'p-val-bonferroni': [0.092, 0.624, 1.000, 0.164]\n",
    "}\n",
    "\n",
    "df_stats = pd.DataFrame(stats_data)\n",
    "print(\"Statistical results DataFrame:\")\n",
    "print(df_stats)\n",
    "\n",
    "# Find all p-value columns\n",
    "pval_cols = stx.pd.find_pval(df_stats, multiple=True)\n",
    "print(f\"\\nP-value columns found: {pval_cols}\")\n",
    "\n",
    "# Extract p-values\n",
    "print(\"\\nP-values:\")\n",
    "for col in pval_cols:\n",
    "    print(f\"{col}: {df_stats[col].tolist()}\")\n",
    "\n",
    "# Find first p-value column only\n",
    "first_pval = stx.pd.find_pval(df_stats, multiple=False)\n",
    "print(f\"\\nFirst p-value column: {first_pval}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Column and Row Movement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Moving Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'A': [1, 2, 3],\n",
    "    'B': [4, 5, 6],\n",
    "    'C': [7, 8, 9],\n",
    "    'D': [10, 11, 12],\n",
    "    'E': [13, 14, 15]\n",
    "})\n",
    "\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "\n",
    "# Move column 'D' to position 1\n",
    "df_moved = stx.pd.mv(df, 'D', 1)\n",
    "print(\"\\nAfter moving 'D' to position 1:\")\n",
    "print(df_moved)\n",
    "print(f\"Columns: {df_moved.columns.tolist()}\")\n",
    "\n",
    "# Move column 'C' to first\n",
    "df_first = stx.pd.mv_to_first(df, 'C')\n",
    "print(\"\\nAfter moving 'C' to first:\")\n",
    "print(df_first)\n",
    "print(f\"Columns: {df_first.columns.tolist()}\")\n",
    "\n",
    "# Move column 'B' to last\n",
    "df_last = stx.pd.mv_to_last(df, 'B')\n",
    "print(\"\\nAfter moving 'B' to last:\")\n",
    "print(df_last)\n",
    "print(f\"Columns: {df_last.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Moving Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame with named index\n",
    "df_rows = pd.DataFrame({\n",
    "    'value': [10, 20, 30, 40, 50],\n",
    "    'category': ['A', 'B', 'C', 'D', 'E']\n",
    "}, index=['row1', 'row2', 'row3', 'row4', 'row5'])\n",
    "\n",
    "print(\"Original DataFrame:\")\n",
    "print(df_rows)\n",
    "\n",
    "# Move row3 to first position\n",
    "df_row_first = stx.pd.mv_to_first(df_rows, 'row3', axis=0)\n",
    "print(\"\\nAfter moving 'row3' to first:\")\n",
    "print(df_row_first)\n",
    "\n",
    "# Move row2 to last position\n",
    "df_row_last = stx.pd.mv_to_last(df_rows, 'row2', axis=0)\n",
    "print(\"\\nAfter moving 'row2' to last:\")\n",
    "print(df_row_last)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Melting Specific Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create wide format data\n",
    "wide_data = pd.DataFrame({\n",
    "    'subject_id': ['S001', 'S002', 'S003'],\n",
    "    'age': [25, 30, 35],\n",
    "    'gender': ['M', 'F', 'M'],\n",
    "    'time_1': [120, 115, 125],\n",
    "    'time_2': [118, 112, 123],\n",
    "    'time_3': [115, 110, 120]\n",
    "})\n",
    "\n",
    "print(\"Wide format data:\")\n",
    "print(wide_data)\n",
    "\n",
    "# Melt only the time columns\n",
    "time_cols = ['time_1', 'time_2', 'time_3']\n",
    "melted = stx.pd.melt_cols(wide_data, time_cols)\n",
    "\n",
    "print(\"\\nAfter melting time columns:\")\n",
    "print(melted)\n",
    "\n",
    "# Melt with specific id columns\n",
    "melted_ids = stx.pd.melt_cols(wide_data, time_cols, id_columns=['subject_id', 'age'])\n",
    "print(\"\\nWith specific ID columns:\")\n",
    "print(melted_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Matrix to Long Format Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create correlation matrix\n",
    "np.random.seed(42)\n",
    "n_vars = 5\n",
    "data = np.random.randn(100, n_vars)\n",
    "df_data = pd.DataFrame(data, columns=[f'var_{i+1}' for i in range(n_vars)])\n",
    "corr_matrix = df_data.corr()\n",
    "\n",
    "print(\"Correlation matrix:\")\n",
    "print(corr_matrix.round(3))\n",
    "\n",
    "# Convert to x, y, z format\n",
    "xyz_data = stx.pd.to_xyz(corr_matrix)\n",
    "print(\"\\nLong format (x, y, z):\")\n",
    "print(xyz_data.head(10))\n",
    "\n",
    "# Visualize using the long format\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Heatmap from matrix\n",
    "sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "            center=0, ax=ax1, square=True)\n",
    "ax1.set_title('Correlation Matrix (Wide Format)')\n",
    "\n",
    "# Scatter plot from long format\n",
    "scatter = ax2.scatter(xyz_data['x'], xyz_data['y'], \n",
    "                     c=xyz_data['z'], cmap='coolwarm', \n",
    "                     s=200, vmin=-1, vmax=1)\n",
    "ax2.set_xlabel('Variable 1')\n",
    "ax2.set_ylabel('Variable 2')\n",
    "ax2.set_title('Correlation Values (Long Format)')\n",
    "ax2.set_xticks(range(len(corr_matrix.columns)))\n",
    "ax2.set_xticklabels(corr_matrix.columns, rotation=45)\n",
    "ax2.set_yticks(range(len(corr_matrix.index)))\n",
    "ax2.set_yticklabels(corr_matrix.index)\n",
    "ax2.invert_yaxis()\n",
    "plt.colorbar(scatter, ax=ax2, label='Correlation')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Advanced DataFrame Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Safe Numeric Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame with mixed types\n",
    "mixed_data = pd.DataFrame({\n",
    "    'A': ['1', '2', '3', '4'],\n",
    "    'B': ['1.5', '2.5', 'invalid', '4.5'],\n",
    "    'C': [1, 2, 3, 4],\n",
    "    'D': ['10%', '20%', '30%', '40%']\n",
    "})\n",
    "\n",
    "print(\"Original DataFrame:\")\n",
    "print(mixed_data)\n",
    "print(\"\\nData types:\")\n",
    "print(mixed_data.dtypes)\n",
    "\n",
    "# Convert to numeric\n",
    "numeric_df = stx.pd.to_numeric(mixed_data)\n",
    "print(\"\\nAfter numeric conversion:\")\n",
    "print(numeric_df)\n",
    "print(\"\\nData types after conversion:\")\n",
    "print(numeric_df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Column Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame with columns to merge\n",
    "df_merge = pd.DataFrame({\n",
    "    'first_name': ['John', 'Jane', 'Bob'],\n",
    "    'last_name': ['Doe', 'Smith', 'Johnson'],\n",
    "    'age': [30, 25, 35],\n",
    "    'city': ['New York', 'London', 'Paris'],\n",
    "    'country': ['USA', 'UK', 'France']\n",
    "})\n",
    "\n",
    "print(\"Original DataFrame:\")\n",
    "print(df_merge)\n",
    "\n",
    "# Merge name columns\n",
    "df_merged_name = stx.pd.merge_columns(\n",
    "    df_merge, \n",
    "    ['first_name', 'last_name'], \n",
    "    'full_name',\n",
    "    separator=' '\n",
    ")\n",
    "print(\"\\nAfter merging name columns:\")\n",
    "print(df_merged_name)\n",
    "\n",
    "# Merge location columns\n",
    "df_merged_all = stx.pd.merge_columns(\n",
    "    df_merged_name,\n",
    "    ['city', 'country'],\n",
    "    'location',\n",
    "    separator=', '\n",
    ")\n",
    "print(\"\\nAfter merging location columns:\")\n",
    "print(df_merged_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Advanced Slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create time series data\n",
    "dates = pd.date_range('2024-01-01', periods=100, freq='D')\n",
    "ts_data = pd.DataFrame({\n",
    "    'date': dates,\n",
    "    'value': np.cumsum(np.random.randn(100)),\n",
    "    'category': np.random.choice(['A', 'B', 'C'], 100)\n",
    "})\n",
    "ts_data.set_index('date', inplace=True)\n",
    "\n",
    "print(\"Time series data:\")\n",
    "print(ts_data.head())\n",
    "\n",
    "# Slice by date range\n",
    "start_date = '2024-02-01'\n",
    "end_date = '2024-02-15'\n",
    "sliced = stx.pd.slice(ts_data, start=start_date, end=end_date)\n",
    "print(f\"\\nSliced data ({start_date} to {end_date}):\")\n",
    "print(sliced)\n",
    "\n",
    "# Slice by row numbers\n",
    "sliced_rows = stx.pd.slice(ts_data.reset_index(), start=10, end=20)\n",
    "print(\"\\nSliced by row numbers (10-20):\")\n",
    "print(sliced_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Handling SettingWithCopyWarning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame that typically causes warning\n",
    "df_original = pd.DataFrame({\n",
    "    'A': [1, 2, 3, 4, 5],\n",
    "    'B': [10, 20, 30, 40, 50]\n",
    "})\n",
    "\n",
    "# This would normally trigger a warning\n",
    "df_subset = df_original[df_original['A'] > 2]\n",
    "\n",
    "# Without warning suppression (would show warning)\n",
    "print(\"Without warning suppression:\")\n",
    "try:\n",
    "    df_subset['C'] = df_subset['A'] * df_subset['B']\n",
    "    print(\"Operation completed\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "# With warning suppression\n",
    "print(\"\\nWith warning suppression:\")\n",
    "with stx.pd.ignore_SettingWithCopyWarning():\n",
    "    df_subset['C'] = df_subset['A'] * df_subset['B']\n",
    "    print(\"Operation completed silently\")\n",
    "\n",
    "print(\"\\nResult:\")\n",
    "print(df_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Practical Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1 Statistical Analysis Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate experimental data\n",
    "np.random.seed(42)\n",
    "n_samples = 50\n",
    "\n",
    "# Control and treatment groups\n",
    "control = np.random.normal(100, 15, n_samples)\n",
    "treatment = np.random.normal(110, 15, n_samples)\n",
    "\n",
    "# Create results dictionary with unequal lengths (some missing data)\n",
    "results = {\n",
    "    'control': control[:45],  # Some missing data\n",
    "    'treatment': treatment,\n",
    "    'placebo': np.random.normal(102, 15, 40)  # Different size\n",
    "}\n",
    "\n",
    "# Convert to DataFrame with force_df\n",
    "df_results = stx.pd.force_df(results)\n",
    "print(\"Experimental results:\")\n",
    "print(df_results.head())\n",
    "print(f\"\\nShape: {df_results.shape}\")\n",
    "\n",
    "# Perform statistical tests\n",
    "from scipy import stats\n",
    "\n",
    "# Compare groups\n",
    "comparisons = []\n",
    "groups = list(results.keys())\n",
    "\n",
    "for i in range(len(groups)):\n",
    "    for j in range(i+1, len(groups)):\n",
    "        group1, group2 = groups[i], groups[j]\n",
    "        data1 = df_results[group1].dropna()\n",
    "        data2 = df_results[group2].dropna()\n",
    "        \n",
    "        t_stat, p_val = stats.ttest_ind(data1, data2)\n",
    "        \n",
    "        comparisons.append({\n",
    "            'comparison': f'{group1}_vs_{group2}',\n",
    "            'mean_diff': data1.mean() - data2.mean(),\n",
    "            't_statistic': t_stat,\n",
    "            'p_value': p_val,\n",
    "            'p_val_bonferroni': p_val * len(groups)  # Simple Bonferroni\n",
    "        })\n",
    "\n",
    "df_stats = pd.DataFrame(comparisons)\n",
    "print(\"\\nStatistical comparisons:\")\n",
    "print(df_stats)\n",
    "\n",
    "# Find p-value columns\n",
    "pval_cols = stx.pd.find_pval(df_stats)\n",
    "print(f\"\\nP-value columns: {pval_cols}\")\n",
    "\n",
    "# Move p-value to first\n",
    "df_stats_reordered = stx.pd.mv_to_first(df_stats, 'p_value')\n",
    "print(\"\\nReordered (p-value first):\")\n",
    "print(df_stats_reordered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 Data Reshaping for Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multi-condition experiment data\n",
    "subjects = [f'S{i:03d}' for i in range(1, 11)]\n",
    "conditions = ['baseline', 'stress', 'recovery']\n",
    "measures = ['heart_rate', 'blood_pressure', 'cortisol']\n",
    "\n",
    "# Generate data\n",
    "data_records = []\n",
    "for subject in subjects:\n",
    "    for condition in conditions:\n",
    "        record = {'subject': subject, 'condition': condition}\n",
    "        for measure in measures:\n",
    "            base_value = {'heart_rate': 70, 'blood_pressure': 120, 'cortisol': 10}[measure]\n",
    "            multiplier = {'baseline': 1.0, 'stress': 1.3, 'recovery': 1.1}[condition]\n",
    "            value = base_value * multiplier + np.random.normal(0, 5)\n",
    "            record[measure] = value\n",
    "        data_records.append(record)\n",
    "\n",
    "df_experiment = pd.DataFrame(data_records)\n",
    "print(\"Experiment data (wide format):\")\n",
    "print(df_experiment.head())\n",
    "\n",
    "# Melt measures for plotting\n",
    "df_melted = stx.pd.melt_cols(df_experiment, measures, \n",
    "                             id_columns=['subject', 'condition'])\n",
    "print(\"\\nMelted data (long format):\")\n",
    "print(df_melted.head(10))\n",
    "\n",
    "# Create visualizations\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "for i, measure in enumerate(measures):\n",
    "    measure_data = df_melted[df_melted['variable'] == measure]\n",
    "    \n",
    "    # Box plot by condition\n",
    "    measure_data.boxplot(column='value', by='condition', ax=axes[i])\n",
    "    axes[i].set_title(measure.replace('_', ' ').title())\n",
    "    axes[i].set_xlabel('Condition')\n",
    "    axes[i].set_ylabel('Value')\n",
    "    axes[i].get_figure().suptitle('')  # Remove automatic title\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create pivot table for heatmap\n",
    "pivot = df_experiment.pivot_table(\n",
    "    values=measures,\n",
    "    index='subject',\n",
    "    columns='condition',\n",
    "    aggfunc='mean'\n",
    ")\n",
    "\n",
    "# Convert to xyz format for custom visualization\n",
    "xyz_pivot = stx.pd.to_xyz(pivot['heart_rate'])\n",
    "print(\"\\nPivot data in xyz format:\")\n",
    "print(xyz_pivot.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Building a Complete Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor:\n",
    "    \"\"\"Complete data processing pipeline using SciTeX pandas utilities.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.data = None\n",
    "        self.processed = None\n",
    "    \n",
    "    def load_data(self, data_source):\n",
    "        \"\"\"Load data from any source using force_df.\"\"\"\n",
    "        self.data = stx.pd.force_df(data_source)\n",
    "        print(f\"Loaded data with shape: {self.data.shape}\")\n",
    "        return self\n",
    "    \n",
    "    def clean_numeric(self):\n",
    "        \"\"\"Convert all possible columns to numeric.\"\"\"\n",
    "        self.data = stx.pd.to_numeric(self.data)\n",
    "        return self\n",
    "    \n",
    "    def find_statistics(self):\n",
    "        \"\"\"Find and highlight statistical columns.\"\"\"\n",
    "        pval_cols = stx.pd.find_pval(self.data, multiple=True)\n",
    "        \n",
    "        if pval_cols:\n",
    "            print(f\"Found p-value columns: {pval_cols}\")\n",
    "            # Move first p-value column to front\n",
    "            self.data = stx.pd.mv_to_first(self.data, pval_cols[0])\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def reshape_for_analysis(self, value_vars, id_vars=None):\n",
    "        \"\"\"Reshape data for analysis.\"\"\"\n",
    "        self.processed = stx.pd.melt_cols(self.data, value_vars, id_vars)\n",
    "        return self\n",
    "    \n",
    "    def get_result(self):\n",
    "        \"\"\"Return processed data.\"\"\"\n",
    "        return self.processed if self.processed is not None else self.data\n",
    "\n",
    "# Example usage\n",
    "# Create complex data\n",
    "complex_data = {\n",
    "    'patient_id': ['P001', 'P002', 'P003', 'P004'],\n",
    "    'age': ['45', '52', '38', '61'],\n",
    "    'baseline_score': [85, 92, 78, 88],\n",
    "    'week_1_score': [88, 95, 82, 90],\n",
    "    'week_2_score': [91, 98, 85],  # Missing value\n",
    "    'p_value_improvement': [0.032, 0.015, 0.048, 0.023],\n",
    "    'significant': ['yes', 'yes', 'no', 'yes']\n",
    "}\n",
    "\n",
    "# Process data\n",
    "processor = DataProcessor()\n",
    "result = (processor\n",
    "          .load_data(complex_data)\n",
    "          .clean_numeric()\n",
    "          .find_statistics()\n",
    "          .reshape_for_analysis(['baseline_score', 'week_1_score', 'week_2_score'],\n",
    "                               ['patient_id', 'age'])\n",
    "          .get_result())\n",
    "\n",
    "print(\"\\nProcessed result:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary and Best Practices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Takeaways\n",
    "\n",
    "1. **Universal DataFrame Conversion**: `force_df` handles any data type gracefully\n",
    "2. **Statistical Analysis**: Automatic p-value detection simplifies result processing\n",
    "3. **Flexible Reshaping**: Melt specific columns while preserving relationships\n",
    "4. **Data Organization**: Easy column and row reordering for presentation\n",
    "5. **Format Conversion**: Transform between wide and long formats effortlessly\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Data Loading**:\n",
    "   ```python\n",
    "   # Always use force_df for consistent DataFrames\n",
    "   df = stx.pd.force_df(any_data_source)\n",
    "   ```\n",
    "\n",
    "2. **Statistical Workflows**:\n",
    "   ```python\n",
    "   # Automatically organize statistical results\n",
    "   pval_cols = stx.pd.find_pval(results)\n",
    "   df_organized = stx.pd.mv_to_first(results, pval_cols[0])\n",
    "   ```\n",
    "\n",
    "3. **Data Reshaping**:\n",
    "   ```python\n",
    "   # Melt only what you need\n",
    "   long_df = stx.pd.melt_cols(wide_df, ['col1', 'col2'])\n",
    "   ```\n",
    "\n",
    "4. **Visualization Prep**:\n",
    "   ```python\n",
    "   # Convert matrices for plotting\n",
    "   xyz = stx.pd.to_xyz(correlation_matrix)\n",
    "   ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nPandas utilities tutorial completed!\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Use force_df for robust data loading\")\n",
    "print(\"2. Automate statistical report formatting\")\n",
    "print(\"3. Simplify data reshaping workflows\")\n",
    "print(\"4. Create reproducible data pipelines\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}