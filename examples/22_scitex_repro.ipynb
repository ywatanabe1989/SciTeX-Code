{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprehensive SciTeX Reproducibility Module Examples\n",
    "\n",
    "This notebook demonstrates the complete functionality of the `scitex.repro` module, which provides reproducibility tools for scientific computing and research.\n",
    "\n",
    "## Module Overview\n",
    "\n",
    "The `scitex.repro` module includes:\n",
    "- Random seed fixing for multiple libraries\n",
    "- Unique identifier generation for experiments\n",
    "- Timestamp generation for versioning and tracking\n",
    "- Tools for ensuring reproducible research\n",
    "\n",
    "## Import Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d99551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect notebook name for output directory\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Get notebook name (for papermill compatibility)\n",
    "notebook_name = \"22_scitex_repro\"\n",
    "if 'PAPERMILL_NOTEBOOK_NAME' in os.environ:\n",
    "    notebook_name = Path(os.environ['PAPERMILL_NOTEBOOK_NAME']).stem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Try to import optional libraries\n",
    "try:\n",
    "    import torch\n",
    "    TORCH_AVAILABLE = True\n",
    "except ImportError:\n",
    "    TORCH_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    TF_AVAILABLE = True\n",
    "except ImportError:\n",
    "    TF_AVAILABLE = False\n",
    "\n",
    "# Import scitex repro module\n",
    "import scitex.repro as srepro\n",
    "\n",
    "print(\"Available functions in scitex.repro:\")\n",
    "repro_attrs = [attr for attr in dir(srepro) if not attr.startswith('_')]\n",
    "for i, attr in enumerate(repro_attrs):\n",
    "    print(f\"{i+1:2d}. {attr}\")\n",
    "\n",
    "print(f\"\\nLibrary availability:\")\n",
    "print(f\"  PyTorch: {'✓' if TORCH_AVAILABLE else '✗'}\")\n",
    "print(f\"  TensorFlow: {'✓' if TF_AVAILABLE else '✗'}\")\n",
    "print(f\"  NumPy: ✓\")\n",
    "print(f\"  Random: ✓\")\n",
    "print(f\"  OS: ✓\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Random Seed Fixing\n",
    "\n",
    "### Basic Seed Fixing\n",
    "\n",
    "The `fix_seeds` function ensures reproducible random number generation across multiple libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Basic seed fixing demonstration\n",
    "print(\"Basic Seed Fixing Demonstration:\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "def generate_random_data(label):\n",
    "    \"\"\"Generate random data from different sources.\"\"\"\n",
    "    print(f\"\\n{label}:\")\n",
    "    \n",
    "    # Python random module\n",
    "    python_random = [random.random() for _ in range(3)]\n",
    "    print(f\"  Python random: {[f'{x:.6f}' for x in python_random]}\")\n",
    "    \n",
    "    # NumPy random\n",
    "    numpy_random = np.random.random(3)\n",
    "    print(f\"  NumPy random:  {[f'{x:.6f}' for x in numpy_random]}\")\n",
    "    \n",
    "    # PyTorch random (if available)\n",
    "    if TORCH_AVAILABLE:\n",
    "        torch_random = torch.rand(3)\n",
    "        print(f\"  PyTorch random: {[f'{x:.6f}' for x in torch_random.tolist()]}\")\n",
    "    \n",
    "    return python_random, numpy_random\n",
    "\n",
    "# Generate random data before seed fixing\n",
    "print(\"Before seed fixing (should be different each time):\")\n",
    "data1_py, data1_np = generate_random_data(\"Run 1\")\n",
    "data2_py, data2_np = generate_random_data(\"Run 2\")\n",
    "\n",
    "# Check if data is different\n",
    "print(f\"\\nData differences (should be True):\")\n",
    "print(f\"  Python random different: {data1_py != data2_py}\")\n",
    "print(f\"  NumPy random different:  {not np.array_equal(data1_np, data2_np)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Now fixing seeds and testing reproducibility...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Reproducibility with seed fixing\n",
    "print(\"Reproducibility with Seed Fixing:\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Fix seeds for all available libraries\n",
    "print(\"Fixing seeds for all libraries...\")\n",
    "if TORCH_AVAILABLE and TF_AVAILABLE:\n",
    "    srepro.fix_seeds(os=os, random=random, np=np, torch=torch, tf=tf, seed=42, verbose=True)\n",
    "elif TORCH_AVAILABLE:\n",
    "    srepro.fix_seeds(os=os, random=random, np=np, torch=torch, seed=42, verbose=True)\n",
    "else:\n",
    "    srepro.fix_seeds(os=os, random=random, np=np, seed=42, verbose=True)\n",
    "\n",
    "# Generate data after first seed fixing\n",
    "print(\"\\nAfter seed fixing - Run A:\")\n",
    "dataA_py, dataA_np = generate_random_data(\"Seed-fixed Run A\")\n",
    "\n",
    "# Fix seeds again with same seed\n",
    "print(\"\\nFixing seeds again with same seed (42)...\")\n",
    "if TORCH_AVAILABLE and TF_AVAILABLE:\n",
    "    srepro.fix_seeds(os=os, random=random, np=np, torch=torch, tf=tf, seed=42, verbose=False)\n",
    "elif TORCH_AVAILABLE:\n",
    "    srepro.fix_seeds(os=os, random=random, np=np, torch=torch, seed=42, verbose=False)\n",
    "else:\n",
    "    srepro.fix_seeds(os=os, random=random, np=np, seed=42, verbose=False)\n",
    "\n",
    "# Generate data after second seed fixing\n",
    "print(\"\\nAfter seed fixing - Run B:\")\n",
    "dataB_py, dataB_np = generate_random_data(\"Seed-fixed Run B\")\n",
    "\n",
    "# Verify reproducibility\n",
    "print(f\"\\nReproducibility check (should be True):\")\n",
    "print(f\"  Python random identical: {dataA_py == dataB_py}\")\n",
    "print(f\"  NumPy random identical:  {np.array_equal(dataA_np, dataB_np)}\")\n",
    "\n",
    "if dataA_py == dataB_py and np.array_equal(dataA_np, dataB_np):\n",
    "    print(\"  ✓ Perfect reproducibility achieved!\")\n",
    "else:\n",
    "    print(\"  ✗ Reproducibility issue detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different Seed Values\n",
    "\n",
    "Let's test reproducibility with different seed values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Testing different seed values\n",
    "print(\"Testing Different Seed Values:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "def test_seed_reproducibility(seed_value):\n",
    "    \"\"\"Test reproducibility with a specific seed.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for run in range(3):  # Run 3 times with same seed\n",
    "        # Fix seeds\n",
    "        if TORCH_AVAILABLE:\n",
    "            srepro.fix_seeds(random=random, np=np, torch=torch, seed=seed_value, verbose=False)\n",
    "        else:\n",
    "            srepro.fix_seeds(random=random, np=np, seed=seed_value, verbose=False)\n",
    "        \n",
    "        # Generate some random data\n",
    "        py_val = random.random()\n",
    "        np_val = np.random.random()\n",
    "        torch_val = torch.rand(1).item() if TORCH_AVAILABLE else 0.0\n",
    "        \n",
    "        results.append((py_val, np_val, torch_val))\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test different seeds\n",
    "test_seeds = [42, 123, 987, 2024]\n",
    "\n",
    "print(f\"Testing seeds: {test_seeds}\")\n",
    "print(f\"Each seed will be tested 3 times to verify reproducibility\\n\")\n",
    "\n",
    "seed_results = {}\n",
    "\n",
    "for seed in test_seeds:\n",
    "    print(f\"Testing seed {seed}:\")\n",
    "    results = test_seed_reproducibility(seed)\n",
    "    seed_results[seed] = results\n",
    "    \n",
    "    # Check if all runs produced identical results\n",
    "    first_result = results[0]\n",
    "    all_identical = all(result == first_result for result in results)\n",
    "    \n",
    "    print(f\"  Run 1: Python={first_result[0]:.6f}, NumPy={first_result[1]:.6f}, PyTorch={first_result[2]:.6f}\")\n",
    "    print(f\"  Reproducibility: {'✓' if all_identical else '✗'}\")\n",
    "    \n",
    "    if not all_identical:\n",
    "        print(f\"  Detailed results: {results}\")\n",
    "    print()\n",
    "\n",
    "# Compare different seeds (should produce different values)\n",
    "print(f\"Comparing results across different seeds:\")\n",
    "print(f\"Seed |    Python    |     NumPy    |   PyTorch\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for seed in test_seeds:\n",
    "    first_result = seed_results[seed][0]\n",
    "    py_val, np_val, torch_val = first_result\n",
    "    print(f\" {seed:3d} | {py_val:.6f}   | {np_val:.6f}   | {torch_val:.6f}\")\n",
    "\n",
    "# Verify that different seeds produce different results\n",
    "all_python_values = [seed_results[seed][0][0] for seed in test_seeds]\n",
    "all_numpy_values = [seed_results[seed][0][1] for seed in test_seeds]\n",
    "\n",
    "python_unique = len(set(all_python_values)) == len(all_python_values)\n",
    "numpy_unique = len(set(all_numpy_values)) == len(all_numpy_values)\n",
    "\n",
    "print(f\"\\nSeed differentiation:\")\n",
    "print(f\"  Python values unique: {'✓' if python_unique else '✗'}\")\n",
    "print(f\"  NumPy values unique:  {'✓' if numpy_unique else '✗'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Unique Identifier Generation\n",
    "\n",
    "### Basic ID Generation\n",
    "\n",
    "The `gen_id` and `gen_ID` functions generate unique identifiers for experiments and runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 4: Unique identifier generation\n",
    "print(\"Unique Identifier Generation:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Generate multiple IDs with default settings\n",
    "print(\"Default ID generation:\")\n",
    "for i in range(5):\n",
    "    experiment_id = srepro.gen_id()\n",
    "    print(f\"  ID {i+1}: {experiment_id}\")\n",
    "    time.sleep(0.1)  # Small delay to ensure different timestamps\n",
    "\n",
    "# Test backward compatibility alias\n",
    "print(f\"\\nBackward compatibility test:\")\n",
    "old_style_id = srepro.gen_ID()\n",
    "new_style_id = srepro.gen_id()\n",
    "print(f\"  gen_ID():  {old_style_id}\")\n",
    "print(f\"  gen_id():  {new_style_id}\")\n",
    "print(f\"  Both functions work: ✓\")\n",
    "\n",
    "# Custom time format\n",
    "print(f\"\\nCustom time formats:\")\n",
    "custom_formats = [\n",
    "    (\"%Y%m%d\", \"YYYYMMDD format\"),\n",
    "    (\"%Y-%m-%d_%H%M\", \"Date and time format\"),\n",
    "    (\"%j_%Y\", \"Day of year format\"),\n",
    "    (\"%W_%Y\", \"Week of year format\"),\n",
    "]\n",
    "\n",
    "for time_format, description in custom_formats:\n",
    "    custom_id = srepro.gen_id(time_format=time_format, N=4)\n",
    "    print(f\"  {description:20s}: {custom_id}\")\n",
    "\n",
    "# Different random string lengths\n",
    "print(f\"\\nDifferent random string lengths:\")\n",
    "for N in [4, 8, 12, 16]:\n",
    "    var_length_id = srepro.gen_id(N=N)\n",
    "    random_part = var_length_id.split('_')[-1]\n",
    "    print(f\"  N={N:2d}: {var_length_id} (random part: '{random_part}', length: {len(random_part)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ID Uniqueness Testing\n",
    "\n",
    "Let's test the uniqueness properties of the generated identifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 5: ID uniqueness testing\n",
    "print(\"ID Uniqueness Testing:\")\n",
    "print(\"=\" * 25)\n",
    "\n",
    "# Generate many IDs to test uniqueness\n",
    "n_ids = 1000\n",
    "print(f\"Generating {n_ids} IDs to test uniqueness...\")\n",
    "\n",
    "generated_ids = []\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(n_ids):\n",
    "    new_id = srepro.gen_id()\n",
    "    generated_ids.append(new_id)\n",
    "    \n",
    "    # Add tiny delay occasionally to ensure timestamp differences\n",
    "    if i % 100 == 0 and i > 0:\n",
    "        time.sleep(0.001)\n",
    "\n",
    "generation_time = time.time() - start_time\n",
    "\n",
    "# Analyze uniqueness\n",
    "unique_ids = set(generated_ids)\n",
    "n_unique = len(unique_ids)\n",
    "n_duplicates = n_ids - n_unique\n",
    "\n",
    "print(f\"\\nUniqueness Analysis:\")\n",
    "print(f\"  Generated IDs: {n_ids:,}\")\n",
    "print(f\"  Unique IDs: {n_unique:,}\")\n",
    "print(f\"  Duplicates: {n_duplicates:,}\")\n",
    "print(f\"  Uniqueness rate: {(n_unique/n_ids)*100:.2f}%\")\n",
    "print(f\"  Generation time: {generation_time:.4f} seconds\")\n",
    "print(f\"  Rate: {n_ids/generation_time:.0f} IDs/second\")\n",
    "\n",
    "if n_duplicates == 0:\n",
    "    print(f\"  ✓ Perfect uniqueness achieved!\")\n",
    "else:\n",
    "    print(f\"  ⚠ {n_duplicates} duplicates found\")\n",
    "    # Show first few duplicates\n",
    "    duplicate_count = {}\n",
    "    for gen_id in generated_ids:\n",
    "        duplicate_count[gen_id] = duplicate_count.get(gen_id, 0) + 1\n",
    "    \n",
    "    duplicates = {k: v for k, v in duplicate_count.items() if v > 1}\n",
    "    print(f\"  First few duplicates: {list(duplicates.items())[:3]}\")\n",
    "\n",
    "# Analyze ID structure\n",
    "print(f\"\\nID Structure Analysis:\")\n",
    "sample_ids = generated_ids[:5]\n",
    "print(f\"  Sample IDs:\")\n",
    "for i, sample_id in enumerate(sample_ids):\n",
    "    timestamp_part = sample_id.split('_')[0]\n",
    "    random_part = sample_id.split('_')[1]\n",
    "    print(f\"    {i+1}. {sample_id}\")\n",
    "    print(f\"       Timestamp: '{timestamp_part}', Random: '{random_part}'\")\n",
    "\n",
    "# Analyze timestamp distribution\n",
    "timestamps = [gen_id.split('_')[0] for gen_id in generated_ids]\n",
    "unique_timestamps = set(timestamps)\n",
    "print(f\"\\n  Timestamp analysis:\")\n",
    "print(f\"    Unique timestamps: {len(unique_timestamps)}\")\n",
    "print(f\"    Timestamp compression: {len(unique_timestamps)/n_ids*100:.1f}% (lower is better for fast generation)\")\n",
    "\n",
    "# Analyze random part distribution\n",
    "random_parts = [gen_id.split('_')[1] for gen_id in generated_ids]\n",
    "unique_random_parts = set(random_parts)\n",
    "print(f\"    Unique random parts: {len(unique_random_parts)}\")\n",
    "print(f\"    Random part uniqueness: {len(unique_random_parts)/n_ids*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Timestamp Generation\n",
    "\n",
    "### Basic Timestamp Generation\n",
    "\n",
    "The `gen_timestamp` and `timestamp` functions generate standardized timestamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 6: Timestamp generation\n",
    "print(\"Timestamp Generation:\")\n",
    "print(\"=\" * 20)\n",
    "\n",
    "# Generate current timestamps\n",
    "print(\"Current timestamps:\")\n",
    "for i in range(5):\n",
    "    ts = srepro.gen_timestamp()\n",
    "    print(f\"  Timestamp {i+1}: {ts}\")\n",
    "    time.sleep(0.1)  # Small delay to show timestamp progression\n",
    "\n",
    "# Test backward compatibility alias\n",
    "print(f\"\\nBackward compatibility:\")\n",
    "ts1 = srepro.gen_timestamp()\n",
    "ts2 = srepro.timestamp()\n",
    "print(f\"  gen_timestamp(): {ts1}\")\n",
    "print(f\"  timestamp():     {ts2}\")\n",
    "print(f\"  Both functions work: ✓\")\n",
    "\n",
    "# Timestamp format analysis\n",
    "current_ts = srepro.gen_timestamp()\n",
    "print(f\"\\nTimestamp format analysis:\")\n",
    "print(f\"  Current timestamp: {current_ts}\")\n",
    "print(f\"  Format: YYYY-MMDD-HHMM\")\n",
    "print(f\"  Length: {len(current_ts)} characters\")\n",
    "\n",
    "# Parse timestamp components\n",
    "try:\n",
    "    # Parse the timestamp format: YYYY-MMDD-HHMM\n",
    "    parts = current_ts.split('-')\n",
    "    if len(parts) == 3:\n",
    "        year = parts[0]\n",
    "        month_day = parts[1]\n",
    "        hour_minute = parts[2]\n",
    "        \n",
    "        month = month_day[:2]\n",
    "        day = month_day[2:]\n",
    "        hour = hour_minute[:2]\n",
    "        minute = hour_minute[2:]\n",
    "        \n",
    "        print(f\"  Components:\")\n",
    "        print(f\"    Year: {year}\")\n",
    "        print(f\"    Month: {month}\")\n",
    "        print(f\"    Day: {day}\")\n",
    "        print(f\"    Hour: {hour}\")\n",
    "        print(f\"    Minute: {minute}\")\n",
    "        \n",
    "        # Verify parsing\n",
    "        reconstructed = f\"{year}-{month}{day}-{hour}{minute}\"\n",
    "        print(f\"  Parsing verification: {reconstructed == current_ts}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"  Error parsing timestamp: {e}\")\n",
    "\n",
    "# Demonstrate timestamp usage for file naming\n",
    "print(f\"\\nPractical usage examples:\")\n",
    "ts = srepro.gen_timestamp()\n",
    "example_filenames = [\n",
    "    f\"experiment_{ts}.csv\",\n",
    "    f\"results_{ts}.json\",\n",
    "    f\"model_weights_{ts}.pt\",\n",
    "    f\"analysis_{ts}.ipynb\",\n",
    "    f\"backup_{ts}.tar.gz\"\n",
    "]\n",
    "\n",
    "print(f\"  Example filenames with timestamp:\")\n",
    "for filename in example_filenames:\n",
    "    print(f\"    {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timestamp Chronological Testing\n",
    "\n",
    "Let's verify that timestamps maintain chronological order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 7: Timestamp chronological testing\n",
    "print(\"Timestamp Chronological Testing:\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Generate timestamps over time\n",
    "timestamps = []\n",
    "generation_times = []\n",
    "\n",
    "print(\"Generating timestamps with delays:\")\n",
    "for i in range(10):\n",
    "    current_time = datetime.now()\n",
    "    ts = srepro.gen_timestamp()\n",
    "    \n",
    "    timestamps.append(ts)\n",
    "    generation_times.append(current_time)\n",
    "    \n",
    "    print(f\"  {i+1:2d}. {ts} (real time: {current_time.strftime('%H:%M:%S.%f')[:-3]})\")\n",
    "    \n",
    "    # Variable delay to test different scenarios\n",
    "    if i < 9:\n",
    "        delay = 0.1 if i % 3 == 0 else 0.05\n",
    "        time.sleep(delay)\n",
    "\n",
    "# Analyze chronological order\n",
    "print(f\"\\nChronological order analysis:\")\n",
    "\n",
    "# Check if timestamps are in order\n",
    "is_chronological = True\n",
    "for i in range(1, len(timestamps)):\n",
    "    if timestamps[i] < timestamps[i-1]:\n",
    "        is_chronological = False\n",
    "        print(f\"  Order violation at position {i}: {timestamps[i-1]} > {timestamps[i]}\")\n",
    "\n",
    "print(f\"  Timestamps in chronological order: {'✓' if is_chronological else '✗'}\")\n",
    "\n",
    "# Check for duplicates\n",
    "unique_timestamps = set(timestamps)\n",
    "n_duplicates = len(timestamps) - len(unique_timestamps)\n",
    "print(f\"  Duplicate timestamps: {n_duplicates}\")\n",
    "\n",
    "if n_duplicates > 0:\n",
    "    print(f\"  Note: Duplicates expected for rapid generation within same minute\")\n",
    "\n",
    "# Analyze timestamp resolution\n",
    "print(f\"\\nTimestamp resolution analysis:\")\n",
    "print(f\"  Total timestamps: {len(timestamps)}\")\n",
    "print(f\"  Unique timestamps: {len(unique_timestamps)}\")\n",
    "print(f\"  Resolution efficiency: {len(unique_timestamps)/len(timestamps)*100:.1f}%\")\n",
    "\n",
    "# Show timestamp distribution\n",
    "from collections import Counter\n",
    "timestamp_counts = Counter(timestamps)\n",
    "if len(timestamp_counts) < len(timestamps):\n",
    "    print(f\"  Timestamp frequency distribution:\")\n",
    "    for ts, count in timestamp_counts.most_common(3):\n",
    "        print(f\"    {ts}: {count} occurrences\")\n",
    "\n",
    "# Demonstrate sorting behavior\n",
    "print(f\"\\nSorting demonstration:\")\n",
    "shuffled_timestamps = timestamps.copy()\n",
    "random.shuffle(shuffled_timestamps)\n",
    "sorted_timestamps = sorted(shuffled_timestamps)\n",
    "\n",
    "print(f\"  Original order matches sorted: {timestamps == sorted_timestamps}\")\n",
    "if timestamps != sorted_timestamps:\n",
    "    print(f\"  Original: {timestamps[:3]}...\")\n",
    "    print(f\"  Sorted:   {sorted_timestamps[:3]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Practical Reproducibility Workflows\n",
    "\n",
    "### Complete Experiment Setup\n",
    "\n",
    "Let's demonstrate a complete reproducible experiment setup using all the tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 8: Complete reproducible experiment setup\n",
    "print(\"Complete Reproducible Experiment Setup:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "class ReproducibleExperiment:\n",
    "    \"\"\"A class for managing reproducible experiments.\"\"\"\n",
    "    \n",
    "    def __init__(self, name, seed=None, description=None):\n",
    "        self.name = name\n",
    "        self.description = description or f\"Experiment: {name}\"\n",
    "        \n",
    "        # Generate experiment metadata\n",
    "        self.experiment_id = srepro.gen_id()\n",
    "        self.timestamp = srepro.gen_timestamp()\n",
    "        self.seed = seed or 42\n",
    "        \n",
    "        # Initialize reproducible state\n",
    "        self._setup_reproducibility()\n",
    "        \n",
    "        # Store experiment info\n",
    "        self.info = {\n",
    "            'name': self.name,\n",
    "            'id': self.experiment_id,\n",
    "            'timestamp': self.timestamp,\n",
    "            'seed': self.seed,\n",
    "            'description': self.description,\n",
    "            'status': 'initialized'\n",
    "        }\n",
    "        \n",
    "        print(f\"Experiment '{self.name}' initialized:\")\n",
    "        print(f\"  ID: {self.experiment_id}\")\n",
    "        print(f\"  Timestamp: {self.timestamp}\")\n",
    "        print(f\"  Seed: {self.seed}\")\n",
    "    \n",
    "    def _setup_reproducibility(self):\n",
    "        \"\"\"Set up reproducible random states.\"\"\"\n",
    "        if TORCH_AVAILABLE:\n",
    "            srepro.fix_seeds(os=os, random=random, np=np, torch=torch, seed=self.seed, verbose=False)\n",
    "        else:\n",
    "            srepro.fix_seeds(os=os, random=random, np=np, seed=self.seed, verbose=False)\n",
    "    \n",
    "    def run_simulation(self, n_samples=1000):\n",
    "        \"\"\"Run a reproducible simulation.\"\"\"\n",
    "        print(f\"\\n  Running simulation with {n_samples} samples...\")\n",
    "        \n",
    "        # Generate reproducible data\n",
    "        data = np.random.normal(0, 1, n_samples)\n",
    "        noise = np.random.random(n_samples) * 0.1\n",
    "        signal = np.sin(np.linspace(0, 4*np.pi, n_samples)) + noise\n",
    "        \n",
    "        # Compute statistics\n",
    "        results = {\n",
    "            'mean': np.mean(data),\n",
    "            'std': np.std(data),\n",
    "            'signal_mean': np.mean(signal),\n",
    "            'signal_std': np.std(signal),\n",
    "            'correlation': np.corrcoef(data[:len(signal)], signal)[0, 1],\n",
    "            'n_samples': n_samples\n",
    "        }\n",
    "        \n",
    "        self.info['results'] = results\n",
    "        self.info['status'] = 'completed'\n",
    "        \n",
    "        print(f\"  Results:\")\n",
    "        for key, value in results.items():\n",
    "            if isinstance(value, float):\n",
    "                print(f\"    {key}: {value:.6f}\")\n",
    "            else:\n",
    "                print(f\"    {key}: {value}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get_summary(self):\n",
    "        \"\"\"Get experiment summary.\"\"\"\n",
    "        return self.info.copy()\n",
    "\n",
    "# Run multiple experiments to test reproducibility\n",
    "experiments = []\n",
    "\n",
    "print(\"\\nRunning multiple experiments with same parameters:\")\n",
    "for i in range(3):\n",
    "    exp_name = f\"RepeatTest_{i+1}\"\n",
    "    exp = ReproducibleExperiment(\n",
    "        name=exp_name,\n",
    "        seed=42,  # Same seed for all\n",
    "        description=f\"Reproducibility test experiment {i+1}\"\n",
    "    )\n",
    "    \n",
    "    results = exp.run_simulation(n_samples=500)\n",
    "    experiments.append(exp)\n",
    "    print()\n",
    "\n",
    "# Verify reproducibility across experiments\n",
    "print(\"Reproducibility verification:\")\n",
    "first_results = experiments[0].get_summary()['results']\n",
    "\n",
    "all_identical = True\n",
    "for i, exp in enumerate(experiments[1:], 1):\n",
    "    current_results = exp.get_summary()['results']\n",
    "    \n",
    "    # Compare key results\n",
    "    for key in ['mean', 'std', 'signal_mean', 'signal_std', 'correlation']:\n",
    "        if abs(first_results[key] - current_results[key]) > 1e-10:\n",
    "            print(f\"  Difference in {key}: {first_results[key]} vs {current_results[key]}\")\n",
    "            all_identical = False\n",
    "\n",
    "if all_identical:\n",
    "    print(f\"  ✓ All {len(experiments)} experiments produced identical results!\")\n",
    "else:\n",
    "    print(f\"  ✗ Reproducibility issues detected\")\n",
    "\n",
    "# Show experiment metadata\n",
    "print(f\"\\nExperiment metadata summary:\")\n",
    "print(f\"Exp # | ID (last 8)     | Timestamp    | Seed | Status\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "for i, exp in enumerate(experiments):\n",
    "    summary = exp.get_summary()\n",
    "    id_short = summary['id'][-8:]\n",
    "    print(f\"  {i+1}   | {id_short}       | {summary['timestamp']} |  {summary['seed']}  | {summary['status']}\")\n",
    "\n",
    "# Demonstrate different seeds produce different results\n",
    "print(f\"\\nTesting different seeds (should produce different results):\")\n",
    "different_seed_exp = ReproducibleExperiment(\n",
    "    name=\"DifferentSeed\",\n",
    "    seed=123,  # Different seed\n",
    "    description=\"Test with different seed\"\n",
    ")\n",
    "\n",
    "different_results = different_seed_exp.run_simulation(n_samples=500)\n",
    "\n",
    "# Compare with first experiment\n",
    "print(f\"\\nComparison with different seed:\")\n",
    "for key in ['mean', 'std', 'signal_mean']:\n",
    "    original = first_results[key]\n",
    "    different = different_results[key]\n",
    "    diff = abs(original - different)\n",
    "    print(f\"  {key:12s}: {original:.6f} vs {different:.6f} (diff: {diff:.6f})\")\n",
    "\n",
    "# Check if results are sufficiently different\n",
    "significant_differences = sum(1 for key in ['mean', 'std', 'signal_mean'] \n",
    "                             if abs(first_results[key] - different_results[key]) > 0.01)\n",
    "\n",
    "print(f\"\\nSeed differentiation: {'✓' if significant_differences > 0 else '✗'} ({significant_differences}/3 metrics significantly different)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment Tracking and Versioning\n",
    "\n",
    "Let's demonstrate how to use the reproducibility tools for experiment tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 9: Experiment tracking and versioning\n",
    "print(\"Experiment Tracking and Versioning:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "class ExperimentTracker:\n",
    "    \"\"\"Track multiple experiments with versioning.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.experiments = []\n",
    "        self.session_id = srepro.gen_id(time_format=\"%Y%m%d_%H%M\", N=6)\n",
    "        print(f\"Experiment tracking session: {self.session_id}\")\n",
    "    \n",
    "    def run_experiment(self, name, config):\n",
    "        \"\"\"Run an experiment with given configuration.\"\"\"\n",
    "        # Generate experiment metadata\n",
    "        exp_id = srepro.gen_id(N=8)\n",
    "        timestamp = srepro.gen_timestamp()\n",
    "        \n",
    "        # Set up reproducibility\n",
    "        seed = config.get('seed', 42)\n",
    "        if TORCH_AVAILABLE:\n",
    "            srepro.fix_seeds(random=random, np=np, torch=torch, seed=seed, verbose=False)\n",
    "        else:\n",
    "            srepro.fix_seeds(random=random, np=np, seed=seed, verbose=False)\n",
    "        \n",
    "        # Run experiment based on configuration\n",
    "        results = self._execute_experiment(config)\n",
    "        \n",
    "        # Store experiment record\n",
    "        experiment_record = {\n",
    "            'id': exp_id,\n",
    "            'name': name,\n",
    "            'timestamp': timestamp,\n",
    "            'session_id': self.session_id,\n",
    "            'config': config.copy(),\n",
    "            'results': results,\n",
    "            'version': len(self.experiments) + 1\n",
    "        }\n",
    "        \n",
    "        self.experiments.append(experiment_record)\n",
    "        \n",
    "        print(f\"\\nExperiment '{name}' completed:\")\n",
    "        print(f\"  ID: {exp_id}\")\n",
    "        print(f\"  Version: {experiment_record['version']}\")\n",
    "        print(f\"  Timestamp: {timestamp}\")\n",
    "        print(f\"  Config: {config}\")\n",
    "        print(f\"  Key result: {results['score']:.4f}\")\n",
    "        \n",
    "        return experiment_record\n",
    "    \n",
    "    def _execute_experiment(self, config):\n",
    "        \"\"\"Execute experiment logic based on config.\"\"\"\n",
    "        n_samples = config.get('n_samples', 1000)\n",
    "        noise_level = config.get('noise_level', 0.1)\n",
    "        method = config.get('method', 'linear')\n",
    "        \n",
    "        # Generate data\n",
    "        x = np.random.random(n_samples)\n",
    "        \n",
    "        if method == 'linear':\n",
    "            y = 2 * x + 1 + np.random.normal(0, noise_level, n_samples)\n",
    "        elif method == 'quadratic':\n",
    "            y = x**2 + 0.5 * x + np.random.normal(0, noise_level, n_samples)\n",
    "        elif method == 'sine':\n",
    "            y = np.sin(2 * np.pi * x) + np.random.normal(0, noise_level, n_samples)\n",
    "        else:\n",
    "            y = x + np.random.normal(0, noise_level, n_samples)\n",
    "        \n",
    "        # Compute results\n",
    "        correlation = np.corrcoef(x, y)[0, 1]\n",
    "        mse = np.mean((y - x)**2)  # Simple baseline MSE\n",
    "        score = correlation - 0.1 * mse  # Combined score\n",
    "        \n",
    "        return {\n",
    "            'correlation': correlation,\n",
    "            'mse': mse,\n",
    "            'score': score,\n",
    "            'data_stats': {\n",
    "                'x_mean': np.mean(x),\n",
    "                'y_mean': np.mean(y),\n",
    "                'x_std': np.std(x),\n",
    "                'y_std': np.std(y)\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def get_summary(self):\n",
    "        \"\"\"Get summary of all experiments.\"\"\"\n",
    "        if not self.experiments:\n",
    "            return \"No experiments recorded\"\n",
    "        \n",
    "        summary = f\"Session {self.session_id} Summary:\\n\"\n",
    "        summary += f\"Total experiments: {len(self.experiments)}\\n\"\n",
    "        summary += \"Ver | Name          | Timestamp    | Score    | Method\\n\"\n",
    "        summary += \"-\" * 55 + \"\\n\"\n",
    "        \n",
    "        for exp in self.experiments:\n",
    "            summary += f\"{exp['version']:3d} | {exp['name']:13s} | {exp['timestamp']} | {exp['results']['score']:7.4f} | {exp['config'].get('method', 'N/A')}\\n\"\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    def get_best_experiment(self):\n",
    "        \"\"\"Get the experiment with the highest score.\"\"\"\n",
    "        if not self.experiments:\n",
    "            return None\n",
    "        \n",
    "        best_exp = max(self.experiments, key=lambda x: x['results']['score'])\n",
    "        return best_exp\n",
    "\n",
    "# Create experiment tracker and run various experiments\n",
    "tracker = ExperimentTracker()\n",
    "\n",
    "# Define different experimental configurations\n",
    "experiment_configs = [\n",
    "    {\n",
    "        'name': 'baseline_linear',\n",
    "        'config': {'method': 'linear', 'n_samples': 1000, 'noise_level': 0.1, 'seed': 42}\n",
    "    },\n",
    "    {\n",
    "        'name': 'low_noise_linear',\n",
    "        'config': {'method': 'linear', 'n_samples': 1000, 'noise_level': 0.05, 'seed': 42}\n",
    "    },\n",
    "    {\n",
    "        'name': 'quadratic_test',\n",
    "        'config': {'method': 'quadratic', 'n_samples': 1000, 'noise_level': 0.1, 'seed': 42}\n",
    "    },\n",
    "    {\n",
    "        'name': 'sine_wave_test',\n",
    "        'config': {'method': 'sine', 'n_samples': 1000, 'noise_level': 0.1, 'seed': 42}\n",
    "    },\n",
    "    {\n",
    "        'name': 'large_sample',\n",
    "        'config': {'method': 'linear', 'n_samples': 5000, 'noise_level': 0.1, 'seed': 42}\n",
    "    }\n",
    "]\n",
    "\n",
    "# Run all experiments\n",
    "for exp_config in experiment_configs:\n",
    "    tracker.run_experiment(exp_config['name'], exp_config['config'])\n",
    "\n",
    "# Display summary\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(tracker.get_summary())\n",
    "\n",
    "# Find and display best experiment\n",
    "best_exp = tracker.get_best_experiment()\n",
    "if best_exp:\n",
    "    print(f\"Best performing experiment:\")\n",
    "    print(f\"  Name: {best_exp['name']}\")\n",
    "    print(f\"  ID: {best_exp['id']}\")\n",
    "    print(f\"  Score: {best_exp['results']['score']:.6f}\")\n",
    "    print(f\"  Config: {best_exp['config']}\")\n",
    "\n",
    "# Test reproducibility by re-running best experiment\n",
    "print(f\"\\nReproducibility test - re-running best experiment:\")\n",
    "best_config = best_exp['config']\n",
    "rerun_exp = tracker.run_experiment(f\"{best_exp['name']}_rerun\", best_config)\n",
    "\n",
    "# Compare results\n",
    "original_score = best_exp['results']['score']\n",
    "rerun_score = rerun_exp['results']['score']\n",
    "score_diff = abs(original_score - rerun_score)\n",
    "\n",
    "print(f\"\\nReproducibility verification:\")\n",
    "print(f\"  Original score: {original_score:.10f}\")\n",
    "print(f\"  Rerun score:    {rerun_score:.10f}\")\n",
    "print(f\"  Difference:     {score_diff:.2e}\")\n",
    "print(f\"  Reproducible:   {'✓' if score_diff < 1e-10 else '✗'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Advanced Reproducibility Patterns\n",
    "\n",
    "### Hierarchical Experiment Organization\n",
    "\n",
    "Let's demonstrate advanced patterns for organizing reproducible experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 10: Advanced reproducibility patterns\n",
    "print(\"Advanced Reproducibility Patterns:\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "class ReproducibilityManager:\n",
    "    \"\"\"Advanced manager for reproducible research workflows.\"\"\"\n",
    "    \n",
    "    def __init__(self, project_name):\n",
    "        self.project_name = project_name\n",
    "        self.project_id = srepro.gen_id(time_format=\"%Y%m%d\", N=4)\n",
    "        self.sessions = {}\n",
    "        self.global_config = {\n",
    "            'project_name': project_name,\n",
    "            'project_id': self.project_id,\n",
    "            'created_at': srepro.gen_timestamp()\n",
    "        }\n",
    "        \n",
    "        print(f\"Reproducibility Manager initialized:\")\n",
    "        print(f\"  Project: {project_name}\")\n",
    "        print(f\"  Project ID: {self.project_id}\")\n",
    "        print(f\"  Created: {self.global_config['created_at']}\")\n",
    "    \n",
    "    def create_session(self, session_name, base_seed=None):\n",
    "        \"\"\"Create a new experimental session.\"\"\"\n",
    "        if base_seed is None:\n",
    "            base_seed = hash(session_name) % 10000  # Deterministic seed from name\n",
    "        \n",
    "        session_id = srepro.gen_id(time_format=\"%Y%m%d_%H%M\", N=4)\n",
    "        \n",
    "        session = {\n",
    "            'name': session_name,\n",
    "            'id': session_id,\n",
    "            'base_seed': base_seed,\n",
    "            'created_at': srepro.gen_timestamp(),\n",
    "            'experiments': [],\n",
    "            'status': 'active'\n",
    "        }\n",
    "        \n",
    "        self.sessions[session_id] = session\n",
    "        \n",
    "        print(f\"\\nSession '{session_name}' created:\")\n",
    "        print(f\"  Session ID: {session_id}\")\n",
    "        print(f\"  Base seed: {base_seed}\")\n",
    "        \n",
    "        return session_id\n",
    "    \n",
    "    def run_experiment_in_session(self, session_id, exp_name, params, seed_offset=0):\n",
    "        \"\"\"Run an experiment within a specific session.\"\"\"\n",
    "        if session_id not in self.sessions:\n",
    "            raise ValueError(f\"Session {session_id} not found\")\n",
    "        \n",
    "        session = self.sessions[session_id]\n",
    "        \n",
    "        # Calculate deterministic seed\n",
    "        experiment_seed = session['base_seed'] + seed_offset\n",
    "        \n",
    "        # Set up reproducibility\n",
    "        if TORCH_AVAILABLE:\n",
    "            srepro.fix_seeds(random=random, np=np, torch=torch, seed=experiment_seed, verbose=False)\n",
    "        else:\n",
    "            srepro.fix_seeds(random=random, np=np, seed=experiment_seed, verbose=False)\n",
    "        \n",
    "        # Generate experiment metadata\n",
    "        exp_id = srepro.gen_id(N=6)\n",
    "        timestamp = srepro.gen_timestamp()\n",
    "        \n",
    "        # Run the actual experiment\n",
    "        results = self._run_simulation(params)\n",
    "        \n",
    "        # Create experiment record\n",
    "        experiment = {\n",
    "            'id': exp_id,\n",
    "            'name': exp_name,\n",
    "            'timestamp': timestamp,\n",
    "            'session_id': session_id,\n",
    "            'seed': experiment_seed,\n",
    "            'seed_offset': seed_offset,\n",
    "            'params': params.copy(),\n",
    "            'results': results,\n",
    "            'version': len(session['experiments']) + 1\n",
    "        }\n",
    "        \n",
    "        session['experiments'].append(experiment)\n",
    "        \n",
    "        print(f\"    Experiment '{exp_name}' (v{experiment['version']}) - Score: {results['metric']:.4f}\")\n",
    "        \n",
    "        return experiment\n",
    "    \n",
    "    def _run_simulation(self, params):\n",
    "        \"\"\"Simulate an experiment.\"\"\"\n",
    "        n_samples = params.get('n_samples', 1000)\n",
    "        complexity = params.get('complexity', 1.0)\n",
    "        noise = params.get('noise', 0.1)\n",
    "        \n",
    "        # Generate synthetic data\n",
    "        x = np.random.uniform(0, 1, n_samples)\n",
    "        y = complexity * np.sin(2 * np.pi * x) + np.random.normal(0, noise, n_samples)\n",
    "        \n",
    "        # Compute metrics\n",
    "        signal_to_noise = np.var(complexity * np.sin(2 * np.pi * x)) / (noise**2 + 1e-8)\n",
    "        correlation = abs(np.corrcoef(x, y)[0, 1])\n",
    "        metric = correlation * np.log(1 + signal_to_noise)\n",
    "        \n",
    "        return {\n",
    "            'metric': metric,\n",
    "            'correlation': correlation,\n",
    "            'signal_to_noise': signal_to_noise,\n",
    "            'data_mean': np.mean(y),\n",
    "            'data_std': np.std(y)\n",
    "        }\n",
    "    \n",
    "    def get_project_summary(self):\n",
    "        \"\"\"Get comprehensive project summary.\"\"\"\n",
    "        total_experiments = sum(len(session['experiments']) for session in self.sessions.values())\n",
    "        \n",
    "        summary = f\"\\nProject: {self.project_name} ({self.project_id})\\n\"\n",
    "        summary += f\"Created: {self.global_config['created_at']}\\n\"\n",
    "        summary += f\"Sessions: {len(self.sessions)}\\n\"\n",
    "        summary += f\"Total experiments: {total_experiments}\\n\"\n",
    "        summary += \"-\" * 50 + \"\\n\"\n",
    "        \n",
    "        for session_id, session in self.sessions.items():\n",
    "            summary += f\"Session: {session['name']} ({session_id[-6:]})\\n\"\n",
    "            summary += f\"  Base seed: {session['base_seed']}\\n\"\n",
    "            summary += f\"  Experiments: {len(session['experiments'])}\\n\"\n",
    "            \n",
    "            if session['experiments']:\n",
    "                best_exp = max(session['experiments'], key=lambda x: x['results']['metric'])\n",
    "                summary += f\"  Best score: {best_exp['results']['metric']:.4f} ({best_exp['name']})\\n\"\n",
    "            \n",
    "            summary += \"\\n\"\n",
    "        \n",
    "        return summary\n",
    "\n",
    "# Create project and run hierarchical experiments\n",
    "manager = ReproducibilityManager(\"Advanced_ML_Study\")\n",
    "\n",
    "# Create different experimental sessions\n",
    "session1 = manager.create_session(\"Hyperparameter_Tuning\", base_seed=1000)\n",
    "session2 = manager.create_session(\"Architecture_Search\", base_seed=2000)\n",
    "session3 = manager.create_session(\"Data_Augmentation\", base_seed=3000)\n",
    "\n",
    "# Run experiments in different sessions\n",
    "print(f\"\\nRunning experiments across sessions:\")\n",
    "\n",
    "# Session 1: Hyperparameter tuning\n",
    "print(f\"\\nSession 1 - Hyperparameter Tuning:\")\n",
    "hp_configs = [\n",
    "    {'n_samples': 1000, 'complexity': 0.5, 'noise': 0.1},\n",
    "    {'n_samples': 1000, 'complexity': 1.0, 'noise': 0.1},\n",
    "    {'n_samples': 1000, 'complexity': 1.5, 'noise': 0.1},\n",
    "    {'n_samples': 1000, 'complexity': 1.0, 'noise': 0.05},\n",
    "]\n",
    "\n",
    "for i, config in enumerate(hp_configs):\n",
    "    manager.run_experiment_in_session(session1, f\"hp_test_{i+1}\", config, seed_offset=i)\n",
    "\n",
    "# Session 2: Architecture search\n",
    "print(f\"\\nSession 2 - Architecture Search:\")\n",
    "arch_configs = [\n",
    "    {'n_samples': 2000, 'complexity': 1.0, 'noise': 0.1},\n",
    "    {'n_samples': 3000, 'complexity': 1.0, 'noise': 0.1},\n",
    "    {'n_samples': 5000, 'complexity': 1.0, 'noise': 0.1},\n",
    "]\n",
    "\n",
    "for i, config in enumerate(arch_configs):\n",
    "    manager.run_experiment_in_session(session2, f\"arch_{i+1}\", config, seed_offset=i*10)\n",
    "\n",
    "# Session 3: Data augmentation\n",
    "print(f\"\\nSession 3 - Data Augmentation:\")\n",
    "aug_configs = [\n",
    "    {'n_samples': 1000, 'complexity': 1.0, 'noise': 0.05},\n",
    "    {'n_samples': 1000, 'complexity': 1.2, 'noise': 0.08},\n",
    "]\n",
    "\n",
    "for i, config in enumerate(aug_configs):\n",
    "    manager.run_experiment_in_session(session3, f\"aug_{i+1}\", config, seed_offset=i*5)\n",
    "\n",
    "# Display comprehensive summary\n",
    "print(manager.get_project_summary())\n",
    "\n",
    "# Test reproducibility across the hierarchy\n",
    "print(\"Reproducibility verification across hierarchy:\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "# Re-run a specific experiment to test reproducibility\n",
    "original_exp = manager.sessions[session1]['experiments'][1]  # Second experiment from session 1\n",
    "print(f\"\\nRe-running experiment: {original_exp['name']}\")\n",
    "print(f\"  Original seed: {original_exp['seed']}\")\n",
    "print(f\"  Original score: {original_exp['results']['metric']:.10f}\")\n",
    "\n",
    "# Re-run with same parameters\n",
    "rerun_exp = manager.run_experiment_in_session(\n",
    "    session1, \n",
    "    f\"{original_exp['name']}_rerun\", \n",
    "    original_exp['params'], \n",
    "    seed_offset=original_exp['seed_offset']\n",
    ")\n",
    "\n",
    "print(f\"  Rerun score:    {rerun_exp['results']['metric']:.10f}\")\n",
    "\n",
    "score_diff = abs(original_exp['results']['metric'] - rerun_exp['results']['metric'])\n",
    "print(f\"  Difference:     {score_diff:.2e}\")\n",
    "print(f\"  Reproducible:   {'✓' if score_diff < 1e-10 else '✗'}\")\n",
    "\n",
    "print(f\"\\nHierarchical reproducibility system successfully demonstrated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook has demonstrated the comprehensive functionality of the `scitex.repro` module:\n",
    "\n",
    "### Core Functions\n",
    "\n",
    "#### Random Seed Management\n",
    "- **`fix_seeds`**: Comprehensive seed fixing across multiple libraries\n",
    "  - Support for Python `random`, `numpy`, `torch`, `tensorflow`, and `os`\n",
    "  - Ensures deterministic behavior across entire computational pipeline\n",
    "  - Verbose reporting of which libraries were configured\n",
    "  - Cross-platform reproducibility\n",
    "\n",
    "#### Unique Identifier Generation\n",
    "- **`gen_id` and `gen_ID`**: Generate unique experiment identifiers\n",
    "  - Timestamp-based prefixes for chronological ordering\n",
    "  - Customizable time formats for different use cases\n",
    "  - Configurable random suffix length\n",
    "  - High uniqueness probability for parallel execution\n",
    "\n",
    "#### Timestamp Generation\n",
    "- **`gen_timestamp` and `timestamp`**: Standardized timestamp generation\n",
    "  - Consistent format: YYYY-MMDD-HHMM\n",
    "  - Suitable for file naming and version control\n",
    "  - Chronologically sortable\n",
    "  - Cross-platform compatibility\n",
    "\n",
    "### Key Features Demonstrated\n",
    "\n",
    "#### Reproducibility Assurance\n",
    "1. **Perfect Determinism**: Same seeds produce identical results across runs\n",
    "2. **Multi-Library Support**: Comprehensive coverage of scientific Python ecosystem\n",
    "3. **Cross-Platform Consistency**: Reproducible results across different systems\n",
    "4. **Seed Differentiation**: Different seeds produce meaningfully different results\n",
    "\n",
    "#### Experiment Management\n",
    "1. **Unique Identification**: Every experiment gets a unique, traceable identifier\n",
    "2. **Temporal Ordering**: Timestamps enable chronological experiment tracking\n",
    "3. **Version Control**: Support for experiment versioning and comparison\n",
    "4. **Hierarchical Organization**: Sessions and projects for complex research workflows\n",
    "\n",
    "#### Scientific Workflow Integration\n",
    "1. **Experiment Classes**: Object-oriented experiment management\n",
    "2. **Tracking Systems**: Comprehensive experiment logging and comparison\n",
    "3. **Reproducibility Verification**: Built-in tools to verify reproducibility\n",
    "4. **Best Practice Patterns**: Templates for reproducible research workflows\n",
    "\n",
    "### Practical Applications\n",
    "\n",
    "#### Research Reproducibility\n",
    "- **Scientific Papers**: Ensure reproducible results for publication\n",
    "- **Collaboration**: Share experiments with guaranteed reproducibility\n",
    "- **Peer Review**: Enable reviewers to reproduce results exactly\n",
    "- **Long-term Archival**: Maintain reproducibility over time\n",
    "\n",
    "#### Machine Learning Workflows\n",
    "- **Model Training**: Reproducible training runs for comparison\n",
    "- **Hyperparameter Tuning**: Systematic exploration with reproducibility\n",
    "- **Ablation Studies**: Controlled experiments with isolated variables\n",
    "- **Benchmark Comparisons**: Fair comparisons across methods\n",
    "\n",
    "#### Data Science Projects\n",
    "- **Analysis Pipelines**: Reproducible data processing and analysis\n",
    "- **A/B Testing**: Controlled experiments with statistical validity\n",
    "- **Model Validation**: Consistent cross-validation and testing\n",
    "- **Production Systems**: Deterministic behavior in deployed models\n",
    "\n",
    "### Best Practices Illustrated\n",
    "\n",
    "#### Seed Management\n",
    "- **Early Initialization**: Set seeds before any random operations\n",
    "- **Comprehensive Coverage**: Include all relevant libraries\n",
    "- **Deterministic Assignment**: Use consistent seed derivation strategies\n",
    "- **Verification Testing**: Always verify reproducibility with test runs\n",
    "\n",
    "#### Experiment Organization\n",
    "- **Unique Identifiers**: Every experiment should have a unique ID\n",
    "- **Metadata Tracking**: Record all relevant experimental parameters\n",
    "- **Hierarchical Structure**: Organize experiments in logical groupings\n",
    "- **Temporal Tracking**: Maintain chronological experiment records\n",
    "\n",
    "#### Documentation and Tracking\n",
    "- **Configuration Recording**: Store all experimental parameters\n",
    "- **Result Documentation**: Comprehensive result recording\n",
    "- **Version Control**: Track experiment versions and iterations\n",
    "- **Reproducibility Testing**: Regular verification of reproducibility\n",
    "\n",
    "### Integration Benefits\n",
    "\n",
    "#### Scientific Computing Ecosystem\n",
    "- **NumPy/SciPy**: Reproducible numerical computations\n",
    "- **PyTorch/TensorFlow**: Deterministic deep learning\n",
    "- **Scikit-learn**: Consistent machine learning results\n",
    "- **Pandas**: Reproducible data analysis\n",
    "\n",
    "#### Research Infrastructure\n",
    "- **Jupyter Notebooks**: Reproducible interactive research\n",
    "- **Version Control**: Git-friendly experiment tracking\n",
    "- **Cluster Computing**: Reproducible distributed experiments\n",
    "- **Continuous Integration**: Automated reproducibility testing\n",
    "\n",
    "The `scitex.repro` module provides essential tools for ensuring reproducible scientific computing, with comprehensive support for the modern Python scientific ecosystem and practical patterns for real-world research workflows."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
