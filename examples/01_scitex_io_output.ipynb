{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0d85ed4",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">An Exception was encountered at '<a href=\"#papermill-error-cell\">In [6]</a>'.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a11150",
   "metadata": {
    "papermill": {
     "duration": 0.004958,
     "end_time": "2025-07-04T01:13:57.210162",
     "exception": false,
     "start_time": "2025-07-04T01:13:57.205204",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# SciTeX I/O Operations Tutorial\n",
    "\n",
    "This comprehensive notebook demonstrates the SciTeX I/O module capabilities, combining features from basic operations, advanced functionality, and complete workflow examples.\n",
    "\n",
    "## Features Covered\n",
    "\n",
    "### Basic I/O Operations\n",
    "* Unified save/load interface with automatic format detection\n",
    "* Symlink creation and management\n",
    "* Basic file operations\n",
    "\n",
    "### Advanced I/O Features\n",
    "* Compression support (gzip, bz2, xz)\n",
    "* HDF5 operations\n",
    "* Configuration file management\n",
    "* Performance comparisons across formats\n",
    "\n",
    "### Complete Workflows\n",
    "* Caching mechanisms\n",
    "* Batch operations\n",
    "* Experiment pipeline integration\n",
    "* Real-world data processing examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d5d3b4e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-04T01:13:57.233348Z",
     "iopub.status.busy": "2025-07-04T01:13:57.232509Z",
     "iopub.status.idle": "2025-07-04T01:13:57.244042Z",
     "shell.execute_reply": "2025-07-04T01:13:57.242721Z"
    },
    "papermill": {
     "duration": 0.018487,
     "end_time": "2025-07-04T01:13:57.245736",
     "exception": false,
     "start_time": "2025-07-04T01:13:57.227249",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SciTeX I/O Tutorial - Ready to begin!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "import scitex\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# Set up example data directory\n",
    "data_dir = Path(\"./io_examples\")\n",
    "data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"SciTeX I/O Tutorial - Ready to begin!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a595a2c0",
   "metadata": {
    "papermill": {
     "duration": 0.004424,
     "end_time": "2025-07-04T01:13:57.255109",
     "exception": false,
     "start_time": "2025-07-04T01:13:57.250685",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Part 1: Basic I/O Operations\n",
    "\n",
    "### 1.1 Unified Save/Load Interface\n",
    "\n",
    "SciTeX provides a unified interface that automatically detects file formats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96d6daf5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-04T01:13:57.266473Z",
     "iopub.status.busy": "2025-07-04T01:13:57.264975Z",
     "iopub.status.idle": "2025-07-04T01:13:57.274617Z",
     "shell.execute_reply": "2025-07-04T01:13:57.273327Z"
    },
    "papermill": {
     "duration": 0.017084,
     "end_time": "2025-07-04T01:13:57.276260",
     "exception": false,
     "start_time": "2025-07-04T01:13:57.259176",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data created:\n",
      "- Array shape: (100, 50)\n",
      "- DataFrame shape: (1000, 3)\n",
      "- Metadata keys: ['experiment', 'date', 'parameters']\n"
     ]
    }
   ],
   "source": [
    "# Create sample data\n",
    "sample_data = {\n",
    "    'array': np.random.randn(100, 50),\n",
    "    'dataframe': pd.DataFrame({\n",
    "        'x': np.random.randn(1000),\n",
    "        'y': np.random.randn(1000),\n",
    "        'category': np.random.choice(['A', 'B', 'C'], 1000)\n",
    "    }),\n",
    "    'metadata': {\n",
    "        'experiment': 'demo',\n",
    "        'date': '2024-01-01',\n",
    "        'parameters': {'alpha': 0.05, 'beta': 0.1}\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"Sample data created:\")\n",
    "print(f\"- Array shape: {sample_data['array'].shape}\")\n",
    "print(f\"- DataFrame shape: {sample_data['dataframe'].shape}\")\n",
    "print(f\"- Metadata keys: {list(sample_data['metadata'].keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5451e36",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-04T01:13:57.287378Z",
     "iopub.status.busy": "2025-07-04T01:13:57.286688Z",
     "iopub.status.idle": "2025-07-04T01:13:57.475032Z",
     "shell.execute_reply": "2025-07-04T01:13:57.473549Z"
    },
    "papermill": {
     "duration": 0.195208,
     "end_time": "2025-07-04T01:13:57.476673",
     "exception": false,
     "start_time": "2025-07-04T01:13:57.281465",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Error occurred while saving: Object of type ndarray is not JSON serializable\n",
      "Debug: Initial script_path = /tmp/ipykernel_48190/3137666053.py\n",
      "Debug: Final spath = /tmp/ipykernel_48190/3137666053_out/io_examples/sample_data.json\n",
      "Debug: specified_path type = <class 'str'>\n",
      "Debug: specified_path = io_examples/sample_data.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m\n",
      "Saved to: /tmp/ipykernel_48190/3137666053_out/io_examples/sample_data.pkl (57.5 KiB)\u001b[0m\n",
      "\u2713 Saved data in PKL format\n",
      "\u2713 Saved data in JSON format\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m\n",
      "Saved to: /tmp/ipykernel_48190/3137666053_out/io_examples/sample_array.npy (39.2 KiB)\u001b[0m\n",
      "\u2713 Saved data in NPY format\n",
      "\u001b[93m\n",
      "Saved to: /tmp/ipykernel_48190/3137666053_out/io_examples/sample_dataframe.csv (40.3 KiB)\u001b[0m\n",
      "\u2713 Saved data in CSV format\n"
     ]
    }
   ],
   "source": [
    "# Save data in multiple formats - automatic format detection\n",
    "formats_to_test = ['pkl', 'json', 'npy', 'csv']\n",
    "\n",
    "for fmt in formats_to_test:\n",
    "    try:\n",
    "        if fmt == 'npy':\n",
    "            # For .npy, save just the array\n",
    "            scitex.io.save(sample_data['array'], data_dir / f\"sample_array.{fmt}\")\n",
    "        elif fmt == 'csv':\n",
    "            # For .csv, save just the dataframe\n",
    "            scitex.io.save(sample_data['dataframe'], data_dir / f\"sample_dataframe.{fmt}\")\n",
    "        else:\n",
    "            # For pkl and json, save the full dictionary\n",
    "            scitex.io.save(sample_data, data_dir / f\"sample_data.{fmt}\")\n",
    "        print(f\"\u2713 Saved data in {fmt.upper()} format\")\n",
    "    except Exception as e:\n",
    "        print(f\"\u2717 Failed to save in {fmt.upper()} format: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a318c1f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-04T01:13:57.489033Z",
     "iopub.status.busy": "2025-07-04T01:13:57.488234Z",
     "iopub.status.idle": "2025-07-04T01:13:57.497837Z",
     "shell.execute_reply": "2025-07-04T01:13:57.495238Z"
    },
    "papermill": {
     "duration": 0.018734,
     "end_time": "2025-07-04T01:13:57.500408",
     "exception": false,
     "start_time": "2025-07-04T01:13:57.481674",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load data back - automatic format detection\n",
    "loaded_data = {}\n",
    "\n",
    "# Load pickle data (full dictionary)\n",
    "if (data_dir / \"sample_data.pkl\").exists():\n",
    "    loaded_data['from_pkl'] = scitex.io.load(data_dir / \"sample_data.pkl\")\n",
    "    print(\"\u2713 Loaded data from pickle\")\n",
    "\n",
    "# Load numpy array\n",
    "if (data_dir / \"sample_array.npy\").exists():\n",
    "    loaded_data['from_npy'] = scitex.io.load(data_dir / \"sample_array.npy\")\n",
    "    print(f\"\u2713 Loaded array from npy: shape {loaded_data['from_npy'].shape}\")\n",
    "\n",
    "# Load CSV dataframe\n",
    "if (data_dir / \"sample_dataframe.csv\").exists():\n",
    "    loaded_data['from_csv'] = scitex.io.load(data_dir / \"sample_dataframe.csv\")\n",
    "    print(f\"\u2713 Loaded dataframe from csv: shape {loaded_data['from_csv'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306cdf3a",
   "metadata": {
    "papermill": {
     "duration": 0.005324,
     "end_time": "2025-07-04T01:13:57.510975",
     "exception": false,
     "start_time": "2025-07-04T01:13:57.505651",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 1.2 Symlink Creation and Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "798a8656",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-04T01:13:57.523192Z",
     "iopub.status.busy": "2025-07-04T01:13:57.522414Z",
     "iopub.status.idle": "2025-07-04T01:13:57.531063Z",
     "shell.execute_reply": "2025-07-04T01:13:57.529672Z"
    },
    "papermill": {
     "duration": 0.016927,
     "end_time": "2025-07-04T01:13:57.533182",
     "exception": false,
     "start_time": "2025-07-04T01:13:57.516255",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original file not found for symlink creation\n"
     ]
    }
   ],
   "source": [
    "# Create symlinks for easy access\n",
    "symlink_dir = data_dir / \"symlinks\"\n",
    "symlink_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Create symlinks to our saved files\n",
    "original_file = data_dir / \"sample_data.pkl\"\n",
    "if original_file.exists():\n",
    "    symlink_path = symlink_dir / \"latest_data.pkl\"\n",
    "    \n",
    "    # Remove existing symlink if it exists\n",
    "    if symlink_path.is_symlink():\n",
    "        symlink_path.unlink()\n",
    "    \n",
    "    # Create new symlink\n",
    "    symlink_path.symlink_to(original_file.resolve())\n",
    "    print(f\"\u2713 Created symlink: {symlink_path} -> {original_file}\")\n",
    "    \n",
    "    # Verify symlink works\n",
    "    symlink_data = scitex.io.load(symlink_path)\n",
    "    print(f\"\u2713 Successfully loaded data through symlink\")\n",
    "    print(f\"  Array shape: {symlink_data['array'].shape}\")\n",
    "else:\n",
    "    print(\"Original file not found for symlink creation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5d6325",
   "metadata": {
    "papermill": {
     "duration": 0.005043,
     "end_time": "2025-07-04T01:13:57.543085",
     "exception": false,
     "start_time": "2025-07-04T01:13:57.538042",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Part 2: Advanced I/O Features\n",
    "\n",
    "### 2.1 Compression Support"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a2a6ad",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span id=\"papermill-error-cell\" style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">Execution using papermill encountered an exception here and stopped:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a192e31",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-04T01:13:57.555139Z",
     "iopub.status.busy": "2025-07-04T01:13:57.553764Z",
     "iopub.status.idle": "2025-07-04T01:13:58.021765Z",
     "shell.execute_reply": "2025-07-04T01:13:58.020264Z"
    },
    "papermill": {
     "duration": 0.47565,
     "end_time": "2025-07-04T01:13:58.023175",
     "exception": true,
     "start_time": "2025-07-04T01:13:57.547525",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m\n",
      "Saved to: /tmp/ipykernel_48190/1001136160_out/io_examples/large_data.pkl (7.7 MiB)\u001b[0m\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'io_examples/large_data.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m uncompressed_file = data_dir / \u001b[33m\"\u001b[39m\u001b[33mlarge_data.pkl\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     12\u001b[39m scitex.io.save(large_data, uncompressed_file)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m file_sizes[\u001b[33m'\u001b[39m\u001b[33muncompressed\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43muncompressed_file\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.st_size\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Save with compression\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m compression \u001b[38;5;129;01min\u001b[39;00m compression_formats:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/pathlib.py:1014\u001b[39m, in \u001b[36mPath.stat\u001b[39m\u001b[34m(self, follow_symlinks)\u001b[39m\n\u001b[32m   1009\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstat\u001b[39m(\u001b[38;5;28mself\u001b[39m, *, follow_symlinks=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m   1010\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1011\u001b[39m \u001b[33;03m    Return the result of the stat() system call on this path, like\u001b[39;00m\n\u001b[32m   1012\u001b[39m \u001b[33;03m    os.stat() does.\u001b[39;00m\n\u001b[32m   1013\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m os.stat(\u001b[38;5;28mself\u001b[39m, follow_symlinks=follow_symlinks)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'io_examples/large_data.pkl'"
     ]
    }
   ],
   "source": [
    "# Test compression formats\n",
    "compression_formats = ['gzip', 'bz2', 'xz']\n",
    "large_data = {\n",
    "    'large_array': np.random.randn(1000, 1000),\n",
    "    'text_data': 'This is a test string that will be repeated many times. ' * 1000\n",
    "}\n",
    "\n",
    "file_sizes = {}\n",
    "\n",
    "# Save uncompressed\n",
    "uncompressed_file = data_dir / \"large_data.pkl\"\n",
    "scitex.io.save(large_data, uncompressed_file)\n",
    "file_sizes['uncompressed'] = uncompressed_file.stat().st_size\n",
    "\n",
    "# Save with compression\n",
    "for compression in compression_formats:\n",
    "    try:\n",
    "        compressed_file = data_dir / f\"large_data.pkl.{compression}\"\n",
    "        scitex.io.save(large_data, compressed_file, compression=compression)\n",
    "        file_sizes[compression] = compressed_file.stat().st_size\n",
    "        print(f\"\u2713 Saved with {compression} compression\")\n",
    "    except Exception as e:\n",
    "        print(f\"\u2717 Failed to save with {compression}: {e}\")\n",
    "\n",
    "# Compare file sizes\n",
    "print(\"\\nFile size comparison:\")\n",
    "for format_name, size in file_sizes.items():\n",
    "    size_mb = size / (1024 * 1024)\n",
    "    if format_name != 'uncompressed':\n",
    "        compression_ratio = file_sizes['uncompressed'] / size\n",
    "        print(f\"{format_name:12}: {size_mb:.2f} MB (compression ratio: {compression_ratio:.1f}x)\")\n",
    "    else:\n",
    "        print(f\"{format_name:12}: {size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70b54a9",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 2.2 HDF5 Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679c049d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# HDF5 operations for hierarchical data\n",
    "try:\n",
    "    import h5py\n",
    "    \n",
    "    # Create hierarchical data structure\n",
    "    hdf5_data = {\n",
    "        'experiment_1': {\n",
    "            'raw_data': np.random.randn(500, 100),\n",
    "            'processed_data': np.random.randn(500, 50),\n",
    "            'metadata': {\n",
    "                'sampling_rate': 1000,\n",
    "                'channels': 100\n",
    "            }\n",
    "        },\n",
    "        'experiment_2': {\n",
    "            'raw_data': np.random.randn(300, 100),\n",
    "            'processed_data': np.random.randn(300, 50),\n",
    "            'metadata': {\n",
    "                'sampling_rate': 500,\n",
    "                'channels': 100\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save as HDF5\n",
    "    hdf5_file = data_dir / \"experiments.h5\"\n",
    "    scitex.io.save(hdf5_data, hdf5_file)\n",
    "    print(f\"\u2713 Saved hierarchical data to HDF5: {hdf5_file}\")\n",
    "    \n",
    "    # Load HDF5 data\n",
    "    loaded_hdf5 = scitex.io.load(hdf5_file)\n",
    "    print(f\"\u2713 Loaded HDF5 data with {len(loaded_hdf5)} experiments\")\n",
    "    \n",
    "    for exp_name, exp_data in loaded_hdf5.items():\n",
    "        print(f\"  {exp_name}: raw_data shape {exp_data['raw_data'].shape}\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"h5py not available - skipping HDF5 examples\")\n",
    "except Exception as e:\n",
    "    print(f\"HDF5 operations failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6db8d00",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 2.3 Performance Comparison Across Formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79248f21",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Performance benchmark for different formats\n",
    "benchmark_data = {\n",
    "    'numeric_array': np.random.randn(1000, 100),\n",
    "    'dataframe': pd.DataFrame({\n",
    "        'col_' + str(i): np.random.randn(5000) \n",
    "        for i in range(20)\n",
    "    }),\n",
    "    'mixed_data': {\n",
    "        'numbers': list(range(10000)),\n",
    "        'strings': [f'item_{i}' for i in range(1000)],\n",
    "        'nested': {'a': [1, 2, 3], 'b': {'c': 4, 'd': 5}}\n",
    "    }\n",
    "}\n",
    "\n",
    "formats_to_benchmark = ['pkl', 'json', 'h5']\n",
    "benchmark_results = {}\n",
    "\n",
    "for fmt in formats_to_benchmark:\n",
    "    try:\n",
    "        test_file = data_dir / f\"benchmark.{fmt}\"\n",
    "        \n",
    "        # Time save operation\n",
    "        start_time = time.time()\n",
    "        if fmt == 'json':\n",
    "            # JSON can't handle numpy arrays directly\n",
    "            json_safe_data = {\n",
    "                'numeric_array': benchmark_data['numeric_array'].tolist(),\n",
    "                'mixed_data': benchmark_data['mixed_data']\n",
    "            }\n",
    "            scitex.io.save(json_safe_data, test_file)\n",
    "        else:\n",
    "            scitex.io.save(benchmark_data, test_file)\n",
    "        save_time = time.time() - start_time\n",
    "        \n",
    "        # Time load operation\n",
    "        start_time = time.time()\n",
    "        loaded = scitex.io.load(test_file)\n",
    "        load_time = time.time() - start_time\n",
    "        \n",
    "        # Get file size\n",
    "        file_size = test_file.stat().st_size / (1024 * 1024)  # MB\n",
    "        \n",
    "        benchmark_results[fmt] = {\n",
    "            'save_time': save_time,\n",
    "            'load_time': load_time,\n",
    "            'file_size_mb': file_size\n",
    "        }\n",
    "        \n",
    "        print(f\"\u2713 {fmt.upper()}: Save {save_time:.3f}s, Load {load_time:.3f}s, Size {file_size:.2f}MB\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\u2717 {fmt.upper()} benchmark failed: {e}\")\n",
    "\n",
    "# Visualize benchmark results\n",
    "if benchmark_results:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    formats = list(benchmark_results.keys())\n",
    "    save_times = [benchmark_results[fmt]['save_time'] for fmt in formats]\n",
    "    load_times = [benchmark_results[fmt]['load_time'] for fmt in formats]\n",
    "    file_sizes = [benchmark_results[fmt]['file_size_mb'] for fmt in formats]\n",
    "    \n",
    "    axes[0].bar(formats, save_times)\n",
    "    axes[0].set_title('Save Time (seconds)')\n",
    "    axes[0].set_ylabel('Time (s)')\n",
    "    \n",
    "    axes[1].bar(formats, load_times)\n",
    "    axes[1].set_title('Load Time (seconds)')\n",
    "    axes[1].set_ylabel('Time (s)')\n",
    "    \n",
    "    axes[2].bar(formats, file_sizes)\n",
    "    axes[2].set_title('File Size (MB)')\n",
    "    axes[2].set_ylabel('Size (MB)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51b4a5f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Part 3: Complete Workflows and Caching\n",
    "\n",
    "### 3.1 Caching Mechanisms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46390c7a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Demonstrate caching for expensive operations\n",
    "cache_dir = data_dir / \"cache\"\n",
    "cache_dir.mkdir(exist_ok=True)\n",
    "\n",
    "@scitex.io.cache_result(cache_dir / \"expensive_computation.pkl\")\n",
    "def expensive_computation(n_samples=10000, n_features=100):\n",
    "    \"\"\"Simulate an expensive computation that we want to cache.\"\"\"\n",
    "    print(f\"Performing expensive computation with {n_samples} samples...\")\n",
    "    time.sleep(1)  # Simulate computation time\n",
    "    \n",
    "    # Generate some \"computed\" result\n",
    "    data = np.random.randn(n_samples, n_features)\n",
    "    features = np.mean(data, axis=0)\n",
    "    correlations = np.corrcoef(data.T)\n",
    "    \n",
    "    return {\n",
    "        'raw_data': data,\n",
    "        'features': features,\n",
    "        'correlations': correlations,\n",
    "        'metadata': {\n",
    "            'n_samples': n_samples,\n",
    "            'n_features': n_features,\n",
    "            'computed_at': time.time()\n",
    "        }\n",
    "    }\n",
    "\n",
    "# First call - will compute and cache\n",
    "print(\"First call (will compute):\")\n",
    "start_time = time.time()\n",
    "result1 = expensive_computation(5000, 50)\n",
    "first_call_time = time.time() - start_time\n",
    "print(f\"First call took {first_call_time:.2f} seconds\")\n",
    "\n",
    "# Second call - will load from cache\n",
    "print(\"\\nSecond call (will load from cache):\")\n",
    "start_time = time.time()\n",
    "result2 = expensive_computation(5000, 50)\n",
    "second_call_time = time.time() - start_time\n",
    "print(f\"Second call took {second_call_time:.2f} seconds\")\n",
    "\n",
    "print(f\"\\nSpeedup from caching: {first_call_time/second_call_time:.1f}x\")\n",
    "print(f\"Results identical: {np.array_equal(result1['features'], result2['features'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7219c91",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 3.2 Batch Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afac565f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Batch file operations\n",
    "batch_dir = data_dir / \"batch_processing\"\n",
    "batch_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Create multiple data files for batch processing\n",
    "batch_files = []\n",
    "for i in range(5):\n",
    "    batch_data = {\n",
    "        'id': i,\n",
    "        'data': np.random.randn(100, 10),\n",
    "        'labels': np.random.choice(['A', 'B', 'C'], 100),\n",
    "        'timestamp': time.time() + i\n",
    "    }\n",
    "    \n",
    "    filename = batch_dir / f\"batch_data_{i:03d}.pkl\"\n",
    "    scitex.io.save(batch_data, filename)\n",
    "    batch_files.append(filename)\n",
    "\n",
    "print(f\"Created {len(batch_files)} batch files\")\n",
    "\n",
    "# Batch loading with pattern matching\n",
    "pattern = batch_dir / \"batch_data_*.pkl\"\n",
    "all_batch_files = list(batch_dir.glob(\"batch_data_*.pkl\"))\n",
    "print(f\"Found {len(all_batch_files)} files matching pattern\")\n",
    "\n",
    "# Load and combine all batch files\n",
    "combined_data = []\n",
    "for file_path in sorted(all_batch_files):\n",
    "    data = scitex.io.load(file_path)\n",
    "    combined_data.append(data)\n",
    "\n",
    "print(f\"Loaded {len(combined_data)} batch files\")\n",
    "print(f\"Total data points: {sum(len(d['data']) for d in combined_data)}\")\n",
    "\n",
    "# Combine all data into single arrays\n",
    "all_data = np.vstack([d['data'] for d in combined_data])\n",
    "all_labels = np.hstack([d['labels'] for d in combined_data])\n",
    "\n",
    "print(f\"Combined data shape: {all_data.shape}\")\n",
    "print(f\"Label distribution: {dict(zip(*np.unique(all_labels, return_counts=True)))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a875dac",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 3.3 Experiment Pipeline Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fc5e5b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Complete experiment pipeline with I/O\n",
    "class ExperimentPipeline:\n",
    "    def __init__(self, experiment_name, output_dir):\n",
    "        self.experiment_name = experiment_name\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.output_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Create subdirectories\n",
    "        (self.output_dir / \"raw\").mkdir(exist_ok=True)\n",
    "        (self.output_dir / \"processed\").mkdir(exist_ok=True)\n",
    "        (self.output_dir / \"results\").mkdir(exist_ok=True)\n",
    "        \n",
    "    def generate_data(self, n_samples=1000, noise_level=0.1):\n",
    "        \"\"\"Generate synthetic experimental data.\"\"\"\n",
    "        print(f\"Generating data for {self.experiment_name}...\")\n",
    "        \n",
    "        # Simulate different experimental conditions\n",
    "        conditions = ['control', 'treatment_A', 'treatment_B']\n",
    "        raw_data = {}\n",
    "        \n",
    "        for condition in conditions:\n",
    "            # Different signal patterns for each condition\n",
    "            if condition == 'control':\n",
    "                signal = np.sin(np.linspace(0, 4*np.pi, n_samples))\n",
    "            elif condition == 'treatment_A':\n",
    "                signal = np.sin(np.linspace(0, 4*np.pi, n_samples)) * 1.5\n",
    "            else:  # treatment_B\n",
    "                signal = np.sin(np.linspace(0, 6*np.pi, n_samples)) * 0.8\n",
    "            \n",
    "            # Add noise\n",
    "            noisy_signal = signal + np.random.normal(0, noise_level, n_samples)\n",
    "            \n",
    "            raw_data[condition] = {\n",
    "                'signal': noisy_signal,\n",
    "                'time': np.linspace(0, 10, n_samples),\n",
    "                'metadata': {\n",
    "                    'condition': condition,\n",
    "                    'n_samples': n_samples,\n",
    "                    'noise_level': noise_level\n",
    "                }\n",
    "            }\n",
    "        \n",
    "        # Save raw data\n",
    "        raw_file = self.output_dir / \"raw\" / \"raw_data.pkl\"\n",
    "        scitex.io.save(raw_data, raw_file)\n",
    "        print(f\"\u2713 Raw data saved to {raw_file}\")\n",
    "        \n",
    "        return raw_data\n",
    "    \n",
    "    def process_data(self, raw_data=None):\n",
    "        \"\"\"Process the raw experimental data.\"\"\"\n",
    "        if raw_data is None:\n",
    "            # Load from file\n",
    "            raw_file = self.output_dir / \"raw\" / \"raw_data.pkl\"\n",
    "            raw_data = scitex.io.load(raw_file)\n",
    "        \n",
    "        print(\"Processing experimental data...\")\n",
    "        processed_data = {}\n",
    "        \n",
    "        for condition, data in raw_data.items():\n",
    "            signal = data['signal']\n",
    "            time = data['time']\n",
    "            \n",
    "            # Apply processing steps\n",
    "            # 1. Smoothing\n",
    "            from scipy import ndimage\n",
    "            smoothed = ndimage.gaussian_filter1d(signal, sigma=2)\n",
    "            \n",
    "            # 2. Feature extraction\n",
    "            features = {\n",
    "                'mean': np.mean(smoothed),\n",
    "                'std': np.std(smoothed),\n",
    "                'max': np.max(smoothed),\n",
    "                'min': np.min(smoothed),\n",
    "                'peak_to_peak': np.ptp(smoothed)\n",
    "            }\n",
    "            \n",
    "            # 3. Spectral analysis\n",
    "            fft = np.fft.fft(smoothed)\n",
    "            freqs = np.fft.fftfreq(len(smoothed), d=time[1]-time[0])\n",
    "            power_spectrum = np.abs(fft)**2\n",
    "            \n",
    "            processed_data[condition] = {\n",
    "                'original_signal': signal,\n",
    "                'smoothed_signal': smoothed,\n",
    "                'features': features,\n",
    "                'power_spectrum': power_spectrum[:len(power_spectrum)//2],\n",
    "                'frequencies': freqs[:len(freqs)//2],\n",
    "                'time': time,\n",
    "                'metadata': data['metadata']\n",
    "            }\n",
    "        \n",
    "        # Save processed data\n",
    "        processed_file = self.output_dir / \"processed\" / \"processed_data.pkl\"\n",
    "        scitex.io.save(processed_data, processed_file)\n",
    "        print(f\"\u2713 Processed data saved to {processed_file}\")\n",
    "        \n",
    "        return processed_data\n",
    "    \n",
    "    def analyze_results(self, processed_data=None):\n",
    "        \"\"\"Analyze processed data and generate results.\"\"\"\n",
    "        if processed_data is None:\n",
    "            processed_file = self.output_dir / \"processed\" / \"processed_data.pkl\"\n",
    "            processed_data = scitex.io.load(processed_file)\n",
    "        \n",
    "        print(\"Analyzing results...\")\n",
    "        \n",
    "        # Statistical analysis\n",
    "        results = {\n",
    "            'summary_statistics': {},\n",
    "            'comparisons': {},\n",
    "            'figures': {}\n",
    "        }\n",
    "        \n",
    "        # Extract features for all conditions\n",
    "        all_features = {}\n",
    "        for condition, data in processed_data.items():\n",
    "            all_features[condition] = data['features']\n",
    "            results['summary_statistics'][condition] = data['features']\n",
    "        \n",
    "        # Generate comparison plots\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "        fig.suptitle(f'Experiment Results: {self.experiment_name}')\n",
    "        \n",
    "        # Plot 1: Original signals\n",
    "        for condition, data in processed_data.items():\n",
    "            axes[0, 0].plot(data['time'], data['smoothed_signal'], label=condition)\n",
    "        axes[0, 0].set_title('Processed Signals')\n",
    "        axes[0, 0].set_xlabel('Time')\n",
    "        axes[0, 0].set_ylabel('Amplitude')\n",
    "        axes[0, 0].legend()\n",
    "        \n",
    "        # Plot 2: Feature comparison\n",
    "        feature_names = list(all_features['control'].keys())\n",
    "        x_pos = np.arange(len(feature_names))\n",
    "        width = 0.25\n",
    "        \n",
    "        for i, condition in enumerate(all_features.keys()):\n",
    "            values = [all_features[condition][feat] for feat in feature_names]\n",
    "            axes[0, 1].bar(x_pos + i*width, values, width, label=condition)\n",
    "        \n",
    "        axes[0, 1].set_title('Feature Comparison')\n",
    "        axes[0, 1].set_xlabel('Features')\n",
    "        axes[0, 1].set_ylabel('Value')\n",
    "        axes[0, 1].set_xticks(x_pos + width)\n",
    "        axes[0, 1].set_xticklabels(feature_names, rotation=45)\n",
    "        axes[0, 1].legend()\n",
    "        \n",
    "        # Plot 3: Power spectra\n",
    "        for condition, data in processed_data.items():\n",
    "            axes[1, 0].semilogy(data['frequencies'], data['power_spectrum'], label=condition)\n",
    "        axes[1, 0].set_title('Power Spectra')\n",
    "        axes[1, 0].set_xlabel('Frequency (Hz)')\n",
    "        axes[1, 0].set_ylabel('Power')\n",
    "        axes[1, 0].legend()\n",
    "        \n",
    "        # Plot 4: Summary statistics\n",
    "        conditions = list(all_features.keys())\n",
    "        means = [all_features[cond]['mean'] for cond in conditions]\n",
    "        stds = [all_features[cond]['std'] for cond in conditions]\n",
    "        \n",
    "        axes[1, 1].bar(conditions, means, yerr=stds, capsize=5)\n",
    "        axes[1, 1].set_title('Mean \u00b1 Std by Condition')\n",
    "        axes[1, 1].set_ylabel('Signal Mean')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save figure\n",
    "        figure_file = self.output_dir / \"results\" / \"analysis_summary.png\"\n",
    "        plt.savefig(figure_file, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        results['figures']['summary_plot'] = str(figure_file)\n",
    "        \n",
    "        # Save results\n",
    "        results_file = self.output_dir / \"results\" / \"analysis_results.pkl\"\n",
    "        scitex.io.save(results, results_file)\n",
    "        print(f\"\u2713 Analysis results saved to {results_file}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def run_complete_pipeline(self, n_samples=1000, noise_level=0.1):\n",
    "        \"\"\"Run the complete experiment pipeline.\"\"\"\n",
    "        print(f\"\\n=== Running Complete Pipeline: {self.experiment_name} ===\")\n",
    "        \n",
    "        # Step 1: Generate data\n",
    "        raw_data = self.generate_data(n_samples, noise_level)\n",
    "        \n",
    "        # Step 2: Process data\n",
    "        processed_data = self.process_data(raw_data)\n",
    "        \n",
    "        # Step 3: Analyze results\n",
    "        results = self.analyze_results(processed_data)\n",
    "        \n",
    "        print(f\"\\n=== Pipeline Complete ===\")\n",
    "        print(f\"Output directory: {self.output_dir}\")\n",
    "        print(f\"Files created:\")\n",
    "        for file in self.output_dir.rglob(\"*\"):\n",
    "            if file.is_file():\n",
    "                print(f\"  {file.relative_to(self.output_dir)}\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Run the complete pipeline\n",
    "pipeline = ExperimentPipeline(\n",
    "    experiment_name=\"SciTeX_IO_Demo\",\n",
    "    output_dir=data_dir / \"experiment_pipeline\"\n",
    ")\n",
    "\n",
    "final_results = pipeline.run_complete_pipeline(n_samples=500, noise_level=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e20aeb",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Part 4: Configuration Management and Advanced Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4223ec",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configuration file management\n",
    "config_dir = data_dir / \"configs\"\n",
    "config_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Create experiment configurations\n",
    "configs = {\n",
    "    'default': {\n",
    "        'data_params': {\n",
    "            'n_samples': 1000,\n",
    "            'noise_level': 0.1,\n",
    "            'sampling_rate': 100\n",
    "        },\n",
    "        'processing_params': {\n",
    "            'smoothing_sigma': 2.0,\n",
    "            'filter_cutoff': 0.5\n",
    "        },\n",
    "        'analysis_params': {\n",
    "            'significance_level': 0.05,\n",
    "            'bootstrap_iterations': 1000\n",
    "        }\n",
    "    },\n",
    "    'high_resolution': {\n",
    "        'data_params': {\n",
    "            'n_samples': 5000,\n",
    "            'noise_level': 0.05,\n",
    "            'sampling_rate': 1000\n",
    "        },\n",
    "        'processing_params': {\n",
    "            'smoothing_sigma': 1.0,\n",
    "            'filter_cutoff': 0.1\n",
    "        },\n",
    "        'analysis_params': {\n",
    "            'significance_level': 0.01,\n",
    "            'bootstrap_iterations': 5000\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save configurations in different formats\n",
    "for config_name, config_data in configs.items():\n",
    "    # Save as JSON (human-readable)\n",
    "    json_file = config_dir / f\"{config_name}_config.json\"\n",
    "    scitex.io.save(config_data, json_file)\n",
    "    \n",
    "    # Save as YAML (if available)\n",
    "    try:\n",
    "        yaml_file = config_dir / f\"{config_name}_config.yaml\"\n",
    "        scitex.io.save(config_data, yaml_file)\n",
    "        print(f\"\u2713 Saved {config_name} config in JSON and YAML formats\")\n",
    "    except Exception:\n",
    "        print(f\"\u2713 Saved {config_name} config in JSON format (YAML not available)\")\n",
    "\n",
    "# Load and use configuration\n",
    "loaded_config = scitex.io.load(config_dir / \"high_resolution_config.json\")\n",
    "print(f\"\\nLoaded configuration:\")\n",
    "for section, params in loaded_config.items():\n",
    "    print(f\"  {section}:\")\n",
    "    for key, value in params.items():\n",
    "        print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf33a63",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Summary and Best Practices\n",
    "\n",
    "This tutorial demonstrated the comprehensive I/O capabilities of the SciTeX library:\n",
    "\n",
    "### Key Features Covered:\n",
    "1. **Unified Interface**: Automatic format detection for save/load operations\n",
    "2. **Multiple Formats**: Support for pickle, JSON, HDF5, CSV, NumPy, and compressed formats\n",
    "3. **Performance Optimization**: Caching, compression, and format-specific optimizations\n",
    "4. **Batch Operations**: Efficient handling of multiple files\n",
    "5. **Complete Workflows**: Integration with experimental pipelines\n",
    "6. **Configuration Management**: Flexible configuration file handling\n",
    "\n",
    "### Best Practices:\n",
    "- Use **pickle** for complex Python objects and mixed data types\n",
    "- Use **HDF5** for large, hierarchical datasets\n",
    "- Use **JSON/YAML** for human-readable configuration files\n",
    "- Apply **compression** for large files when storage space is limited\n",
    "- Implement **caching** for expensive computations\n",
    "- Organize data with **clear directory structures**\n",
    "- Use **symlinks** for easy access to frequently used files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b587d6c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cleanup - remove example files (optional)\n",
    "import shutil\n",
    "\n",
    "# cleanup = \"n\"  # input(\"Clean up example files? (y/n): \").lower().startswith('y')\n",
    "if cleanup:\n",
    "    shutil.rmtree(data_dir)\n",
    "    print(\"\u2713 Example files cleaned up\")\n",
    "else:\n",
    "    print(f\"Example files preserved in: {data_dir}\")\n",
    "    print(f\"Total size: {sum(f.stat().st_size for f in data_dir.rglob('*') if f.is_file()) / (1024*1024):.1f} MB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 10.183674,
   "end_time": "2025-07-04T01:14:00.750992",
   "environment_variables": {},
   "exception": true,
   "input_path": "01_scitex_io.ipynb",
   "output_path": "01_scitex_io_output.ipynb",
   "parameters": {},
   "start_time": "2025-07-04T01:13:50.567318",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}