================================================================================
Copied 17 files at 2025-07-24 15:11:22
================================================================================

================================================================================
File: /home/ywatanabe/proj/SciTeX-Code/src/scitex/scholar/config/config_template_minimal.yaml
Relative: ../../../../SciTeX-Code/src/scitex/scholar/config/config_template_minimal.yaml
Size: 382 bytes, Lines: 7, Words: ~63
================================================================================
# SciTeX Scholar Configuration (Minimal Template)
# Copy to ~/.scitex/scholar/config.yaml and add your API keys

# Required for PubMed searches
pubmed_email: "your.email@example.com"

# Optional but recommended for better features
semantic_scholar_api_key: null  # Get free at: https://www.semanticscholar.org/product/api

# That's it! All other settings will use sensible defaults.

================================================================================
File: /home/ywatanabe/proj/SciTeX-Code/src/scitex/scholar/config/default_config.yaml
Relative: ../../../../SciTeX-Code/src/scitex/scholar/config/default_config.yaml
Size: 3841 bytes, Lines: 83, Words: ~640
================================================================================
# SciTeX Scholar Default Configuration
# 
# This file shows all available configuration options with their default values.
# To use custom configuration:
#   1. Copy this file to one of these locations:
#      - ~/.scitex/scholar/config.yaml (recommended)
#      - ./scholar_config.yaml (project-specific)
#      - Set SCITEX_SCHOLAR_CONFIG environment variable to your config path
#   2. Modify the values as needed
#   3. Remove or comment out any settings you want to keep as default
#
# Note: API keys and sensitive data should be set via environment variables
# or in your personal config file, not committed to version control.

# ============================================================================
# API AUTHENTICATION
# ============================================================================

# Semantic Scholar API key (get free key at: https://www.semanticscholar.org/product/api)
# Can also set via: SCITEX_SEMANTIC_SCHOLAR_API_KEY
semantic_scholar_api_key: null

# CrossRef API key (optional - for higher rate limits)
# Can also set via: SCITEX_CROSSREF_API_KEY
crossref_api_key: null

# Email for PubMed API access (required for PubMed searches)
# Can also set via: SCITEX_PUBMED_EMAIL
pubmed_email: "research@example.com"

# Email for CrossRef API access
# Can also set via: SCITEX_CROSSREF_EMAIL
crossref_email: "research@example.com"

# ============================================================================
# FEATURE SETTINGS
# ============================================================================

# Automatically enrich papers with impact factors and citation counts
# Can also set via: SCITEX_SCHOLAR_AUTO_ENRICH
enable_auto_enrich: true

# Use impact_factor package for real journal impact factors (2024 JCR data)
# Requires: pip install impact-factor
# Can also set via: SCITEX_USE_IMPACT_FACTOR_PACKAGE
use_impact_factor_package: true

# ============================================================================
# SEARCH CONFIGURATION
# ============================================================================

# Default sources to search (can override per search)
# Available: ["pubmed", "arxiv", "semantic_scholar"]
default_search_sources:
  - pubmed
  - arxiv
  - semantic_scholar

# Default number of search results
default_search_limit: 20

# ============================================================================
# PDF MANAGEMENT
# ============================================================================

# Directory for storing downloaded PDFs
# Can also set via: SCITEX_PDF_DIR
pdf_dir: "~/.scitex/scholar/pdfs"

# Enable PDF text extraction features
enable_pdf_extraction: true

# ============================================================================
# PERFORMANCE SETTINGS
# ============================================================================

# Maximum number of parallel API requests
# Be respectful to API providers - don't set too high
max_parallel_requests: 3

# Request timeout in seconds
request_timeout: 30

# Size of LRU cache for journal lookups
cache_size: 1000

# ============================================================================
# ADVANCED SETTINGS
# ============================================================================

# SSL certificate verification (disable only for debugging)
verify_ssl: true

# User agent string for API requests
# This identifies your software to API providers (format: name/version)
# Note: This is NOT the package version - it's just an HTTP identifier
user_agent: "SciTeX-Scholar/1.0"

# ============================================================================
# LOGGING
# ============================================================================

# Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
# log_level: "INFO"

# Log file path (null for console only)
# log_file: null

================================================================================
File: /home/ywatanabe/proj/SciTeX-Code/src/scitex/scholar/_Config.py
Relative: ../../../../SciTeX-Code/src/scitex/scholar/_Config.py
Size: 15271 bytes, Lines: 386, Words: ~2545
================================================================================
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Timestamp: "2025-07-23 16:28:00 (ywatanabe)"
# File: /home/ywatanabe/proj/scitex_repo/src/scitex/scholar/_Config.py
# ----------------------------------------
import os
__FILE__ = (
    "./src/scitex/scholar/_Config.py"
)
__DIR__ = os.path.dirname(__FILE__)
# ----------------------------------------

"""
Configuration management for SciTeX Scholar.

Provides centralized configuration with environment variable support
and sensible defaults.

Priority order for configuration values:
1. Direct parameter specification (highest priority)
2. Configuration file (YAML)
3. Environment variables (SCITEX_* prefix)
4. Default values (lowest priority)
"""

import os
from dataclasses import dataclass, field
from pathlib import Path
from typing import Optional, Dict, Any, Union
import yaml


@dataclass
class ScholarConfig:
    """
    Configuration for Scholar module.
    
    All parameters can be set via environment variables (SCITEX_SCHOLAR_* prefix)
    or passed directly.
    
    Priority order:
    1. Direct parameter specification (highest priority)
    2. Configuration file (YAML)
    3. Environment variables (SCITEX_SCHOLAR_* prefix)
    4. Default values (lowest priority)
    
    Example:
        # Using environment variables
        config = ScholarConfig()
        
        # Using direct parameters
        config = ScholarConfig(
            semantic_scholar_api_key="your-key",
            enable_auto_enrich=False
        )
        
        # Pass to Scholar
        scholar = Scholar(config=config)
    """
    
    # API Keys
    semantic_scholar_api_key: Optional[str] = field(
        default_factory=lambda: os.getenv("SCITEX_SCHOLAR_SEMANTIC_SCHOLAR_API_KEY")
    )
    crossref_api_key: Optional[str] = field(
        default_factory=lambda: os.getenv("SCITEX_SCHOLAR_CROSSREF_API_KEY")
    )
    
    # Email addresses for API access
    pubmed_email: str = field(
        default_factory=lambda: os.getenv("SCITEX_SCHOLAR_PUBMED_EMAIL", "research@example.com")
    )
    crossref_email: str = field(
        default_factory=lambda: os.getenv("SCITEX_SCHOLAR_CROSSREF_EMAIL", "research@example.com")
    )
    
    # Feature toggles
    enable_auto_enrich: bool = field(
        default_factory=lambda: os.getenv("SCITEX_SCHOLAR_AUTO_ENRICH", "true").lower() == "true"
    )
    use_impact_factor_package: bool = field(
        default_factory=lambda: os.getenv("SCITEX_SCHOLAR_USE_IMPACT_FACTOR_PACKAGE", "true").lower() == "true"
    )
    enable_auto_download: bool = field(
        default_factory=lambda: os.getenv("SCITEX_SCHOLAR_AUTO_DOWNLOAD", "false").lower() == "true"
    )
    acknowledge_scihub_ethical_usage: bool = field(
        default_factory=lambda: os.getenv("SCITEX_SCHOLAR_ACKNOWLEDGE_SCIHUB_ETHICAL_USAGE", "false").lower() == "true"
    )
    
    # Search configuration
    default_search_sources: list = field(
        default_factory=lambda: ["pubmed", "arxiv", "semantic_scholar"]
    )
    default_search_limit: int = 20
    
    # PDF management
    pdf_dir: Optional[str] = field(
        default_factory=lambda: os.getenv("SCITEX_SCHOLAR_PDF_DIR")
    )
    enable_pdf_extraction: bool = True
    
    # Performance settings
    max_parallel_requests: int = 3
    request_timeout: int = 30
    cache_size: int = 1000
    
    # Advanced settings
    verify_ssl: bool = True
    
    # OpenAthens authentication
    openathens_enabled: bool = field(
        default_factory=lambda: os.getenv("SCITEX_SCHOLAR_OPENATHENS_ENABLED", "false").lower() == "true"
    )
    openathens_org_id: Optional[str] = field(
        default_factory=lambda: os.getenv("SCITEX_SCHOLAR_OPENATHENS_ORG_ID")
    )
    openathens_idp_url: Optional[str] = field(
        default_factory=lambda: os.getenv("SCITEX_SCHOLAR_OPENATHENS_IDP_URL")
    )
    openathens_email: Optional[str] = field(
        default_factory=lambda: os.getenv("SCITEX_SCHOLAR_OPENATHENS_EMAIL")
    )
    openathens_username: Optional[str] = field(
        default_factory=lambda: os.getenv("SCITEX_SCHOLAR_OPENATHENS_USERNAME")  # Deprecated
    )
    openathens_password: Optional[str] = field(
        default_factory=lambda: os.getenv("SCITEX_SCHOLAR_OPENATHENS_PASSWORD")  # Deprecated
    )
    openathens_institution_name: Optional[str] = field(
        default_factory=lambda: os.getenv("SCITEX_SCHOLAR_OPENATHENS_INSTITUTION_NAME")
    )
    user_agent: str = "SciTeX-Scholar/1.0"  # HTTP User-Agent for API requests
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert configuration to dictionary."""
        return {
            "semantic_scholar_api_key": self.semantic_scholar_api_key,
            "crossref_api_key": self.crossref_api_key,
            "pubmed_email": self.pubmed_email,
            "crossref_email": self.crossref_email,
            "enable_auto_enrich": self.enable_auto_enrich,
            "use_impact_factor_package": self.use_impact_factor_package,
            "enable_auto_download": self.enable_auto_download,
            "acknowledge_scihub_ethical_usage": self.acknowledge_scihub_ethical_usage,
            "default_search_sources": self.default_search_sources,
            "default_search_limit": self.default_search_limit,
            "pdf_dir": self.pdf_dir,
            "enable_pdf_extraction": self.enable_pdf_extraction,
            "max_parallel_requests": self.max_parallel_requests,
            "request_timeout": self.request_timeout,
            "cache_size": self.cache_size,
            "verify_ssl": self.verify_ssl,
            "user_agent": self.user_agent,
        }
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "ScholarConfig":
        """Create configuration from dictionary."""
        return cls(**data)
    
    @classmethod
    def from_yaml(cls, path: Union[str, Path]) -> "ScholarConfig":
        """Load configuration from YAML file.
        
        Example YAML file:
            # ~/.scitex/scholar/config.yaml
            semantic_scholar_api_key: "your-key-here"
            pubmed_email: "your.email@example.com"
            enable_auto_enrich: true
            use_impact_factor_package: true
            enable_auto_download: false
            acknowledge_scihub_ethical_usage: false
            default_search_sources:
              - pubmed
              - arxiv
            pdf_dir: "~/.scitex/scholar/pdfs"
        """
        path = Path(path).expanduser()
        if not path.exists():
            raise FileNotFoundError(f"Config file not found: {path}")
        
        with open(path, 'r') as f:
            data = yaml.safe_load(f)
        
        return cls.from_dict(data)
    
    def to_yaml(self, path: Union[str, Path]) -> None:
        """Save configuration to YAML file."""
        path = Path(path).expanduser()
        path.parent.mkdir(parents=True, exist_ok=True)
        
        # Convert to dict and remove None values
        data = {k: v for k, v in self.to_dict().items() if v is not None}
        
        with open(path, 'w') as f:
            yaml.dump(data, f, default_flow_style=False, sort_keys=False)
    
    @classmethod
    def load(cls, path: Optional[Union[str, Path]] = None) -> "ScholarConfig":
        """Load configuration from file or environment.
        
        Args:
            path: Path to YAML config file. If None, checks:
                  1. SCITEX_SCHOLAR_CONFIG environment variable
                  2. ~/.scitex/scholar/config.yaml
                  3. ./scholar_config.yaml
                  4. Falls back to environment variables
        """
        if path:
            return cls.from_yaml(path)
        
        # Check environment variable for config path
        env_path = os.getenv("SCITEX_SCHOLAR_CONFIG")
        if env_path and Path(env_path).exists():
            return cls.from_yaml(env_path)
        
        # Check default locations
        default_paths = [
            Path("~/.scitex/scholar/config.yaml").expanduser(),
            Path("./scholar_config.yaml"),
            Path("./.scitex_scholar.yaml"),
        ]
        
        for default_path in default_paths:
            if default_path.exists():
                return cls.from_yaml(default_path)
        
        # Fall back to environment variables
        return cls()
    
    def merge(self, **kwargs) -> "ScholarConfig":
        """Create new config with merged values."""
        current = self.to_dict()
        current.update(kwargs)
        return self.from_dict(current)
    
    @classmethod
    def show_env_vars(cls) -> str:
        """Show all environment variables and their current values."""
        env_vars = {
            "SCITEX_SCHOLAR_SEMANTIC_SCHOLAR_API_KEY": "API key for Semantic Scholar",
            "SCITEX_SCHOLAR_CROSSREF_API_KEY": "API key for CrossRef",
            "SCITEX_SCHOLAR_PUBMED_EMAIL": "Email for PubMed API (default: research@example.com)",
            "SCITEX_SCHOLAR_CROSSREF_EMAIL": "Email for CrossRef API (default: research@example.com)",
            "SCITEX_SCHOLAR_AUTO_ENRICH": "Auto-enrich papers with citations/impact factors (default: true)",
            "SCITEX_SCHOLAR_USE_IMPACT_FACTOR_PACKAGE": "Use impact_factor package for journal metrics (default: true)",
            "SCITEX_SCHOLAR_AUTO_DOWNLOAD": "Auto-download open-access PDFs (default: false)",
            "SCITEX_SCHOLAR_ACKNOWLEDGE_SCIHUB_ETHICAL_USAGE": "Acknowledge ethical usage terms for Sci-Hub access (default: false)",
            "SCITEX_SCHOLAR_PDF_DIR": "Directory for storing PDFs",
            "SCITEX_SCHOLAR_CONFIG": "Path to config file",
        }
        
        output = ["Environment Variables for SciTeX Scholar:"]
        output.append("-" * 80)
        
        for var, desc in env_vars.items():
            value = os.getenv(var)
            if value and "API_KEY" in var:
                # Mask API keys for security
                value = value[:4] + "..." + value[-4:] if len(value) > 8 else "***"
            elif value is None:
                value = "<not set>"
            
            output.append(f"{var}")
            output.append(f"  Description: {desc}")
            output.append(f"  Current value: {value}")
            output.append("")
        
        return "\n".join(output)
    
    def show_config(self) -> str:
        """Show current configuration with sources."""
        output = ["Current SciTeX Scholar Configuration:"]
        output.append("-" * 80)
        
        # Map fields to their environment variable names
        field_to_env = {
            "semantic_scholar_api_key": "SCITEX_SCHOLAR_SEMANTIC_SCHOLAR_API_KEY",
            "crossref_api_key": "SCITEX_SCHOLAR_CROSSREF_API_KEY",
            "pubmed_email": "SCITEX_SCHOLAR_PUBMED_EMAIL",
            "crossref_email": "SCITEX_SCHOLAR_CROSSREF_EMAIL",
            "enable_auto_enrich": "SCITEX_SCHOLAR_AUTO_ENRICH",
            "use_impact_factor_package": "SCITEX_SCHOLAR_USE_IMPACT_FACTOR_PACKAGE",
            "enable_auto_download": "SCITEX_SCHOLAR_AUTO_DOWNLOAD",
            "acknowledge_scihub_ethical_usage": "SCITEX_SCHOLAR_ACKNOWLEDGE_SCIHUB_ETHICAL_USAGE",
            "pdf_dir": "SCITEX_SCHOLAR_PDF_DIR",
        }
        
        config_dict = self.to_dict()
        
        for field, value in config_dict.items():
            # Mask sensitive values
            display_value = value
            if value and "api_key" in field and isinstance(value, str):
                display_value = value[:4] + "..." + value[-4:] if len(value) > 8 else "***"
            
            output.append(f"{field}: {display_value}")
            
            # Show environment variable if applicable
            if field in field_to_env:
                env_var = field_to_env[field]
                env_value = os.getenv(env_var)
                if env_value:
                    output.append(f"  (from environment: {env_var})")
            
        return "\n".join(output)
    
    def show_secure_config(self) -> str:
        """
        Display configuration with sensitive data masked.
        
        Returns:
            Formatted string with configuration details (sensitive data masked)
        """
        def mask_value(value: Any, field_name: str) -> str:
            """Mask sensitive values based on field type."""
            if value is None:
                return "Not set"
            
            # API keys
            if "api_key" in field_name.lower():
                if len(str(value)) > 8:
                    return f"{str(value)[:4]}{'*' * (len(str(value)) - 8)}{str(value)[-4:]}"
                else:
                    return "*" * len(str(value))
            
            # Emails
            elif "email" in field_name.lower():
                parts = str(value).split('@')
                if len(parts) == 2:
                    user_part = parts[0]
                    if len(user_part) > 2:
                        masked_user = f"{user_part[:2]}{'*' * (len(user_part) - 2)}"
                    else:
                        masked_user = "*" * len(user_part)
                    return f"{masked_user}@{parts[1]}"
                else:
                    return "*" * len(str(value))
            
            # Paths (show only last directory)
            elif "dir" in field_name.lower() or "path" in field_name.lower():
                path_str = str(value)
                if '/' in path_str:
                    parts = path_str.split('/')
                    return f".../{'/'.join(parts[-2:])}" if len(parts) > 2 else path_str
                return path_str
            
            # Everything else
            else:
                return str(value)
        
        lines = []
        lines.append("=== SciTeX Scholar Configuration (Secure View) ===\n")
        
        # Group configurations
        api_keys = ["semantic_scholar_api_key", "crossref_api_key", "pubmed_email", "crossref_email"]
        features = ["enable_auto_enrich", "use_impact_factor_package", "enable_auto_download", 
                   "acknowledge_scihub_ethical_usage"]
        settings = ["pdf_dir", "default_search_limit", "default_search_sources", "max_parallel_requests"]
        
        # API Keys section
        lines.append("📚 API Keys & Credentials:")
        for field in api_keys:
            value = getattr(self, field)
            masked = mask_value(value, field)
            status = "✓" if value else "✗"
            lines.append(f"  {status} {field}: {masked}")
        
        # Features section
        lines.append("\n⚙️  Features:")
        for field in features:
            value = getattr(self, field)
            status = "✓ Enabled" if value else "✗ Disabled"
            lines.append(f"  • {field}: {status}")
        
        # Settings section
        lines.append("\n📁 Settings:")
        for field in settings:
            value = getattr(self, field)
            if isinstance(value, list):
                value_str = ", ".join(value)
            else:
                value_str = mask_value(value, field)
            lines.append(f"  • {field}: {value_str}")
        
        lines.append("\n" + "=" * 45)
        
        return "\n".join(lines)


# Default configuration instance
DEFAULT_CONFIG = ScholarConfig()

# EOF

================================================================================
File: /home/ywatanabe/proj/SciTeX-Code/src/scitex/scholar/_DOIResolver.py
Relative: ../../../../SciTeX-Code/src/scitex/scholar/_DOIResolver.py
Size: 25984 bytes, Lines: 692, Words: ~4330
================================================================================
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Timestamp: "2025-07-23 15:51:01 (ywatanabe)"
# File: /home/ywatanabe/proj/scitex_repo/src/scitex/scholar/_DOIResolver.py
# ----------------------------------------
import os
__FILE__ = (
    "./src/scitex/scholar/_DOIResolver.py"
)
__DIR__ = os.path.dirname(__FILE__)
# ----------------------------------------

"""Clean, optimized DOI resolver with pluggable sources."""

import logging
import re
import time
from abc import ABC, abstractmethod
from concurrent.futures import ThreadPoolExecutor, as_completed
from functools import lru_cache
from typing import Dict, List, Optional, Type

from tqdm import tqdm

logger = logging.getLogger(__name__)


class DOISource(ABC):
    """Abstract base class for DOI sources."""

    @abstractmethod
    def search(
        self,
        title: str,
        year: Optional[int] = None,
        authors: Optional[List[str]] = None,
    ) -> Optional[str]:
        """Search for DOI by title."""
        pass

    @abstractmethod
    def get_abstract(self, doi: str) -> Optional[str]:
        """Get abstract by DOI."""
        pass

    @property
    @abstractmethod
    def name(self) -> str:
        """Source name for logging."""
        pass

    @property
    def rate_limit_delay(self) -> float:
        """Delay between requests in seconds."""
        return 0.5

    def extract_doi_from_url(self, url: str) -> Optional[str]:
        """Extract DOI from URL if present."""
        if not url:
            return None

        # Direct DOI URLs
        if "doi.org/" in url:
            match = re.search(r"doi\.org/(.+?)(?:\?|$|#)", url)
            if match:
                return match.group(1).strip()

        # DOI pattern in URL
        doi_pattern = r"10\.\d{4,}/[-._;()/:\w]+"
        match = re.search(doi_pattern, url)
        if match:
            return match.group(0)

        return None


class CrossRefSource(DOISource):
    """CrossRef DOI source - no API key required, generous rate limits."""

    def __init__(self, email: str = "research@example.com"):
        self.email = email
        self._session = None

    @property
    def session(self):
        """Lazy load session."""
        if self._session is None:
            import requests

            self._session = requests.Session()
            self._session.headers.update(
                {"User-Agent": f"SciTeX/1.0 (mailto:{self.email})"}
            )
        return self._session

    @property
    def name(self) -> str:
        return "CrossRef"

    @property
    def rate_limit_delay(self) -> float:
        return 0.1  # CrossRef is very generous

    def search(
        self,
        title: str,
        year: Optional[int] = None,
        authors: Optional[List[str]] = None,
    ) -> Optional[str]:
        """Search CrossRef for DOI."""
        url = "https://api.crossref.org/works"
        params = {
            "query": title,
            "rows": 5,
            "select": "DOI,title,published-print",
            "mailto": self.email,
        }

        if year:
            params["filter"] = f"from-pub-date:{year},until-pub-date:{year}"

        try:
            response = self.session.get(url, params=params, timeout=30)
            if response.status_code == 200:
                data = response.json()
                items = data.get("message", {}).get("items", [])

                for item in items:
                    item_title = " ".join(item.get("title", []))
                    if self._is_title_match(title, item_title):
                        return item.get("DOI")
        except Exception as e:
            logger.debug(f"CrossRef error: {e}")

        return None

    def get_abstract(self, doi: str) -> Optional[str]:
        """Get abstract from CrossRef."""
        url = f"https://api.crossref.org/works/{doi}"
        params = {"mailto": self.email}

        try:
            response = self.session.get(url, params=params, timeout=30)
            if response.status_code == 200:
                data = response.json()
                return data.get("message", {}).get("abstract")
        except Exception as e:
            logger.debug(f"CrossRef abstract error: {e}")

        return None

    @staticmethod
    def _is_title_match(
        title1: str, title2: str, threshold: float = 0.85
    ) -> bool:
        """Simple title matching."""
        import string

        # Normalize
        t1 = title1.lower().strip()
        t2 = title2.lower().strip()

        # Remove punctuation
        for p in string.punctuation:
            t1 = t1.replace(p, " ")
            t2 = t2.replace(p, " ")

        # Word overlap
        words1 = set(t1.split())
        words2 = set(t2.split())

        if not words1 or not words2:
            return False

        intersection = len(words1 & words2)
        union = len(words1 | words2)

        return (intersection / union) >= threshold if union > 0 else False


class PubMedSource(DOISource):
    """PubMed DOI source - free, no API key required."""

    def __init__(self, email: str = "research@example.com"):
        self.email = email
        self._session = None

    @property
    def session(self):
        """Lazy load session."""
        if self._session is None:
            import requests

            self._session = requests.Session()
        return self._session

    @property
    def name(self) -> str:
        return "PubMed"

    @property
    def rate_limit_delay(self) -> float:
        return 0.35  # NCBI requests 3 per second max

    def search(
        self,
        title: str,
        year: Optional[int] = None,
        authors: Optional[List[str]] = None,
    ) -> Optional[str]:
        """Search PubMed for DOI."""
        # Build query
        query_parts = [f'"{title}"[Title]']
        if year:
            query_parts.append(f"{year}[pdat]")

        query = " AND ".join(query_parts)

        # Search
        search_url = (
            "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi"
        )
        search_params = {
            "db": "pubmed",
            "term": query,
            "retmode": "json",
            "retmax": 5,
            "email": self.email,
        }

        try:
            response = self.session.get(
                search_url, params=search_params, timeout=30
            )
            if response.status_code == 200:
                data = response.json()
                pmids = data.get("esearchresult", {}).get("idlist", [])

                # Check each PMID
                for pmid in pmids:
                    doi = self._fetch_doi_for_pmid(pmid, title)
                    if doi:
                        return doi
        except Exception as e:
            logger.debug(f"PubMed error: {e}")

        return None

    def _fetch_doi_for_pmid(
        self, pmid: str, expected_title: str
    ) -> Optional[str]:
        """Fetch DOI for a specific PMID."""
        fetch_url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi"
        fetch_params = {
            "db": "pubmed",
            "id": pmid,
            "retmode": "xml",
            "email": self.email,
        }

        try:
            response = self.session.get(
                fetch_url, params=fetch_params, timeout=30
            )
            if response.status_code == 200:
                import xml.etree.ElementTree as ET

                root = ET.fromstring(response.text)

                # Verify title match
                title_elem = root.find(".//ArticleTitle")
                if title_elem is not None and title_elem.text:
                    if CrossRefSource._is_title_match(
                        expected_title, title_elem.text
                    ):
                        # Extract DOI
                        for id_elem in root.findall(".//ArticleId"):
                            if id_elem.get("IdType") == "doi":
                                return id_elem.text
        except Exception as e:
            logger.debug(f"PubMed fetch error: {e}")

        return None

    def get_abstract(self, doi: str) -> Optional[str]:
        """Get abstract from PubMed by DOI."""
        # First find PMID by DOI
        search_url = (
            "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi"
        )
        search_params = {
            "db": "pubmed",
            "term": f"{doi}[doi]",
            "retmode": "json",
            "email": self.email,
        }

        try:
            response = self.session.get(
                search_url, params=search_params, timeout=30
            )
            if response.status_code == 200:
                data = response.json()
                pmids = data.get("esearchresult", {}).get("idlist", [])

                if pmids:
                    # Fetch abstract
                    fetch_url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi"
                    fetch_params = {
                        "db": "pubmed",
                        "id": pmids[0],
                        "retmode": "xml",
                        "email": self.email,
                    }

                    response = self.session.get(
                        fetch_url, params=fetch_params, timeout=30
                    )
                    if response.status_code == 200:
                        import xml.etree.ElementTree as ET

                        root = ET.fromstring(response.text)
                        abstract_elem = root.find(".//AbstractText")
                        if abstract_elem is not None:
                            return abstract_elem.text
        except Exception as e:
            logger.debug(f"PubMed abstract error: {e}")

        return None


class OpenAlexSource(DOISource):
    """OpenAlex - free and open alternative to proprietary databases."""

    def __init__(self, email: str = "research@example.com"):
        self.email = email
        self._session = None

    @property
    def session(self):
        """Lazy load session."""
        if self._session is None:
            import requests

            self._session = requests.Session()
            self._session.headers.update(
                {"User-Agent": f"SciTeX/1.0 (mailto:{self.email})"}
            )
        return self._session

    @property
    def name(self) -> str:
        return "OpenAlex"

    @property
    def rate_limit_delay(self) -> float:
        return 0.1  # OpenAlex is generous

    def search(
        self,
        title: str,
        year: Optional[int] = None,
        authors: Optional[List[str]] = None,
    ) -> Optional[str]:
        """Search OpenAlex for DOI."""
        url = "https://api.openalex.org/works"

        # Build filter
        filters = [f'title.search:"{title}"']
        if year:
            filters.append(f"publication_year:{year}")

        params = {
            "filter": ",".join(filters),
            "per_page": 5,
            "mailto": self.email,
        }

        try:
            response = self.session.get(url, params=params, timeout=30)
            if response.status_code == 200:
                data = response.json()
                results = data.get("results", [])

                for work in results:
                    work_title = work.get("title", "")
                    if work_title and CrossRefSource._is_title_match(
                        title, work_title
                    ):
                        doi_url = work.get("doi", "")
                        if doi_url:
                            # Extract DOI from URL
                            return doi_url.replace("https://doi.org/", "")
        except Exception as e:
            logger.debug(f"OpenAlex error: {e}")

        return None

    def get_abstract(self, doi: str) -> Optional[str]:
        """OpenAlex doesn't provide abstracts."""
        return None


class DOIResolver:
    """Clean, optimized DOI resolver with configurable sources."""

    # Default source order (based on rate limits and reliability)
    DEFAULT_SOURCES = ["crossref", "pubmed", "openalex"]

    # Source registry
    SOURCE_CLASSES: Dict[str, Type[DOISource]] = {
        "crossref": CrossRefSource,
        "pubmed": PubMedSource,
        "openalex": OpenAlexSource,
    }

    def __init__(
        self,
        email: str = "research@example.com",
        sources: Optional[List[str]] = None,
    ):
        """
        Initialize resolver with specified sources.

        Args:
            email: Email for API access
            sources: List of source names to use (default: all available)
        """
        self.email = email
        self.sources = sources or self.DEFAULT_SOURCES
        self._source_instances: Dict[str, DOISource] = {}

    def _get_source(self, name: str) -> Optional[DOISource]:
        """Get or create source instance."""
        if name not in self._source_instances:
            source_class = self.SOURCE_CLASSES.get(name)
            if source_class:
                self._source_instances[name] = source_class(self.email)
        return self._source_instances.get(name)

    @lru_cache(maxsize=1000)
    def title_to_doi(
        self,
        title: str,
        year: Optional[int] = None,
        authors: Optional[tuple] = None,  # Tuple for hashability
        sources: Optional[tuple] = None,  # Tuple for hashability
    ) -> Optional[str]:
        """
        Resolve DOI from title with caching.

        Args:
            title: Paper title
            year: Publication year
            authors: Author names as tuple
            sources: Override default source list as tuple

        Returns:
            DOI if found
        """
        sources_list = list(sources) if sources else self.sources
        authors_list = list(authors) if authors else None

        logger.info(f"Resolving DOI for: {title[:60]}...")

        for source_name in sources_list:
            source = self._get_source(source_name)
            if not source:
                continue

            try:
                doi = source.search(title, year, authors_list)
                if doi:
                    logger.info(f"  ✓ Found DOI via {source.name}: {doi}")
                    return doi
            except Exception as e:
                logger.debug(f"Error with {source_name}: {e}")

            # Rate limiting
            time.sleep(source.rate_limit_delay)

        logger.info(f"  ✗ No DOI found")
        return None

    def get_abstract(
        self, doi: str, sources: Optional[List[str]] = None
    ) -> Optional[str]:
        """Get abstract for DOI from available sources."""
        sources = sources or [
            "crossref",
            "pubmed",
        ]  # OpenAlex doesn't have abstracts

        for source_name in sources:
            source = self._get_source(source_name)
            if not source:
                continue

            try:
                abstract = source.get_abstract(doi)
                if abstract:
                    return abstract
            except Exception as e:
                logger.debug(f"Abstract error with {source_name}: {e}")

        return None

    def add_source(self, name: str, source_class: Type[DOISource]):
        """Add a custom source."""
        self.SOURCE_CLASSES[name] = source_class
        if name not in self.sources:
            self.sources.append(name)

    def resolve_from_url(self, url: str) -> Optional[str]:
        """
        Resolve DOI from URL using multiple strategies.

        Args:
            url: URL that might contain or lead to a DOI

        Returns:
            DOI if found
        """
        if not url:
            return None

        # 1. Try direct DOI extraction
        doi = self._extract_doi_from_url(url)
        if doi:
            return doi

        # 2. Try Semantic Scholar
        if "semanticscholar.org" in url and "CorpusId:" in url:
            doi = self._resolve_semantic_scholar(url)
            if doi:
                return doi

        # 3. Try PubMed
        if "ncbi.nlm.nih.gov" in url and "pubmed" in url:
            doi = self._resolve_pubmed_url(url)
            if doi:
                return doi

        return None

    def _extract_doi_from_url(self, url: str) -> Optional[str]:
        """Extract DOI directly from URL."""
        # Use the base class method
        source = self._get_source("crossref")
        if source:
            return source.extract_doi_from_url(url)
        return None

    def _resolve_semantic_scholar(self, url: str) -> Optional[str]:
        """Resolve DOI from Semantic Scholar URL."""
        match = re.search(r"CorpusId:(\d+)", url)
        if not match:
            return None

        corpus_id = match.group(1)

        try:
            import requests

            api_url = f"https://api.semanticscholar.org/graph/v1/paper/CorpusID:{corpus_id}"
            params = {"fields": "externalIds"}

            response = requests.get(api_url, params=params, timeout=10)
            if response.status_code == 200:
                data = response.json()
                external_ids = data.get("externalIds", {})
                if external_ids and "DOI" in external_ids:
                    logger.info(
                        f"  ✓ Found DOI from Semantic Scholar: {external_ids['DOI']}"
                    )
                    return external_ids["DOI"]
            elif response.status_code == 429:
                logger.warning("Semantic Scholar rate limited")

            time.sleep(1.5)  # Rate limiting
        except Exception as e:
            logger.debug(f"Semantic Scholar error: {e}")

        return None

    def _resolve_pubmed_url(self, url: str) -> Optional[str]:
        """Resolve DOI from PubMed URL."""
        match = re.search(r"pubmed/(\d+)", url)
        if not match:
            return None

        pmid = match.group(1)

        try:
            import requests

            api_url = (
                "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esummary.fcgi"
            )
            params = {
                "db": "pubmed",
                "id": pmid,
                "retmode": "json",
                "email": self.email,
            }

            response = requests.get(api_url, params=params, timeout=10)
            if response.status_code == 200:
                data = response.json()
                result = data.get("result", {}).get(pmid, {})

                for article_id in result.get("articleids", []):
                    if article_id.get("idtype") == "doi":
                        doi = article_id.get("value")
                        logger.info(f"  ✓ Found DOI from PubMed: {doi}")
                        return doi

            time.sleep(0.3)  # NCBI rate limit
        except Exception as e:
            logger.debug(f"PubMed URL error: {e}")

        return None


class BatchDOIResolver:
    """Batch DOI resolver with parallel processing and progress tracking."""

    def __init__(
        self,
        email: str = "research@example.com",
        max_workers: int = 3,
        delay_between_papers: float = 0.5,
    ):
        """
        Initialize batch resolver.

        Args:
            email: Email for API access
            max_workers: Number of parallel workers (be respectful to APIs)
            delay_between_papers: Delay between processing papers
        """
        self.email = email
        self.max_workers = max_workers
        self.delay_between_papers = delay_between_papers
        self._resolver = DOIResolver(email=email)

    def resolve_batch(
        self, papers: List[Dict[str, any]], show_progress: bool = True
    ) -> List[Dict[str, any]]:
        """
        Resolve DOIs for a batch of papers.

        Args:
            papers: List of dicts with 'title', 'year', 'authors' keys
            show_progress: Show progress bar

        Returns:
            List of results with 'doi', 'abstract', 'title' keys
        """
        results = []

        # Use progress bar if requested
        iterator = (
            tqdm(papers, desc="Resolving DOIs") if show_progress else papers
        )

        # Process in small batches to respect rate limits
        batch_size = self.max_workers
        for i in range(0, len(papers), batch_size):
            batch = papers[i : i + batch_size]

            # Process batch in parallel
            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                # Submit all papers in batch
                future_to_paper = {
                    executor.submit(self._process_paper, paper): paper
                    for paper in batch
                }

                # Collect results as they complete
                for future in as_completed(future_to_paper):
                    paper = future_to_paper[future]
                    try:
                        result = future.result()
                        results.append(result)

                        if show_progress and result["doi"]:
                            tqdm.write(
                                f"✓ Found DOI for: {result['title'][:50]}..."
                            )

                    except Exception as e:
                        logger.error(f"Error processing paper: {e}")
                        results.append(
                            {
                                "title": paper.get("title", ""),
                                "doi": None,
                                "abstract": None,
                                "error": str(e),
                            }
                        )

            # Delay between batches
            if i + batch_size < len(papers):
                time.sleep(self.delay_between_papers)

        return results

    def _process_paper(self, paper: Dict[str, any]) -> Dict[str, any]:
        """Process a single paper."""
        title = paper.get("title", "")
        year = paper.get("year")
        authors = paper.get("authors", [])
        url = paper.get("url") or paper.get("pdf_url")

        doi = None

        # First try URL resolution if available
        if url and not doi:
            doi = self._resolver.resolve_from_url(url)
            if doi:
                logger.debug(f"Found DOI from URL for: {title[:50]}...")

        # If no DOI from URL, try title-based search
        if not doi:
            # Convert authors to tuple for caching
            authors_tuple = tuple(authors) if authors else None

            # Resolve DOI
            doi = self._resolver.title_to_doi(
                title=title, year=year, authors=authors_tuple
            )

        # Get abstract if DOI found
        abstract = None
        if doi:
            abstract = self._resolver.get_abstract(doi)

        return {
            "title": title,
            "doi": doi,
            "abstract": abstract,
            "year": year,
            "authors": authors,
            "url": url,
        }

    def enhance_papers_parallel(
        self, papers: List[any], show_progress: bool = True  # Paper objects
    ) -> Dict[str, Dict[str, any]]:
        """
        Enhance Paper objects with DOIs and abstracts in parallel.

        Args:
            papers: List of Paper objects
            show_progress: Show progress bar

        Returns:
            Dict mapping paper identifiers to enhancement results
        """
        # Prepare paper data for batch processing
        paper_data = []
        paper_map = {}

        for paper in papers:
            paper_id = paper.get_identifier()
            paper_data.append(
                {
                    "title": paper.title,
                    "year": paper.year,
                    "authors": paper.authors,
                    "pdf_url": paper.pdf_url,  # Include URL for resolution
                }
            )
            paper_map[paper.title] = paper_id

        # Process in batch
        results = self.resolve_batch(paper_data, show_progress)

        # Map results back to paper IDs
        enhanced_data = {}
        for result in results:
            paper_id = paper_map.get(result["title"])
            if paper_id:
                enhanced_data[paper_id] = result

        # Update paper objects
        success_count = 0
        for paper in papers:
            paper_id = paper.get_identifier()
            if paper_id in enhanced_data:
                data = enhanced_data[paper_id]

                if data.get("doi") and not paper.doi:
                    paper.doi = data["doi"]
                    success_count += 1

                if data.get("abstract") and not paper.abstract:
                    paper.abstract = data["abstract"]

        if show_progress:
            print(
                f"\n✓ Enhanced {success_count}/{len(papers)} papers with DOIs"
            )

        return enhanced_data


# Example usage
if __name__ == "__main__":
    # Test papers
    test_papers = [
        {
            "title": "The functional role of cross-frequency coupling",
            "year": 2010,
        },
        {
            "title": "Measuring phase-amplitude coupling between neuronal oscillations of different frequencies",
            "year": 2010,
        },
        {
            "title": "Phase-amplitude coupling supports phase coding in human ECoG",
            "year": 2015,
        },
    ]

    # Create batch resolver
    resolver = BatchDOIResolver(
        email="research@example.com",
        max_workers=2,  # Process 2 papers in parallel
    )

    # Resolve in batch
    print("Batch DOI Resolution Demo")
    print("=" * 60)

    results = resolver.resolve_batch(test_papers)

    # Display results
    print(f"\nProcessed {len(results)} papers:")
    for result in results:
        print(f"\nTitle: {result['title'][:60]}...")
        if result["doi"]:
            print(f"  DOI: {result['doi']}")
            print(f"  Abstract: {'Yes' if result['abstract'] else 'No'}")
        else:
            print("  DOI: Not found")

# EOF

================================================================================
File: /home/ywatanabe/proj/SciTeX-Code/src/scitex/scholar/_ethical_usage.py
Relative: ../../../../SciTeX-Code/src/scitex/scholar/_ethical_usage.py
Size: 6268 bytes, Lines: 146, Words: ~1044
================================================================================
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Timestamp: "2025-07-23 17:17:00 (ywatanabe)"
# File: /home/ywatanabe/proj/scitex_repo/src/scitex/scholar/_ethical_usage.py
# ----------------------------------------
import os
__FILE__ = (
    "./src/scitex/scholar/_ethical_usage.py"
)
__DIR__ = os.path.dirname(__FILE__)
# ----------------------------------------

"""
Ethical usage guidelines for Sci-Hub integration.

This module contains the ethical usage text and functions to display it.
"""

ETHICAL_USAGE_NOTICE = """
⚖️  SCI-HUB USAGE NOTICE (PDF Download Only)

IMPORTANT: This notice applies ONLY to the Sci-Hub PDF download feature.
All other SciTeX Scholar features (search, enrichment, citation tracking) are completely legitimate.

The Sci-Hub integration is designed to help researchers access papers through various channels.
Please use this specific feature responsibly and in accordance with your local laws and institutional policies.

✅ Appropriate Uses:
- Open access papers with broken download links
- Papers you have legitimate institutional access to
- Your own published work
- Publicly funded research
- Papers with Creative Commons licenses
- Educational/research purposes with proper authorization

❌ Please Don't Use For:
- Recently published commercial papers under active copyright
- Papers you don't have legitimate access to
- Bulk downloading of entire journal collections
- Commercial redistribution
- Violating publisher terms of service

📚 Alternative Legal Access Methods:
1. Institutional Access
2. Open Access Repositories (PubMed Central, arXiv, bioRxiv)
3. Author Websites
4. ResearchGate/Academia.edu
5. Library Services
6. Contact Authors Directly
7. Unpaywall Browser Extension

See docs/SCIHUB_ETHICAL_USAGE.md for complete guidelines.
"""

ETHICAL_USAGE_FULL = """
# Sci-Hub PDF Download - Ethical Usage Guidelines

## ⚠️ IMPORTANT: This Only Applies to Sci-Hub PDF Downloads

**SciTeX Scholar is a completely legitimate research tool.** All features including:
- ✅ Literature search (PubMed, arXiv, Semantic Scholar)
- ✅ Impact factor enrichment
- ✅ Citation tracking
- ✅ BibTeX management
- ✅ Local PDF indexing

...are 100% ethical and legal.

## ⚖️ Sci-Hub Integration Notice

The Sci-Hub PDF download feature is an optional tool designed to help researchers access papers. 
This specific feature should be used responsibly and in accordance with your local laws and institutional policies.

### ✅ Appropriate Uses:
- **Open access papers** with broken download links on publisher sites
- **Papers you have legitimate institutional access to** when official sites are down
- **Your own published work** when you can't access it through normal channels
- **Publicly funded research** that should be freely available
- **Papers with Creative Commons or similar open licenses**
- **Educational purposes** in accordance with fair use policies
- **Research purposes** where you have proper authorization

### ❌ Please Don't Use For:
- Recently published commercial papers under active copyright
- Papers you don't have legitimate access to
- Bulk downloading of entire journal collections
- Commercial redistribution
- Violating publisher terms of service
- Circumventing paywalls for commercial gain

## 📚 Alternative Legal Access Methods

Before using this tool, consider these legitimate alternatives:

1. **Institutional Access**: Check if your university/organization provides access
2. **Open Access Repositories**: 
   - PubMed Central
   - arXiv
   - bioRxiv
   - PLOS ONE
   - Directory of Open Access Journals (DOAJ)
3. **Author Websites**: Many authors post preprints on personal pages
4. **ResearchGate/Academia.edu**: Authors often share their work
5. **Library Services**: Inter-library loan programs
6. **Contact Authors Directly**: Most researchers are happy to share their work
7. **Unpaywall Browser Extension**: Finds legal open access versions

## 🔒 Privacy and Security

- This tool uses Selenium with headless Chrome
- Your searches may be visible to third-party services
- Use institutional VPNs when appropriate
- Be aware of your organization's policies

## 📝 Legal Disclaimer

This tool is provided for educational and research purposes only. Users are responsible for ensuring their use complies with all applicable laws, regulations, and institutional policies. The developers do not endorse or encourage any illegal activity.

## 🤝 Responsible Research

Support open science by:
- Publishing in open access journals when possible
- Depositing preprints in repositories
- Advocating for fair access to publicly funded research
- Respecting the work of publishers who add value
- Supporting sustainable academic publishing models

---

*Remember: The goal is to advance human knowledge while respecting intellectual property rights and supporting sustainable academic publishing.*
"""


def get_ethical_usage_notice(full: bool = False) -> str:
    """
    Get the ethical usage notice text.
    
    Args:
        full: If True, return the complete guidelines. If False, return the brief notice.
        
    Returns:
        The ethical usage text.
    """
    return ETHICAL_USAGE_FULL if full else ETHICAL_USAGE_NOTICE


def display_ethical_usage(full: bool = False) -> None:
    """
    Display the ethical usage notice.
    
    Args:
        full: If True, display the complete guidelines. If False, display the brief notice.
    """
    print(get_ethical_usage_notice(full))


def check_ethical_usage(acknowledged: bool = None) -> bool:
    """
    Check if ethical usage has been acknowledged.
    
    Args:
        acknowledged: If provided, sets the acknowledgment status. If None, prompts user.
        
    Returns:
        True if acknowledged, False otherwise.
    """
    if acknowledged is not None:
        return acknowledged
        
    # In automated environments, return False by default
    import sys
    if not sys.stdin.isatty():
        return False
        
    print(ETHICAL_USAGE_NOTICE)
    response = input("\nDo you acknowledge and agree to use this feature ethically? (yes/no): ")
    return response.lower() in ['yes', 'y']


# Alias for backward compatibility
ETHICAL_USAGE_MESSAGE = ETHICAL_USAGE_NOTICE


# EOF

================================================================================
File: /home/ywatanabe/proj/SciTeX-Code/src/scitex/scholar/__init__.py
Relative: ../../../../SciTeX-Code/src/scitex/scholar/__init__.py
Size: 6774 bytes, Lines: 207, Words: ~1129
================================================================================
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Timestamp: "2025-07-23 15:49:17 (ywatanabe)"
# File: /home/ywatanabe/proj/scitex_repo/src/scitex/scholar/__init__.py
# ----------------------------------------
import os
__FILE__ = (
    "./src/scitex/scholar/__init__.py"
)
__DIR__ = os.path.dirname(__FILE__)
# ----------------------------------------

"""
SciTeX Scholar - Scientific Literature Management Made Simple

This module provides a unified interface for:
- Searching scientific literature across multiple sources
- Automatic paper enrichment with journal metrics
- PDF downloads and local library management
- Bibliography generation in multiple formats

Quick Start:
    from scitex.scholar import Scholar

    scholar = Scholar()
    papers = scholar.search("deep learning")
    papers.save("papers.bib")
"""

# Import main class
from ._Scholar import Scholar, search, search_quick, enrich_bibtex

# Import configuration
from ._Config import ScholarConfig

# Import core classes for advanced users
from ._Paper import Paper
from ._Papers import Papers

# Backward compatibility alias
PaperCollection = Papers

# Import utility functions
from ._utils import (
    papers_to_bibtex,
    papers_to_ris,
    papers_to_json,
    papers_to_markdown
)

# Import enrichment functionality
from ._MetadataEnricher import (
    MetadataEnricher,
    _enrich_papers_with_all,
    _enrich_papers_with_impact_factors,
    _enrich_papers_with_citations,
)

# PDF download functionality
from ._PDFDownloader import (
    PDFDownloader,
    download_pdf,
    download_pdfs as download_pdfs_async,
)

# Create module-level convenience function
def download_pdfs(
    dois,
    download_dir=None,
    force=False,
    max_workers=4,
    show_progress=True,
    acknowledge_ethical_usage=None,
    **kwargs
):
    """
    Download PDFs for DOIs using default Scholar instance.
    
    This is a convenience function that creates a Scholar instance if needed.
    For more control, use Scholar().download_pdfs() directly.
    
    Args:
        dois: DOI strings (list or single string) or Papers/Paper objects
        download_dir: Directory to save PDFs
        force: Force re-download
        max_workers: Maximum concurrent downloads
        show_progress: Show download progress
        acknowledge_ethical_usage: Acknowledge ethical usage for Sci-Hub
        **kwargs: Additional arguments
        
    Returns:
        Dictionary with download results
        
    Examples:
        >>> import scitex as stx
        >>> stx.scholar.download_pdfs(["10.1234/doi1", "10.5678/doi2"])
        >>> stx.scholar.download_pdfs("10.1234/single-doi")
    """
    scholar = Scholar()
    return scholar.download_pdfs(
        dois,
        download_dir=download_dir,
        force=force,
        max_workers=max_workers,
        show_progress=show_progress,
        acknowledge_ethical_usage=acknowledge_ethical_usage,
        **kwargs
    )

# Version
__version__ = "2.0.0"

# What users see with "from scitex.scholar import *"
__all__ = [
    # Main interface
    'Scholar',
    'ScholarConfig',

    # Convenience functions
    'search',
    'search_quick',
    'enrich_bibtex',
    'download_pdfs',  # NEW: Module-level convenience function

    # Core classes
    'Paper',
    'Papers',
    'PaperCollection',  # Backward compatibility alias

    # Format converters
    'papers_to_bibtex',
    'papers_to_ris',
    'papers_to_json',
    'papers_to_markdown',

    # Enrichment
    'MetadataEnricher',
    
    # PDF download functionality
    'PDFDownloader',
    'download_pdf',
    'download_pdfs_async',
]

# For backward compatibility, provide access to old functions with deprecation warnings
def __getattr__(name):
    """Provide backward compatibility with deprecation warnings."""
    import warnings
    
    # Handle special IPython attributes
    if name in ['__custom_documentations__', '__wrapped__']:
        raise AttributeError(f"module '{__name__}' has no attribute '{name}'")

    # Map old names to new functionality
    compatibility_map = {
        'search_sync': 'search',
        'build_index': 'Scholar()._index_local_pdfs',
        'get_scholar_dir': 'Scholar().workspace_dir',
        'LocalSearchEngine': 'Scholar',
        'VectorSearchEngine': 'Scholar',
        'PDFDownloader': 'Scholar',
        'search_papers': 'search',
        'SemanticScholarPaper': 'Paper',
        'PaperMetadata': 'Paper',
        'PaperAcquisition': 'Scholar',
        'SemanticScholarClient': 'Scholar',
        'JournalMetrics': 'Scholar',
        'PaperEnrichmentService': 'Scholar',
        'generate_enriched_bibliography': 'Papers.save'
    }

    if name in compatibility_map:
        warnings.warn(
            f"{name} is deprecated. Use {compatibility_map[name]} instead.",
            DeprecationWarning,
            stacklevel=2
        )

        # Return the Scholar class for most cases
        if name in ['search_sync', 'search_papers']:
            return search
        elif name == 'build_index':
            def build_index(paths, **kwargs):
                scholar = Scholar()
                stats = {}
                for path in paths:
                    stats.update(scholar._index_local_pdfs(path))
                return stats
            return build_index
        else:
            return Scholar

    from ..errors import ScholarError
    raise ScholarError(
        f"Module attribute not found: '{name}'",
        context={"module": __name__, "attribute": name},
        suggestion=f"Available attributes: Scholar, Paper, Papers, search, enrich_bibtex"
    )


# Module docstring for help()
def _module_docstring():
    """
    SciTeX Scholar - Scientific Literature Management

    Main Classes:
        Scholar: Main interface for all functionality
        Paper: Represents a scientific paper
        Papers: Collection of papers with analysis tools

    Quick Start:
        >>> from scitex.scholar import Scholar
        >>> scholar = Scholar()
        >>> papers = scholar.search("machine learning")
        >>> papers.filter(year_min=2020).save("ml_papers.bib")

    Common Workflows:
        # Search and enrich
        papers = scholar.search("deep learning", year_min=2022)

        # Download PDFs
        scholar.download_pdfs(papers)

        # Filter results
        high_impact = papers.filter(impact_factor_min=5.0)

        # Save bibliography
        papers.save("bibliography.bib", format="bibtex")

        # Search local library
        scholar._index_local_pdfs("./my_papers")
        local = scholar.search_local("transformer")

    For more information, see the documentation at:
    https://github.com/ywatanabe1989/SciTeX-Code
    """
    pass

# Set module docstring
__doc__ = _module_docstring.__doc__

# EOF

================================================================================
File: /home/ywatanabe/proj/SciTeX-Code/src/scitex/scholar/_MetadataEnricher.py
Relative: ../../../../SciTeX-Code/src/scitex/scholar/_MetadataEnricher.py
Size: 23222 bytes, Lines: 585, Words: ~3870
================================================================================
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Timestamp: "2025-07-23 15:52:29 (ywatanabe)"
# File: /home/ywatanabe/proj/scitex_repo/src/scitex/scholar/_MetadataEnricher.py
# ----------------------------------------
import os
__FILE__ = (
    "./src/scitex/scholar/_MetadataEnricher.py"
)
__DIR__ = os.path.dirname(__FILE__)
# ----------------------------------------

"""
Metadata enrichment module for SciTeX Scholar.

This module enriches scientific papers with additional metadata:
- Journal impact factors from impact_factor package
- Citation counts from Semantic Scholar and CrossRef
- Journal metrics (quartiles, rankings)
- Future: h-index, author metrics, altmetrics, etc.

All enrichment is done in-place on Paper objects.
"""

import asyncio
import logging
from difflib import SequenceMatcher
from functools import lru_cache
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

from ..errors import EnrichmentError, warn_performance
from ._Paper import Paper

logger = logging.getLogger(__name__)


def _get_jcr_year():
    """Dynamically determine JCR data year from impact_factor package files."""
    try:
        import glob
        import re

        import impact_factor

        # Find Excel files in the package data directory
        package_dir = os.path.dirname(impact_factor.__file__)
        data_dir = os.path.join(package_dir, "data")

        if os.path.exists(data_dir):
            excel_files = glob.glob(os.path.join(data_dir, "*.xlsx"))
            years = []

            for file in excel_files:
                basename = os.path.basename(file)
                # Extract year from filename (e.g., "CopyofImpactFactor2024.xlsx")
                year_match = re.search(r"20\d{2}", basename)
                if year_match:
                    years.append(int(year_match.group()))

            if years:
                return max(years)  # Return the latest year
    except Exception as e:
        logger.debug(f"Could not determine JCR year from package: {e}")

    # Fallback to hardcoded year
    return 2024


# JCR data year - dynamically determined from impact_factor package
JCR_YEAR = _get_jcr_year()


class MetadataEnricher:
    """
    Metadata enricher for scientific papers.

    Enriches Paper objects with impact factors, citation counts,
    journal metrics, and other scholarly metadata from various sources.
    """

    def __init__(
        self,
        semantic_scholar_api_key: Optional[str] = None,
        crossref_api_key: Optional[str] = None,
        email: Optional[str] = None,
        journal_data_path: Optional[Path] = None,
        use_impact_factor_package: bool = True,
        cache_size: int = 1000,
    ) -> None:
        """
        Initialize unified enricher.

        Args:
            semantic_scholar_api_key: API key for Semantic Scholar
            crossref_api_key: API key for CrossRef (optional, for higher rate limits)
            email: Email for CrossRef API (used in User-Agent)
            journal_data_path: Path to custom journal metrics data
            use_impact_factor_package: Whether to use impact_factor package
            cache_size: Size of LRU cache for journal lookups
        """
        # API keys and email
        self.semantic_scholar_api_key: Optional[str] = semantic_scholar_api_key
        self.crossref_api_key: Optional[str] = crossref_api_key
        self.email: Optional[str] = email

        # Journal data
        self.journal_data_path: Optional[Path] = journal_data_path
        self.use_impact_factor_package: bool = use_impact_factor_package
        self._journal_data: Optional[Dict[str, Dict[str, Any]]] = None
        self._impact_factor_instance: Optional[Any] = None

        # Configure cache
        self._cache_size: int = cache_size
        self._get_journal_metrics = lru_cache(maxsize=cache_size)(
            self._get_journal_metrics_uncached
        )

        # Initialize components
        self._init_impact_factor_package()
        self._load_journal_data()

    def enrich_all(
        self,
        papers: List[Paper],
        enrich_impact_factors: bool = True,
        enrich_citations: bool = True,
        enrich_journal_metrics: bool = True,
        parallel: bool = True,
    ) -> List[Paper]:
        """
        Enrich papers with all available metadata.

        Args:
            papers: List of papers to enrich
            enrich_impact_factors: Add journal impact factors
            enrich_citations: Add citation counts from Semantic Scholar
            enrich_journal_metrics: Add quartiles, rankings
            parallel: Use parallel processing for API calls

        Returns:
            Same list with papers enriched in-place

        Raises:
            EnrichmentError: If enrichment fails critically
        """
        if not papers:
            return papers

        logger.info(f"Starting enrichment for {len(papers)} papers")

        # Enrich impact factors and journal metrics together (same data source)
        if enrich_impact_factors or enrich_journal_metrics:
            self._enrich_journal_data(
                papers,
                include_impact_factors=enrich_impact_factors,
                include_metrics=enrich_journal_metrics,
            )

        # Enrich citations (requires API calls)
        if enrich_citations:
            if parallel and len(papers) > 50:
                warn_performance(
                    "Citation enrichment",
                    f"Enriching {len(papers)} papers in parallel. This may take time.",
                )

            # Run async enrichment
            try:
                loop = asyncio.get_event_loop()
                if loop.is_running():
                    # If already in async context, create task
                    task = asyncio.create_task(
                        self._enrich_citations_async(papers)
                    )
                    papers = asyncio.run_coroutine_threadsafe(
                        task, loop
                    ).result()
                else:
                    papers = loop.run_until_complete(
                        self._enrich_citations_async(papers)
                    )
            except RuntimeError:
                # No event loop, create new one
                papers = asyncio.run(self._enrich_citations_async(papers))

        logger.info("Enrichment completed")
        return papers

    def enrich_impact_factors(self, papers: List[Paper]) -> List[Paper]:
        """
        Enrich papers with journal impact factors only.

        Args:
            papers: List of papers to enrich

        Returns:
            Same list with impact factors added
        """
        return self.enrich_all(
            papers,
            enrich_impact_factors=True,
            enrich_citations=False,
            enrich_journal_metrics=False,
        )

    def enrich_citations(self, papers: List[Paper]) -> List[Paper]:
        """
        Enrich papers with citation counts only.

        Args:
            papers: List of papers to enrich

        Returns:
            Same list with citation counts added
        """
        return self.enrich_all(
            papers,
            enrich_impact_factors=False,
            enrich_citations=True,
            enrich_journal_metrics=False,
        )

    # Private methods for impact factor functionality

    def _init_impact_factor_package(self) -> None:
        """Initialize impact_factor package if available."""
        if self.use_impact_factor_package:
            try:
                from impact_factor.core import Factor

                self._impact_factor_instance = Factor()
                logger.info(
                    f"Impact factor package initialized (JCR {JCR_YEAR} data from impact_factor package)"
                )
            except ImportError:
                logger.warning(
                    "impact_factor package not available. Install with: pip install impact-factor\n"
                    "Journal impact factors will use fallback data if available."
                )
                self._impact_factor_instance = None

    def _load_journal_data(self) -> None:
        """Load custom journal data if provided."""
        if self.journal_data_path and self.journal_data_path.exists():
            try:
                import json

                with open(self.journal_data_path, "r") as f:
                    self._journal_data = json.load(f)
                logger.info(
                    f"Loaded custom journal data from {self.journal_data_path}"
                )
            except Exception as e:
                logger.warning(f"Failed to load custom journal data: {e}")
                self._journal_data = None
        else:
            self._journal_data = None

    def _enrich_journal_data(
        self,
        papers: List[Paper],
        include_impact_factors: bool = True,
        include_metrics: bool = True,
    ) -> None:
        """
        Enrich papers with journal-related data.

        Args:
            papers: Papers to enrich (modified in-place)
            include_impact_factors: Add impact factors
            include_metrics: Add quartiles, rankings
        """
        enriched_count: int = 0

        for paper in papers:
            if not paper.journal:
                continue

            metrics = self._get_journal_metrics(paper.journal)
            if not metrics:
                continue

            # Add requested data
            if include_impact_factors and "impact_factor" in metrics:
                paper.impact_factor = metrics["impact_factor"]
                paper.impact_factor_source = metrics.get(
                    "source", f"JCR {JCR_YEAR}"
                )
                paper.metadata["impact_factor_source"] = (
                    paper.impact_factor_source
                )
                enriched_count += 1

            if include_metrics:
                if "quartile" in metrics:
                    paper.journal_quartile = metrics["quartile"]
                    paper.quartile_source = metrics.get(
                        "source", f"JCR {JCR_YEAR}"
                    )
                    paper.metadata["quartile_source"] = paper.quartile_source
                if "rank" in metrics:
                    paper.journal_rank = metrics["rank"]
                if "h_index" in metrics:
                    paper.h_index = metrics["h_index"]

        logger.info(
            f"Enriched {enriched_count}/{len(papers)} papers with journal data"
        )

    def _get_journal_metrics_uncached(
        self, journal_name: str
    ) -> Optional[Dict[str, Any]]:
        """
        Get journal metrics from available sources (uncached version).

        Args:
            journal_name: Name of the journal

        Returns:
            Dictionary with available metrics or None
        """
        metrics: Dict[str, Any] = {}

        # Try impact_factor package first (real 2024 JCR data)
        if self._impact_factor_instance:
            try:
                # Search for journal
                search_results = self._impact_factor_instance.search(
                    journal_name
                )

                if search_results:
                    # Get best match
                    best_match = self._find_best_journal_match(
                        journal_name, [r["journal"] for r in search_results]
                    )

                    if best_match:
                        # Find the matching result
                        for result in search_results:
                            if result["journal"] == best_match:
                                factor_value = result.get("factor")
                                if factor_value is not None:
                                    metrics["impact_factor"] = float(
                                        factor_value
                                    )
                                metrics["quartile"] = result.get(
                                    "jcr", "Unknown"
                                )
                                metrics["rank"] = result.get("rank")
                                metrics["source"] = f"JCR {JCR_YEAR}"
                                break

            except Exception as e:
                logger.debug(
                    f"Impact factor lookup failed for '{journal_name}': {e}"
                )

        # Fall back to custom data if no impact factor found
        if not metrics and self._journal_data:
            normalized_name = self._normalize_journal_name(journal_name)
            if normalized_name in self._journal_data:
                custom_metrics = self._journal_data[normalized_name]
                metrics.update(custom_metrics)
                metrics["source"] = "Custom data"

        return metrics if metrics else None

    def _find_best_journal_match(
        self, query: str, candidates: List[str], threshold: float = 0.85
    ) -> Optional[str]:
        """
        Find best matching journal name from candidates.

        Args:
            query: Journal name to match
            candidates: List of candidate journal names
            threshold: Minimum similarity score

        Returns:
            Best matching journal name or None
        """
        if not candidates:
            return None

        query_normalized = self._normalize_journal_name(query)
        best_match: Optional[str] = None
        best_score: float = 0.0

        for candidate in candidates:
            candidate_normalized = self._normalize_journal_name(candidate)
            score = SequenceMatcher(
                None, query_normalized, candidate_normalized
            ).ratio()

            if score > best_score and score >= threshold:
                best_score = score
                best_match = candidate

        return best_match

    def _normalize_journal_name(self, name: str) -> str:
        """Normalize journal name for matching."""
        import re

        # Convert to lowercase and remove punctuation
        normalized = name.lower()
        normalized = re.sub(r"[^\w\s]", " ", normalized)
        normalized = " ".join(normalized.split())
        return normalized

    # Private methods for citation functionality

    async def _enrich_citations_async(
        self, papers: List[Paper]
    ) -> List[Paper]:
        """
        Asynchronously enrich papers with citation counts.

        Args:
            papers: Papers to enrich

        Returns:
            Same list with citations added where possible
        """
        # Group papers that need citation enrichment
        papers_needing_citations: List[Tuple[int, Paper]] = [
            (i, p) for i, p in enumerate(papers) if p.citation_count is None
        ]

        if not papers_needing_citations:
            logger.info("All papers already have citation counts")
            return papers

        logger.info(
            f"Enriching {len(papers_needing_citations)} papers with citation counts"
        )

        # Create tasks for concurrent enrichment
        tasks = [
            self._get_citation_count_for_paper(paper)
            for _, paper in papers_needing_citations
        ]

        # Run with semaphore to limit concurrent requests
        semaphore = asyncio.Semaphore(5)  # Max 5 concurrent requests

        async def limited_task(task):
            async with semaphore:
                return await task

        results = await asyncio.gather(
            *[limited_task(task) for task in tasks], return_exceptions=True
        )

        # Update papers with results
        enriched_count: int = 0
        for (idx, paper), result in zip(papers_needing_citations, results):
            if isinstance(result, Exception):
                logger.debug(
                    f"Failed to get citations for '{paper.title[:50]}...': {result}"
                )
            elif result is not None:
                paper.citation_count = result
                enriched_count += 1

        logger.info(
            f"Successfully enriched {enriched_count}/{len(papers_needing_citations)} papers with citations"
        )
        return papers

    async def _get_citation_count_for_paper(
        self, paper: Paper
    ) -> Optional[int]:
        """
        Get citation count for a single paper.

        Args:
            paper: Paper to get citation count for

        Returns:
            Citation count or None
        """
        # Try CrossRef first if DOI is available
        if paper.doi:
            try:
                from ..web._search_pubmed import get_crossref_metrics

                metrics = get_crossref_metrics(
                    paper.doi, api_key=self.crossref_api_key, email=self.email
                )
                if metrics and "citations" in metrics:
                    logger.debug(
                        f"Got citation count from CrossRef for {paper.doi}: {metrics['citations']}"
                    )
                    # Update paper metadata to indicate source
                    paper.metadata["citation_count_source"] = "CrossRef"
                    return metrics["citations"]
            except Exception as e:
                logger.debug(
                    f"CrossRef citation lookup failed for {paper.doi}: {e}"
                )

        # Fall back to Semantic Scholar
        # Import here to avoid circular dependency
        from ._SearchEngines import SemanticScholarEngine

        # Build search query
        query: Optional[str] = None
        if paper.doi:
            query = f"doi:{paper.doi}"
        elif paper.title:
            query = paper.title
        else:
            return None

        try:
            # Create Semantic Scholar engine
            semantic_scholar_engine = SemanticScholarEngine(
                api_key=self.semantic_scholar_api_key
            )

            # Search for the paper
            results = await semantic_scholar_engine.search(query, limit=3)

            # Find best match
            for result in results:
                # Check if titles match
                if self._papers_match(paper, result):
                    # Update metadata to indicate source
                    paper.metadata["citation_count_source"] = (
                        "Semantic Scholar"
                    )
                    return result.citation_count

        except Exception as e:
            logger.debug(f"Semantic Scholar citation lookup failed: {e}")

        return None

    def _papers_match(
        self, paper1: Paper, paper2: Paper, threshold: float = 0.85
    ) -> bool:
        """
        Check if two papers are the same.

        Args:
            paper1: First paper
            paper2: Second paper
            threshold: Similarity threshold for title matching

        Returns:
            True if papers match
        """
        # Check DOI match first (most reliable)
        if paper1.doi and paper2.doi:
            return paper1.doi.lower() == paper2.doi.lower()

        # Check title match
        if paper1.title and paper2.title:
            return self._titles_match(paper1.title, paper2.title, threshold)

        return False

    def _titles_match(
        self, title1: str, title2: str, threshold: float = 0.85
    ) -> bool:
        """
        Check if two titles match using fuzzy matching.

        Args:
            title1: First title
            title2: Second title
            threshold: Minimum similarity score

        Returns:
            True if titles match
        """
        if not title1 or not title2:
            return False

        # Normalize titles
        t1 = title1.lower().strip()
        t2 = title2.lower().strip()

        # Exact match
        if t1 == t2:
            return True

        # Fuzzy match
        similarity = SequenceMatcher(None, t1, t2).ratio()
        return similarity >= threshold

    def get_enrichment_stats(self, papers: List[Paper]) -> Dict[str, Any]:
        """
        Get statistics about enrichment coverage.

        Args:
            papers: List of papers to analyze

        Returns:
            Dictionary with enrichment statistics
        """
        total = len(papers)
        if total == 0:
            return {
                "total_papers": 0,
                "with_impact_factor": 0,
                "with_citations": 0,
                "with_quartile": 0,
                "fully_enriched": 0,
                "coverage_percentage": 0.0,
            }

        with_if = sum(1 for p in papers if p.impact_factor is not None)
        with_cite = sum(1 for p in papers if p.citation_count is not None)
        with_quartile = sum(
            1 for p in papers if p.journal_quartile is not None
        )
        fully_enriched = sum(
            1
            for p in papers
            if p.impact_factor is not None and p.citation_count is not None
        )

        return {
            "total_papers": total,
            "with_impact_factor": with_if,
            "with_citations": with_cite,
            "with_quartile": with_quartile,
            "fully_enriched": fully_enriched,
            "impact_factor_coverage": (with_if / total) * 100,
            "citation_coverage": (with_cite / total) * 100,
            "quartile_coverage": (with_quartile / total) * 100,
            "full_coverage": (fully_enriched / total) * 100,
        }


# Convenience functions for backward compatibility


def _enrich_papers_with_all(
    papers: List[Paper],
    semantic_scholar_api_key: Optional[str] = None,
    **kwargs,
) -> List[Paper]:
    """
    Convenience function to enrich papers with all available data.

    Args:
        papers: List of papers to enrich
        semantic_scholar_api_key: Optional API key
        **kwargs: Additional arguments for MetadataEnricher

    Returns:
        Enriched papers
    """
    enricher = MetadataEnricher(
        semantic_scholar_api_key=semantic_scholar_api_key, **kwargs
    )
    return enricher.enrich_all(papers)


def _enrich_papers_with_impact_factors(
    papers: List[Paper], use_impact_factor_package: bool = True
) -> List[Paper]:
    """
    Convenience function to enrich papers with impact factors only.

    Args:
        papers: List of papers
        use_impact_factor_package: Whether to use impact_factor package

    Returns:
        Papers with impact factors
    """
    enricher = MetadataEnricher(
        use_impact_factor_package=use_impact_factor_package
    )
    return enricher.enrich_impact_factors(papers)


def _enrich_papers_with_citations(
    papers: List[Paper], semantic_scholar_api_key: Optional[str] = None
) -> List[Paper]:
    """
    Convenience function to enrich papers with citations only.

    Args:
        papers: List of papers
        semantic_scholar_api_key: Optional API key

    Returns:
        Papers with citation counts
    """
    enricher = MetadataEnricher(
        semantic_scholar_api_key=semantic_scholar_api_key
    )
    return enricher.enrich_citations(papers)


__all__ = [
    "MetadataEnricher",
    "_enrich_papers_with_all",
    "_enrich_papers_with_impact_factors",
    "_enrich_papers_with_citations",
]

# EOF

================================================================================
File: /home/ywatanabe/proj/SciTeX-Code/src/scitex/scholar/_OpenAthensAuthenticator.py
Relative: ../../../../SciTeX-Code/src/scitex/scholar/_OpenAthensAuthenticator.py
Size: 34271 bytes, Lines: 819, Words: ~5711
================================================================================
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Timestamp: "2025-07-24 12:40:00 (ywatanabe)"
# File: /home/ywatanabe/proj/scitex_repo/src/scitex/scholar/_OpenAthensAuthenticator.py
# ----------------------------------------
import os
__FILE__ = (
    "./src/scitex/scholar/_OpenAthensAuthenticator.py"
)
__DIR__ = os.path.dirname(__FILE__)
# ----------------------------------------

"""
OpenAthens authentication for institutional access to academic papers.

This module provides authentication through OpenAthens single sign-on
to enable legal PDF downloads via institutional subscriptions.
"""

import asyncio
import logging
import re
import json
import base64
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
from urllib.parse import urljoin, urlparse

import aiohttp
from playwright.async_api import async_playwright, Page, Browser
from cryptography.fernet import Fernet
from cryptography.hazmat.primitives import hashes
from cryptography.hazmat.primitives.kdf.pbkdf2 import PBKDF2HMAC

from ..errors import ScholarError, SciTeXWarning
import warnings

logger = logging.getLogger(__name__)


class OpenAthensError(ScholarError):
    """Raised when OpenAthens authentication fails."""
    pass


class OpenAthensAuthenticator:
    """
    Handles OpenAthens authentication for institutional access.
    
    OpenAthens is a single sign-on system used by many universities
    and institutions to provide seamless access to academic resources.
    
    This authenticator:
    1. Logs in via the institution's identity provider
    2. Maintains authenticated sessions
    3. Provides authenticated download capabilities
    4. Handles session refresh automatically
    """
    
    def __init__(
        self,
        email: Optional[str] = None,
        cache_dir: Optional[Path] = None,
        timeout: int = 300,  # 5 minutes for manual login
    ):
        """
        Initialize OpenAthens authenticator.
        
        Args:
            email: Institutional email for identification (e.g., 'user@institution.edu')
            cache_dir: Directory for session cache
            timeout: Authentication timeout in seconds (default: 5 minutes)
        
        Note:
            Uses the unified MyAthens interface at https://my.openathens.net/
            Authentication is done manually in the browser.
        """
        self.email = email
        self.myathens_url = "https://my.openathens.net/?passiveLogin=false"
        self.cache_dir = Path(cache_dir or Path.home() / ".scitex" / "scholar" / "openathens_sessions")
        self.timeout = timeout
        
        # Session management
        self._session: Optional[aiohttp.ClientSession] = None
        self._cookies: Dict[str, str] = {}
        self._full_cookies: List[Dict[str, Any]] = []  # Full cookie objects
        self._session_expiry: Optional[datetime] = None
        self._browser: Optional[Browser] = None
        self._context: Optional[Any] = None  # Browser context for tabs
        self._page: Optional[Page] = None
        
        # Encryption setup
        self._cipher = self._setup_encryption()
        
        # Ensure cache directory exists
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        # Log cookie storage location on first use
        if not hasattr(OpenAthensAuthenticator, '_location_logged'):
            logger.info(f"OpenAthens session cookies stored in: {self.cache_dir}")
            OpenAthensAuthenticator._location_logged = True
        
    async def __aenter__(self):
        """Async context manager entry."""
        await self.initialize()
        return self
        
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Async context manager exit."""
        await self.close()
        
    async def initialize(self):
        """Initialize the authenticator."""
        # Load cached session if available
        await self._load_session_cache()
        
    async def close(self):
        """Clean up resources."""
        if self._session:
            await self._session.close()
        if self._page:
            await self._page.close()
        if self._browser:
            await self._browser.close()
            
    async def authenticate(self, force: bool = False) -> bool:
        """
        Authenticate with OpenAthens via manual browser login.
        
        Args:
            force: Force re-authentication even if session exists
            
        Returns:
            True if authentication successful
        
        Note:
            Opens MyAthens (https://my.openathens.net/) for manual login.
            The system will detect successful login and capture the session.
        """
        # Check if we have a valid session
        if not force and await self.is_authenticated():
            logger.info("Using existing OpenAthens session")
            return True
            
        logger.info("Starting manual OpenAthens authentication")
        if self.email:
            logger.info(f"Account: {self.email}")
        
        try:
            # Use Playwright with visible browser for manual login
            async with async_playwright() as p:
                # Always use non-headless mode for manual login
                self._browser = await p.chromium.launch(headless=False)
                self._page = await self._browser.new_page()
                
                # Navigate to unified MyAthens login page
                logger.info(f"Opening MyAthens: {self.myathens_url}")
                await self._page.goto(self.myathens_url, wait_until='networkidle')
                
                # Let user complete login manually
                success = await self._perform_login()
                
                if success:
                    # Extract cookies
                    cookies = await self._page.context.cookies()
                    self._cookies = {c['name']: c['value'] for c in cookies}
                    self._full_cookies = cookies  # Save full cookie objects for reuse
                    self._session_expiry = datetime.now() + timedelta(hours=8)
                    
                    # Save session
                    await self._save_session_cache()
                    
                    logger.info("OpenAthens authentication successful")
                    return True
                else:
                    raise OpenAthensError("Authentication failed or timed out")
                    
        except Exception as e:
            logger.error(f"OpenAthens authentication error: {e}")
            raise OpenAthensError(f"Authentication failed: {str(e)}")
        finally:
            if self._page:
                await self._page.close()
            if self._browser:
                await self._browser.close()
                
    async def _perform_login(self) -> bool:
        """
        Open browser for manual OpenAthens login and capture authentication cookies.
        
        This approach:
        1. Opens the OpenAthens login page
        2. Waits for user to complete login manually
        3. Captures cookies after successful authentication
        """
        try:
            logger.info("Opening browser for manual OpenAthens login")
            
            # Show login instructions
            print("\n" + "="*60)
            print("OpenAthens Authentication Required")
            print("="*60)
            print(f"\nMyAthens login page is opening...")
            if self.email:
                print(f"Account: {self.email}")
            print("\nPlease complete the login process:")
            print("1. Enter your institutional email")
            print("2. Click your institution when it appears") 
            print("3. Complete login on your institution's page")
            print("\nThe system will detect successful login automatically.")
            print("="*60 + "\n")
            
            # Wait a moment for user to read instructions
            await asyncio.sleep(2)
            
            # Monitor for successful login
            max_wait_time = 300  # 5 minutes
            check_interval = 2  # Check every 2 seconds
            elapsed_time = 0
            
            while elapsed_time < max_wait_time:
                current_url = self._page.url
                
                # Check for various success indicators
                # First check URL-based indicators (safe during navigation)
                url_indicators = [
                    # MyAthens account/app pages mean logged in
                    'my.openathens.net/account' in current_url,
                    'my.openathens.net/app' in current_url,
                    'openathens.net/account' in current_url.lower(),
                    
                    # Redirected to institution's authenticated area
                    'ezproxy' in current_url.lower(),
                    '/secure/' in current_url,
                    'sso.unimelb.edu.au' in current_url,  # Institution SSO
                ]
                
                # If URL indicates possible success, check for logout button
                has_logout_button = False
                if any(url_indicators):
                    try:
                        # Use evaluate to avoid context destruction issues
                        has_logout_button = await self._page.evaluate('''
                            () => {
                                const links = document.querySelectorAll('a, button');
                                for (const link of links) {
                                    const text = link.textContent.toLowerCase();
                                    if (text.includes('logout') || text.includes('sign out') || text.includes('log out')) {
                                        return true;
                                    }
                                }
                                return false;
                            }
                        ''')
                    except:
                        # Ignore errors during navigation
                        pass
                
                success_indicators = url_indicators + [has_logout_button]
                
                if any(success_indicators):
                    logger.info("Login successful - capturing session")
                    print("\n✓ Login detected! Capturing session...")
                    
                    # Log captured cookies for debugging
                    cookies = await self._page.context.cookies()
                    logger.debug(f"Captured {len(cookies)} cookies")
                    
                    # Show important cookies (without values for security)
                    important_cookies = []
                    for cookie in cookies:
                        if any(key in cookie['name'].lower() for key in ['auth', 'session', 'token', 'openathens']):
                            important_cookies.append(cookie['name'])
                    
                    if important_cookies:
                        logger.info(f"Important cookies captured: {', '.join(important_cookies)}")
                    
                    return True
                
                # Show progress to user
                if elapsed_time % 10 == 0 and elapsed_time > 0:
                    print(f"Waiting for login... ({elapsed_time}s elapsed)")
                
                await asyncio.sleep(check_interval)
                elapsed_time += check_interval
            
            print("\n✗ Login timeout - please try again")
            return False
            
        except Exception as e:
            logger.error(f"Manual login process failed: {e}")
            return False
            
    async def is_authenticated(self) -> bool:
        """Check if we have a valid authenticated session."""
        if not self._cookies or not self._session_expiry:
            return False
            
        if datetime.now() > self._session_expiry:
            logger.info("OpenAthens session expired")
            return False
            
        return True
        
    async def download_with_auth(
        self,
        url: str,
        output_path: Path,
        chunk_size: int = 8192
    ) -> Optional[Path]:
        """
        Download a file using OpenAthens authenticated browser session.
        
        Args:
            url: URL to download
            output_path: Where to save the file
            chunk_size: Not used (kept for compatibility)
            
        Returns:
            Path to downloaded file or None if failed
        """
        if not await self.is_authenticated():
            raise OpenAthensError("Not authenticated. Call authenticate() first.")
            
        try:
            # Use browser for download
            logger.info(f"Downloading via OpenAthens browser: {url}")
            
            async with async_playwright() as p:
                browser = await p.chromium.launch(headless=True)  # Can be headless for downloads
                context = await browser.new_context()
                
                # Add cookies to browser context
                if self._full_cookies:
                    # Use full cookie objects if available
                    await context.add_cookies(self._full_cookies)
                else:
                    # Fallback to simple cookies
                    await context.add_cookies([
                        {
                            'name': name,
                            'value': value,
                            'domain': '.openathens.net',
                            'path': '/'
                        }
                        for name, value in self._cookies.items()
                    ])
                
                page = await context.new_page()
                
                # Set up download handling
                download_path = output_path.parent
                download_path.mkdir(parents=True, exist_ok=True)
                
                # Navigate to URL
                try:
                    # For PDFs, don't wait for networkidle as they may be large
                    wait_until = 'domcontentloaded' if 'pdf' in url.lower() else 'networkidle'
                    response = await page.goto(url, wait_until=wait_until, timeout=60000)
                    
                    # Handle common popups
                    await self._handle_popups(page)
                    
                    # Wait a bit for any redirects
                    await asyncio.sleep(2)
                    
                    # Check if we have a PDF
                    content_type = await page.evaluate('() => document.contentType || ""')
                    current_url = page.url
                    
                    if 'pdf' in content_type.lower() or 'pdf' in current_url.lower():
                        # We're on a PDF page
                        # For PDFs displayed in browser, we need to trigger download
                        
                        # Start waiting for download
                        async with page.expect_download() as download_info:
                            # Try different methods to trigger download
                            
                            # Method 1: Ctrl+S
                            await page.keyboard.press('Control+s')
                            await asyncio.sleep(1)
                            
                            # Method 2: Look for download button
                            download_btn = await page.query_selector(
                                'a[download], button:has-text("Download"), a:has-text("Download PDF")'
                            )
                            if download_btn:
                                await download_btn.click()
                            
                            try:
                                download = await download_info.value
                                await download.save_as(output_path)
                                logger.info(f"Downloaded via browser: {output_path}")
                                await browser.close()
                                return output_path
                            except:
                                pass
                        
                        # If download didn't work, try direct fetch of the PDF URL
                        pdf_content = await page.evaluate('''
                            async () => {
                                const response = await fetch(window.location.href);
                                const buffer = await response.arrayBuffer();
                                return btoa(String.fromCharCode(...new Uint8Array(buffer)));
                            }
                        ''')
                        
                        if pdf_content:
                            import base64
                            with open(output_path, 'wb') as f:
                                f.write(base64.b64decode(pdf_content))
                            logger.info(f"Downloaded via fetch: {output_path}")
                            await browser.close()
                            return output_path
                    
                    else:
                        # Not a PDF page, look for download link
                        logger.debug(f"Not a PDF page: {content_type}")
                        download_link = await page.query_selector(
                            'a[href*=".pdf"], a:has-text("Download PDF"), button:has-text("Download")'
                        )
                        
                        if download_link:
                            async with page.expect_download() as download_info:
                                await download_link.click()
                                download = await download_info.value
                                await download.save_as(output_path)
                                logger.info(f"Downloaded via link: {output_path}")
                                await browser.close()
                                return output_path
                                
                except Exception as e:
                    logger.error(f"Browser download failed: {e}")
                finally:
                    await browser.close()
                    
        except Exception as e:
            logger.error(f"OpenAthens download failed: {e}")
            
        return None
            
    async def _handle_popups(self, page) -> None:
        """
        Handle common popups like cookie consent, notifications, etc.
        """
        try:
            # Common cookie consent selectors
            cookie_selectors = [
                'button:has-text("Accept")',
                'button:has-text("Accept all")',
                'button:has-text("Accept cookies")',
                'button:has-text("I agree")',
                'button:has-text("OK")',
                'button[id*="accept"]',
                'button[class*="accept"]',
                'a:has-text("Accept")',
            ]
            
            for selector in cookie_selectors:
                try:
                    button = await page.wait_for_selector(selector, timeout=2000)
                    if button:
                        await button.click()
                        logger.debug(f"Clicked popup button: {selector}")
                        await asyncio.sleep(1)
                        break
                except:
                    continue
                    
            # Close any notification popups
            close_selectors = [
                'button[aria-label="Close"]',
                'button:has-text("Close")',
                'button:has-text("×")',
                'button.close',
                'a.close',
            ]
            
            for selector in close_selectors:
                try:
                    button = await page.query_selector(selector)
                    if button:
                        await button.click()
                        logger.debug(f"Closed popup: {selector}")
                except:
                    continue
                    
        except Exception as e:
            logger.debug(f"Popup handling error (non-critical): {e}")
            
    async def _handle_publisher_download(
        self,
        url: str,
        output_path: Path
    ) -> Optional[Path]:
        """
        Handle publisher-specific download flows.
        
        Some publishers require additional navigation after OpenAthens auth.
        """
        # This would be extended with publisher-specific logic
        # For now, return None to fall back to other methods
        logger.debug(f"Publisher-specific download not implemented for {url}")
        return None
    
    async def download_batch(
        self,
        downloads: List[Tuple[str, Path]],
        max_concurrent: int = 3
    ) -> Dict[str, Optional[Path]]:
        """
        Download multiple PDFs concurrently using browser tabs.
        
        Args:
            downloads: List of (url, output_path) tuples
            max_concurrent: Maximum number of concurrent downloads
            
        Returns:
            Dict mapping URLs to downloaded paths (or None if failed)
        """
        if not await self.is_authenticated():
            raise OpenAthensError("Not authenticated. Call authenticate() first.")
            
        results = {}
        
        try:
            logger.info(f"Starting batch download of {len(downloads)} PDFs")
            
            async with async_playwright() as p:
                # Use persistent browser context for session
                browser = await p.chromium.launch(headless=True)
                context = await browser.new_context()
                
                # Add cookies to context
                if self._full_cookies:
                    # Use full cookie objects if available
                    await context.add_cookies(self._full_cookies)
                else:
                    # Fallback to simple cookies
                    await context.add_cookies([
                        {
                            'name': name,
                            'value': value,
                            'domain': '.openathens.net',
                            'path': '/'
                        }
                        for name, value in self._cookies.items()
                    ])
                
                # Process downloads in batches
                for i in range(0, len(downloads), max_concurrent):
                    batch = downloads[i:i + max_concurrent]
                    
                    # Create tasks for concurrent downloads
                    tasks = []
                    for url, output_path in batch:
                        task = self._download_in_tab(context, url, output_path)
                        tasks.append(task)
                    
                    # Wait for batch to complete
                    batch_results = await asyncio.gather(*tasks, return_exceptions=True)
                    
                    # Process results
                    for (url, output_path), result in zip(batch, batch_results):
                        if isinstance(result, Exception):
                            logger.error(f"Download failed for {url}: {result}")
                            results[url] = None
                        else:
                            results[url] = result
                            
                await browser.close()
                
        except Exception as e:
            logger.error(f"Batch download error: {e}")
            
        # Log summary
        successful = sum(1 for path in results.values() if path is not None)
        logger.info(f"Batch download complete: {successful}/{len(downloads)} successful")
        
        return results
    
    async def _download_in_tab(
        self,
        context,
        url: str,
        output_path: Path
    ) -> Optional[Path]:
        """
        Download a single PDF in a browser tab.
        
        Args:
            context: Browser context
            url: URL to download
            output_path: Where to save the file
            
        Returns:
            Path to downloaded file or None if failed
        """
        page = None
        try:
            # Create new tab
            page = await context.new_page()
            logger.debug(f"Opening tab for: {url}")
            
            # Navigate to URL
            await page.goto(url, wait_until='networkidle', timeout=60000)
            
            # Handle popups
            await self._handle_popups(page)
            
            # Wait for page to stabilize
            await asyncio.sleep(2)
            
            # Check content type
            content_type = await page.evaluate('() => document.contentType || ""')
            current_url = page.url
            
            if 'pdf' in content_type.lower() or 'pdf' in current_url.lower():
                # PDF page - try to download
                
                # Method 1: If it's a direct PDF URL, fetch the content
                if current_url.endswith('.pdf') or 'pdf' in content_type.lower():
                    try:
                        # Use page's network context to download
                        response = await page.evaluate(f'''
                            async () => {{
                                const response = await fetch("{current_url}");
                                const blob = await response.blob();
                                const buffer = await blob.arrayBuffer();
                                return btoa(String.fromCharCode(...new Uint8Array(buffer)));
                            }}
                        ''')
                        
                        if response:
                            import base64
                            output_path.parent.mkdir(parents=True, exist_ok=True)
                            with open(output_path, 'wb') as f:
                                f.write(base64.b64decode(response))
                            logger.info(f"Downloaded: {output_path.name}")
                            return output_path
                    except Exception as e:
                        logger.debug(f"Direct fetch failed: {e}")
                
                # Method 2: Look for download button
                download_btn = await page.query_selector(
                    'a[download], button:has-text("Download"), a:has-text("Download PDF")'
                )
                if download_btn:
                    # Set up download handler
                    async with page.expect_download() as download_info:
                        await download_btn.click()
                        try:
                            download = await asyncio.wait_for(download_info.value, timeout=30)
                            await download.save_as(output_path)
                            logger.info(f"Downloaded via button: {output_path.name}")
                            return output_path
                        except asyncio.TimeoutError:
                            logger.debug("Download timeout")
                            
            else:
                # Not a PDF page - look for PDF link
                pdf_link = await page.query_selector(
                    'a[href*=".pdf"], a:has-text("Download PDF"), a:has-text("PDF")'
                )
                
                if pdf_link:
                    # Click and download
                    async with page.expect_download() as download_info:
                        await pdf_link.click()
                        try:
                            download = await asyncio.wait_for(download_info.value, timeout=30)
                            await download.save_as(output_path)
                            logger.info(f"Downloaded via link: {output_path.name}")
                            return output_path
                        except asyncio.TimeoutError:
                            logger.debug("Download timeout")
                            
        except Exception as e:
            logger.error(f"Tab download error for {url}: {e}")
        finally:
            if page:
                await page.close()
                
        return None
        
    async def get_authenticated_url(self, url: str) -> str:
        """
        Get an OpenAthens-authenticated URL.
        
        Note: With manual authentication through MyAthens, we rely on
        cookies rather than URL rewriting.
        """
        if not await self.is_authenticated():
            raise OpenAthensError("Not authenticated")
            
        # With MyAthens manual login, we use the original URL with cookies
        return url
    
    def _setup_encryption(self) -> Fernet:
        """
        Setup encryption for cookie storage.
        
        Uses a key derived from the user's email and a machine-specific salt.
        This provides reasonable security while being reproducible on the same machine.
        """
        # Get or create a machine-specific salt
        salt_file = self.cache_dir.parent / ".scitex_salt"
        if salt_file.exists():
            with open(salt_file, 'rb') as f:
                salt = f.read()
        else:
            salt = os.urandom(16)
            salt_file.parent.mkdir(parents=True, exist_ok=True)
            with open(salt_file, 'wb') as f:
                f.write(salt)
            # Restrict permissions
            os.chmod(salt_file, 0o600)
        
        # Derive key from email (or default) and salt
        password = (self.email or "default").encode()
        kdf = PBKDF2HMAC(
            algorithm=hashes.SHA256(),
            length=32,
            salt=salt,
            iterations=100000,
        )
        key = base64.urlsafe_b64encode(kdf.derive(password))
        return Fernet(key)
    
    def _encrypt_data(self, data: dict) -> str:
        """Encrypt sensitive data."""
        json_data = json.dumps(data)
        encrypted = self._cipher.encrypt(json_data.encode())
        return base64.urlsafe_b64encode(encrypted).decode()
    
    def _decrypt_data(self, encrypted_data: str) -> dict:
        """Decrypt sensitive data."""
        try:
            decoded = base64.urlsafe_b64decode(encrypted_data.encode())
            decrypted = self._cipher.decrypt(decoded)
            return json.loads(decrypted.decode())
        except Exception as e:
            logger.warning(f"Failed to decrypt session data: {e}")
            return None
        
    async def _save_session_cache(self):
        """Save session cookies to cache."""
        # Use email domain or 'default' for cache file name
        cache_name = "default"
        if self.email and "@" in self.email:
            cache_name = self.email.split("@")[1].replace(".", "_")
        
        cache_file = self.cache_dir / f"openathens_{cache_name}_session.enc"
        
        cache_data = {
            'cookies': self._cookies,
            'full_cookies': self._full_cookies,  # Save full cookie objects
            'expiry': self._session_expiry.isoformat() if self._session_expiry else None,
            'email': self.email,
        }
        
        # Encrypt the data
        encrypted_data = self._encrypt_data(cache_data)
        
        # Save encrypted data
        with open(cache_file, 'w') as f:
            json.dump({
                'version': 1,
                'encrypted': encrypted_data,
                'email_hash': base64.urlsafe_b64encode(
                    (self.email or "default").encode()
                ).decode()[:16]  # For identification only
            }, f)
        
        # Restrict file permissions
        os.chmod(cache_file, 0o600)
        logger.debug(f"Session saved to: {cache_file}")
            
    async def _load_session_cache(self):
        """Load session cookies from cache."""
        # Use email domain or 'default' for cache file name
        cache_name = "default"
        if self.email and "@" in self.email:
            cache_name = self.email.split("@")[1].replace(".", "_")
        
        # Try encrypted file first, then legacy JSON
        cache_file_enc = self.cache_dir / f"openathens_{cache_name}_session.enc"
        cache_file_json = self.cache_dir / f"openathens_{cache_name}_session.json"
        
        cache_file = cache_file_enc if cache_file_enc.exists() else cache_file_json
        
        if not cache_file.exists():
            return
            
        try:
            with open(cache_file, 'r') as f:
                file_data = json.load(f)
            
            # Check if it's encrypted format
            if 'encrypted' in file_data:
                # Decrypt the data
                cache_data = self._decrypt_data(file_data['encrypted'])
                if not cache_data:
                    logger.warning("Failed to decrypt session cache")
                    return
            else:
                # Legacy unencrypted format
                cache_data = file_data
                logger.info("Migrating unencrypted session to encrypted format")
                
            # Load if email matches or no email specified
            if not self.email or cache_data.get('email') == self.email:
                self._cookies = cache_data.get('cookies', {})
                self._full_cookies = cache_data.get('full_cookies', [])
                expiry_str = cache_data.get('expiry')
                if expiry_str:
                    self._session_expiry = datetime.fromisoformat(expiry_str)
                
                # If loaded from legacy format, save as encrypted
                if 'encrypted' not in file_data and self._cookies:
                    await self._save_session_cache()
                    # Remove old unencrypted file
                    if cache_file_json.exists():
                        cache_file_json.unlink()
                        logger.info("Removed unencrypted session file")
                    
        except Exception as e:
            logger.debug(f"Failed to load session cache: {e}")

================================================================================
File: /home/ywatanabe/proj/SciTeX-Code/src/scitex/scholar/_Paper.py
Relative: ../../../../SciTeX-Code/src/scitex/scholar/_Paper.py
Size: 13966 bytes, Lines: 373, Words: ~2327
================================================================================
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Timestamp: "2025-07-23 10:35:00 (ywatanabe)"
# File: /home/ywatanabe/proj/scitex_repo/src/scitex/scholar/_Paper.py
# ----------------------------------------
import os
__FILE__ = (
    "./src/scitex/scholar/_Paper.py"
)
__DIR__ = os.path.dirname(__FILE__)
# ----------------------------------------

"""
Paper class for SciTeX Scholar module.

Represents a scientific paper with comprehensive metadata and methods.
"""

import re
import json
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional, Union
from datetime import datetime
from difflib import SequenceMatcher

from ..errors import ScholarError

logger = logging.getLogger(__name__)


class Paper:
    """
    Represents a scientific paper with comprehensive metadata.
    
    This class consolidates functionality from _paper.py, _paper_enhanced.py,
    and includes enrichment capabilities.
    """
    
    def __init__(self, 
                 title: str,
                 authors: List[str],
                 abstract: str,
                 source: str,
                 year: Optional[Union[int, str]] = None,
                 doi: Optional[str] = None,
                 pmid: Optional[str] = None,
                 arxiv_id: Optional[str] = None,
                 journal: Optional[str] = None,
                 keywords: Optional[List[str]] = None,
                 citation_count: Optional[int] = None,
                 pdf_url: Optional[str] = None,
                 pdf_path: Optional[Path] = None,
                 impact_factor: Optional[float] = None,
                 journal_quartile: Optional[str] = None,
                 journal_rank: Optional[int] = None,
                 h_index: Optional[int] = None,
                 metadata: Optional[Dict[str, Any]] = None):
        """Initialize paper with comprehensive metadata."""
        self.title = title
        self.authors = authors
        self.abstract = abstract
        self.source = source
        self.year = str(year) if year else None
        self.doi = doi
        self.pmid = pmid
        self.arxiv_id = arxiv_id
        self.journal = journal
        self.keywords = keywords or []
        self.citation_count = citation_count
        self.pdf_url = pdf_url
        self.pdf_path = Path(pdf_path) if pdf_path else None
        
        # Enriched metadata
        self.impact_factor = impact_factor
        self.journal_quartile = journal_quartile
        self.journal_rank = journal_rank
        self.h_index = h_index
        
        # Additional metadata
        self.metadata = metadata or {}
        
        # Track data sources
        self.citation_count_source = self.metadata.get('citation_count_source', None)
        self.impact_factor_source = self.metadata.get('impact_factor_source', None)
        self.quartile_source = self.metadata.get('quartile_source', None)
        
        # Track all sources where this paper was found (for deduplication)
        self.all_sources = self.metadata.get('all_sources', [source] if source else [])
        if self.all_sources and source not in self.all_sources:
            self.all_sources.append(source)
        
        # Computed properties
        self._bibtex_key = None
        self._formatted_authors = None
    
    def __str__(self) -> str:
        """String representation of the paper."""
        authors_str = self.authors[0] if self.authors else "Unknown"
        if len(self.authors) > 1:
            authors_str += " et al."
        
        year_str = f" ({self.year})" if self.year else ""
        journal_str = f" - {self.journal}" if self.journal else ""
        
        return f"{authors_str}{year_str}. {self.title}{journal_str}"
    
    def __repr__(self) -> str:
        """Developer-friendly representation."""
        return f"Paper(title='{self.title[:50]}...', authors={len(self.authors)}, year={self.year})"
    
    def get_identifier(self) -> str:
        """
        Get unique identifier for the paper.
        Priority: DOI > PMID > arXiv ID > title-based hash
        """
        if self.doi:
            return f"doi:{self.doi}"
        elif self.pmid:
            return f"pmid:{self.pmid}"
        elif self.arxiv_id:
            return f"arxiv:{self.arxiv_id}"
        else:
            # Create deterministic hash from title and first author
            import hashlib
            text = f"{self.title}_{self.authors[0] if self.authors else 'unknown'}"
            return f"hash:{hashlib.md5(text.encode()).hexdigest()[:12]}"
    
    def _to_bibtex(self, include_enriched: bool = True) -> str:
        """
        Convert paper to BibTeX format.
        
        Args:
            include_enriched: Include enriched metadata (impact factor, etc.)
            
        Returns:
            BibTeX formatted string
        """
        # Generate BibTeX key if not cached
        if not self._bibtex_key:
            self._generate_bibtex_key()
        
        # Determine entry type
        if self.arxiv_id:
            entry_type = "misc"
        elif self.journal:
            entry_type = "article"
        else:
            entry_type = "misc"
        
        # Build BibTeX entry
        lines = [f"@{entry_type}{{{self._bibtex_key},"]
        
        # Required fields
        lines.append(f'  title = {{{self._escape_bibtex(self.title)}}},')
        lines.append(f'  author = {{{self._format_authors_bibtex()}}},')
        
        # Optional fields
        if self.year:
            lines.append(f'  year = {{{self.year}}},')
        
        if self.journal:
            lines.append(f'  journal = {{{self._escape_bibtex(self.journal)}}},')
        
        if self.doi:
            lines.append(f'  doi = {{{self.doi}}},')
        
        if self.arxiv_id:
            lines.append(f'  eprint = {{{self.arxiv_id}}},')
            lines.append('  archivePrefix = {arXiv},')
        
        if self.abstract:
            abstract_escaped = self._escape_bibtex(self.abstract)
            lines.append(f'  abstract = {{{abstract_escaped}}},')
        
        if self.keywords:
            keywords_str = ", ".join(self.keywords)
            lines.append(f'  keywords = {{{keywords_str}}},')
        
        # Enriched metadata
        if include_enriched:
            # Get JCR year dynamically from enrichment module
            from ._MetadataEnricher import JCR_YEAR
            
            if self.impact_factor is not None:
                # Only add if it's a real value (not 0.0)
                if self.impact_factor > 0:
                    lines.append(f'  JCR_{JCR_YEAR}_impact_factor = {{{self.impact_factor}}},')
                    if self.impact_factor_source:
                        lines.append(f'  impact_factor_source = {{{self.impact_factor_source}}},')
            
            if self.journal_quartile and self.journal_quartile != 'Unknown':
                lines.append(f'  JCR_{JCR_YEAR}_quartile = {{{self.journal_quartile}}},')
                if self.quartile_source:
                    lines.append(f'  quartile_source = {{{self.quartile_source}}},')
            
            if self.citation_count is not None:
                lines.append(f'  citation_count = {{{self.citation_count}}},')
                # Add citation source if available
                if self.citation_count_source:
                    lines.append(f'  citation_count_source = {{{self.citation_count_source}}},')
        
        # Add note about SciTeX
        lines.append('  note = {Generated by SciTeX Scholar}')
        
        lines.append('}')
        
        return '\n'.join(lines)
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert paper to dictionary format."""
        return {
            'title': self.title,
            'authors': self.authors,
            'abstract': self.abstract,
            'year': self.year,
            'journal': self.journal,
            'doi': self.doi,
            'pmid': self.pmid,
            'arxiv_id': self.arxiv_id,
            'keywords': self.keywords,
            'citation_count': self.citation_count,
            'citation_count_source': self.citation_count_source,
            'impact_factor': self.impact_factor,
            'impact_factor_source': self.impact_factor_source,
            'journal_quartile': self.journal_quartile,
            'journal_rank': self.journal_rank,
            'h_index': self.h_index,
            'pdf_url': self.pdf_url,
            'pdf_path': str(self.pdf_path) if self.pdf_path else None,
            'source': self.source,
            'metadata': self.metadata
        }
    
    def similarity_score(self, other: 'Paper') -> float:
        """
        Calculate similarity score with another paper.
        
        Returns:
            Score between 0 and 1 (1 = identical)
        """
        # If both have DOIs and they match, they're the same paper
        if self.doi and other.doi and self.doi == other.doi:
            return 1.0
        
        # Title similarity (40% weight)
        if self.title and other.title:
            title_sim = SequenceMatcher(None, 
                                       self.title.lower(), 
                                       other.title.lower()).ratio() * 0.4
        else:
            title_sim = 0
        
        # Author similarity (20% weight)
        if self.authors and other.authors:
            # Check first author match
            author_sim = 0.2 if self.authors[0].lower() == other.authors[0].lower() else 0
        else:
            author_sim = 0
        
        # Abstract similarity (30% weight)
        if self.abstract and other.abstract:
            abstract_sim = SequenceMatcher(None,
                                         self.abstract[:200].lower(),
                                         other.abstract[:200].lower()).ratio() * 0.3
        else:
            abstract_sim = 0
        
        # Year similarity (10% weight)
        if self.year and other.year:
            year_diff = abs(int(self.year) - int(other.year))
            year_sim = max(0, 1 - year_diff / 10) * 0.1
        else:
            year_sim = 0
        
        return title_sim + author_sim + abstract_sim + year_sim
    
    def _generate_bibtex_key(self) -> None:
        """Generate BibTeX citation key."""
        # Get first author last name
        if self.authors:
            first_author = self.authors[0]
            # Handle "Last, First" format
            if ',' in first_author:
                last_name = first_author.split(',')[0].strip()
            else:
                # Handle "First Last" format
                last_name = first_author.split()[-1]
            
            last_name = re.sub(r'[^a-zA-Z]', '', last_name).lower()
        else:
            last_name = "unknown"
        
        # Get year
        year = self.year or "0000"
        
        # Get first significant word from title
        title_words = re.findall(r'\b\w+\b', self.title.lower())
        # Skip common words
        stop_words = {'a', 'an', 'the', 'of', 'in', 'on', 'at', 'to', 'for', 
                     'and', 'or', 'but', 'with', 'by', 'from'}
        significant_words = [w for w in title_words if w not in stop_words and len(w) > 3]
        
        if significant_words:
            title_part = significant_words[0][:6]
        else:
            title_part = title_words[0][:6] if title_words else "paper"
        
        self._bibtex_key = f"{last_name}{year}{title_part}"
    
    def _format_authors_bibtex(self) -> str:
        """Format authors for BibTeX."""
        if not self._formatted_authors:
            self._formatted_authors = " and ".join(self.authors)
        return self._formatted_authors
    
    def _escape_bibtex(self, text: str) -> str:
        """Escape special characters for BibTeX."""
        # Handle special characters
        replacements = {
            '\\': r'\\',
            '{': r'\{',
            '}': r'\}',
            '_': r'\_',
            '&': r'\&',
            '%': r'\%',
            '$': r'\$',
            '#': r'\#',
            '~': r'\textasciitilde{}',
            '^': r'\textasciicircum{}'
        }
        
        for old, new in replacements.items():
            text = text.replace(old, new)
        
        return text
    
    def save(self, output_path: Union[str, Path], format: Optional[str] = None) -> None:
        """
        Save single paper to file.
        
        Simple save method - just writes the file without extra features.
        For symlinks, verbose output, etc., use scitex.io.save() instead.
        
        Args:
            output_path: Output file path
            format: Output format ('bibtex', 'json'). Auto-detected from extension if None.
        """
        output_path = Path(output_path)
        
        # Auto-detect format from extension
        if format is None:
            ext = output_path.suffix.lower()
            if ext in ['.bib', '.bibtex']:
                format = 'bibtex'
            elif ext == '.json':
                format = 'json'
            else:
                format = 'bibtex'
        
        # Ensure parent directory exists
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        if format.lower() == "bibtex":
            # Write BibTeX content
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(f"% BibTeX entry\n")
                f.write(f"% Generated by SciTeX Scholar on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
                f.write(self._to_bibtex())
        
        elif format.lower() == "json":
            # Write JSON
            import json
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(self.to_dict(), f, indent=2, ensure_ascii=False)
        
        else:
            raise ValueError(f"Unsupported format for Paper: {format}")


# Export all classes and functions
__all__ = ['Paper']

================================================================================
File: /home/ywatanabe/proj/SciTeX-Code/src/scitex/scholar/_Papers.py
Relative: ../../../../SciTeX-Code/src/scitex/scholar/_Papers.py
Size: 46693 bytes, Lines: 1156, Words: ~7782
================================================================================
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Timestamp: "2025-07-23 10:40:00 (ywatanabe)"
# File: /home/ywatanabe/proj/scitex_repo/src/scitex/scholar/_Papers.py
# ----------------------------------------
import os
__FILE__ = (
    "./src/scitex/scholar/_Papers.py"
)
__DIR__ = os.path.dirname(__FILE__)
# ----------------------------------------

"""
Papers class for SciTeX Scholar module.

A collection of papers with analysis and export capabilities.
"""

import re
import json
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional, Union, Iterator
from datetime import datetime
import pandas as pd
from difflib import SequenceMatcher

from ..errors import ScholarError
from ._Paper import Paper

logger = logging.getLogger(__name__)


class Papers:
    """
    A collection of papers with analysis and export capabilities.
    
    Provides fluent interface for filtering, sorting, and batch operations.
    """
    
    def __init__(self, papers: List[Paper], auto_deduplicate: bool = True, source_priority: List[str] = None):
        """
        Initialize collection with list of papers.
        
        Args:
            papers: List of Paper objects
            auto_deduplicate: Automatically remove duplicates (default: True)
            source_priority: List of sources in priority order for deduplication
        """
        self._papers = papers
        self._enriched = False
        self._df_cache = None
        self._source_priority = source_priority
        
        # Automatically deduplicate unless explicitly disabled
        if auto_deduplicate and papers:
            self._deduplicate_in_place(source_priority=source_priority)
    
    @classmethod
    def from_bibtex(cls, bibtex_input: Union[str, Path]) -> 'Papers':
        """
        Create Papers from BibTeX file path or content string.
        
        This method intelligently detects whether the input is a file path or
        BibTeX content string and handles it appropriately.
        
        Args:
            bibtex_input: Either:
                - Path to a BibTeX file (str or Path object)
                - BibTeX content as a string
        
        Returns:
            Papers instance
            
        Examples:
            >>> # From file path
            >>> collection = Papers.from_bibtex("papers.bib")
            >>> collection = Papers.from_bibtex(Path("~/refs/papers.bib"))
            
            >>> # From BibTeX content string
            >>> bibtex_content = '''@article{example2023,
            ...     title = {Example Paper},
            ...     author = {John Doe},
            ...     year = {2023}
            ... }'''
            >>> collection = Papers.from_bibtex(bibtex_content)
        """
        # Detect if input is a file path or content
        is_path = False
        
        # Convert to string for checking
        input_str = str(bibtex_input)
        
        # Check if it's likely a path
        if len(input_str) < 500:  # Paths are typically shorter than BibTeX content
            # Check for path-like characteristics
            if (input_str.endswith('.bib') or 
                input_str.endswith('.bibtex') or
                '/' in input_str or 
                '\\' in input_str or
                input_str.startswith('~') or
                input_str.startswith('.') or
                os.path.exists(os.path.expanduser(input_str))):
                is_path = True
        
        # If it contains @ at the beginning of a line, it's likely content
        if '\n@' in input_str or input_str.strip().startswith('@'):
            is_path = False
        
        # Delegate to appropriate method
        if is_path:
            return cls._from_bibtex_file(input_str)
        else:
            return cls._from_bibtex_text(input_str)
    
    @classmethod
    def _from_bibtex_file(cls, file_path: Union[str, Path]) -> 'Papers':
        """
        Create Papers from a BibTeX file.
        
        Args:
            file_path: Path to the BibTeX file
            
        Returns:
            Papers instance
        """
        # Load from file
        bibtex_path = Path(os.path.expanduser(str(file_path)))
        if not bibtex_path.exists():
            raise ScholarError(f"BibTeX file not found: {bibtex_path}")
        
        # Use scitex.io to load the file
        from scitex.io import load
        entries = load(str(bibtex_path))
        logger.info(f"Loaded {len(entries)} entries from {bibtex_path}")
        
        # Convert entries to Paper objects
        papers = []
        for entry in entries:
            paper = cls._bibtex_entry_to_paper(entry)
            if paper:
                papers.append(paper)
        
        logger.info(f"Created Papers with {len(papers)} papers from file")
        return cls(papers, auto_deduplicate=True)
    
    @classmethod
    def _from_bibtex_text(cls, bibtex_content: str) -> 'Papers':
        """
        Create Papers from BibTeX content string.
        
        Args:
            bibtex_content: BibTeX content as a string
            
        Returns:
            Papers instance
        """
        # Parse BibTeX content directly
        # Write to temp file and load
        import tempfile
        with tempfile.NamedTemporaryFile(mode='w', suffix='.bib', delete=False) as f:
            f.write(bibtex_content)
            temp_path = f.name
        
        try:
            from scitex.io import load
            entries = load(temp_path)
        finally:
            import os
            os.unlink(temp_path)
        logger.info(f"Parsed {len(entries)} entries from BibTeX content")
        
        # Convert entries to Paper objects
        papers = []
        for entry in entries:
            paper = cls._bibtex_entry_to_paper(entry)
            if paper:
                papers.append(paper)
        
        logger.info(f"Created Papers with {len(papers)} papers from text")
        return cls(papers, auto_deduplicate=True)
    
    @staticmethod
    def _bibtex_entry_to_paper(entry: Dict[str, Any]) -> Optional[Paper]:
        """
        Convert a BibTeX entry dict to a Paper object.
        
        Args:
            entry: BibTeX entry dictionary with 'entry_type', 'key', and 'fields'
            
        Returns:
            Paper object or None if conversion fails
        """
        try:
            # Extract fields
            fields = entry.get('fields', {})
            
            # Required fields
            title = fields.get('title', '')
            if not title:
                logger.warning(f"Skipping entry {entry.get('key', 'unknown')} - no title")
                return None
            
            # Authors
            author_str = fields.get('author', '')
            authors = []
            if author_str:
                # Split by ' and ' for BibTeX format
                authors = [a.strip() for a in author_str.split(' and ')]
            
            # Create Paper object
            paper = Paper(
                title=title,
                authors=authors,
                abstract=fields.get('abstract', ''),
                source='bibtex',
                year=fields.get('year'),
                doi=fields.get('doi'),
                pmid=fields.get('pmid'),
                arxiv_id=fields.get('eprint'),  # arXiv ID often stored as eprint
                journal=fields.get('journal'),
                keywords=fields.get('keywords', '').split(', ') if fields.get('keywords') else [],
                metadata={'bibtex_key': entry.get('key', ''), 'bibtex_entry_type': entry.get('entry_type', 'misc')}
            )
            
            # Store original BibTeX fields for later reconstruction
            paper._original_bibtex_fields = fields.copy()
            paper._bibtex_entry_type = entry.get('entry_type', 'misc')
            paper._bibtex_key = entry.get('key', '')
            
            # Check for enriched metadata
            # Citation count
            if 'citation_count' in fields:
                try:
                    paper.citation_count = int(fields['citation_count'])
                    paper.citation_count_source = fields.get('citation_count_source', 'bibtex')
                except ValueError:
                    pass
            
            # Impact factor (check various field names)
            for field_name in fields:
                if 'impact_factor' in field_name and 'JCR' in field_name:
                    try:
                        paper.impact_factor = float(fields[field_name])
                        paper.impact_factor_source = fields.get('impact_factor_source', 'bibtex')
                        break
                    except ValueError:
                        pass
            
            # Journal quartile
            for field_name in fields:
                if 'quartile' in field_name and 'JCR' in field_name:
                    paper.journal_quartile = fields[field_name]
                    paper.quartile_source = fields.get('quartile_source', 'bibtex')
                    break
            
            # Additional fields that might be present
            if 'volume' in fields:
                paper.volume = fields['volume']
            if 'pages' in fields:
                paper.pages = fields['pages']
            if 'url' in fields:
                paper.pdf_url = fields['url']
            
            return paper
            
        except Exception as e:
            logger.error(f"Error converting BibTeX entry to Paper: {e}")
            return None
    
    @property
    def papers(self) -> List[Paper]:
        """Get the list of papers."""
        return self._papers
    
    @property
    def summary(self) -> Dict[str, Any]:
        """
        Get basic summary statistics as a dictionary.
        
        Returns:
            Dictionary with basic statistics (fast, suitable for properties)
            
        Examples:
            >>> papers_obj.summary
            {'total': 20, 'sources': {'pubmed': 20}, 'years': {'min': 2020, 'max': 2025}}
        """
        summary_dict = {
            'total': len(self._papers),
            'sources': {},
            'years': None,
            'has_citations': 0,
            'has_impact_factors': 0,
            'has_pdfs': 0
        }
        
        if not self._papers:
            return summary_dict
        
        # Count by source
        for p in self._papers:
            summary_dict['sources'][p.source] = summary_dict['sources'].get(p.source, 0) + 1
        
        # Year range
        years = [int(p.year) for p in self._papers if p.year and p.year.isdigit()]
        if years:
            summary_dict['years'] = {'min': min(years), 'max': max(years)}
        
        # Quick counts
        summary_dict['has_citations'] = sum(1 for p in self._papers if p.citation_count is not None)
        summary_dict['has_impact_factors'] = sum(1 for p in self._papers if p.impact_factor is not None)
        summary_dict['has_pdfs'] = sum(1 for p in self._papers if p.pdf_url or p.pdf_path)
        
        return summary_dict
    
    def __len__(self) -> int:
        """Number of papers in collection."""
        return len(self._papers)
    
    def __iter__(self) -> Iterator[Paper]:
        """Iterate over papers."""
        return iter(self._papers)
    
    def __getitem__(self, index: Union[int, slice]) -> Union[Paper, 'Papers']:
        """Get paper by index or slice."""
        if isinstance(index, slice):
            return Papers(self._papers[index], auto_deduplicate=False)
        return self._papers[index]
    
    def __dir__(self) -> List[str]:
        """Return list of attributes for tab completion."""
        # Include all public methods and properties
        return ['papers', 'summary', 'filter', 'save', 'sort_by', 'summarize', 'to_dataframe', 'from_bibtex']
    
    def __repr__(self) -> str:
        """String representation for REPL."""
        return f"<Papers with {len(self._papers)} papers>"
    
    def filter(self, 
               year_min: Optional[int] = None,
               year_max: Optional[int] = None,
               min_citations: Optional[int] = None,
               max_citations: Optional[int] = None,
               citation_count_min: Optional[int] = None,  # Alias for min_citations
               impact_factor_min: Optional[float] = None,
               open_access_only: bool = False,
               journals: Optional[List[str]] = None,
               authors: Optional[List[str]] = None,
               keywords: Optional[List[str]] = None,
               title_keywords: Optional[List[str]] = None,
               source: Optional[Union[str, List[str]]] = None,
               has_pdf: Optional[bool] = None) -> 'Papers':
        """
        Filter papers by various criteria.
        
        Returns new Papers with filtered results.
        """
        filtered = []
        
        # Handle citation_count_min as alias for min_citations
        if citation_count_min is not None:
            min_citations = citation_count_min
        
        for paper in self._papers:
            # Year filters
            if year_min and paper.year:
                try:
                    if int(paper.year) < year_min:
                        continue
                except ValueError:
                    continue
                    
            if year_max and paper.year:
                try:
                    if int(paper.year) > year_max:
                        continue
                except ValueError:
                    continue
            
            # Citation filters
            if min_citations and (not paper.citation_count or paper.citation_count < min_citations):
                continue
            if max_citations and paper.citation_count and paper.citation_count > max_citations:
                continue
            
            # Impact factor filter
            if impact_factor_min and (not paper.impact_factor or paper.impact_factor < impact_factor_min):
                continue
            
            # Open access filter
            if open_access_only and not paper.pdf_url:
                continue
            
            # PDF availability filter
            if has_pdf is not None:
                if has_pdf and not (paper.pdf_url or paper.pdf_path):
                    continue
                elif not has_pdf and (paper.pdf_url or paper.pdf_path):
                    continue
            
            # Journal filter
            if journals and paper.journal not in journals:
                continue
            
            # Author filter
            if authors:
                author_match = any(
                    any(author_name.lower() in paper_author.lower() 
                        for paper_author in paper.authors)
                    for author_name in authors
                )
                if not author_match:
                    continue
            
            # Keyword filter
            if keywords:
                # Check in title, abstract, and keywords
                text_to_search = (
                    f"{paper.title} {paper.abstract} {' '.join(paper.keywords)}"
                ).lower()
                
                keyword_match = any(
                    keyword.lower() in text_to_search
                    for keyword in keywords
                )
                if not keyword_match:
                    continue
            
            # Title keywords filter
            if title_keywords and paper.title:
                title_lower = paper.title.lower()
                title_match = any(
                    keyword.lower() in title_lower
                    for keyword in title_keywords
                )
                if not title_match:
                    continue
            
            # Source filter
            if source:
                sources = [source] if isinstance(source, str) else source
                if paper.source not in sources:
                    continue
            
            filtered.append(paper)
        
        logger.info(f"Filtered {len(self._papers)} papers to {len(filtered)} papers")
        return Papers(filtered, auto_deduplicate=False)
    
    def sort_by(self, *criteria, **kwargs) -> 'Papers':
        """
        Sort papers by multiple criteria.
        
        Args:
            *criteria: Either:
                - Single string: sort_by('impact_factor')
                - Multiple strings: sort_by('impact_factor', 'year')
                - Tuples of (criteria, reverse): sort_by(('impact_factor', True), ('year', False))
                - Mixed: sort_by('impact_factor', ('year', False))
            **kwargs:
                - reverse: Default reverse setting for all criteria (default True)
            
        Supported criteria:
            - 'citations' or 'citation_count': Number of citations
            - 'year': Publication year
            - 'impact_factor': Journal impact factor
            - 'title': Paper title (alphabetical)
            - 'journal': Journal name (alphabetical)
            - 'first_author': First author name (alphabetical)
            - 'relevance': Currently uses citation count
            
        Returns:
            New sorted Papers
            
        Examples:
            # Sort by impact factor (descending)
            papers.sort_by('impact_factor')
            
            # Sort by impact factor (desc), then year (desc)
            papers.sort_by('impact_factor', 'year')
            
            # Sort by impact factor (desc), then year (asc)
            papers.sort_by(('impact_factor', True), ('year', False))
            
            # Mixed format
            papers.sort_by('impact_factor', ('year', False))
        """
        default_reverse = kwargs.get('reverse', True)
        
        # Normalize criteria to list of (criterion, reverse) tuples
        normalized_criteria = []
        for criterion in criteria:
            if isinstance(criterion, tuple) and len(criterion) == 2:
                normalized_criteria.append(criterion)
            elif isinstance(criterion, str):
                normalized_criteria.append((criterion, default_reverse))
            else:
                from ..errors import DataError
                raise DataError(
                    f"Invalid sort criterion: {criterion}",
                    context={"criterion": criterion, "valid_criteria": list(criteria.keys())},
                    suggestion="Use one of: relevance, year, citations, impact_factor"
                )
        
        # If no criteria specified, default to citations
        if not normalized_criteria:
            normalized_criteria = [('citations', default_reverse)]
        
        def get_sort_value(paper, criterion):
            """Get the sort value for a paper based on criterion."""
            if criterion in ('citations', 'citation_count'):
                return paper.citation_count or 0
            elif criterion == 'year':
                try:
                    return int(paper.year) if paper.year else 0
                except ValueError:
                    return 0
            elif criterion == 'impact_factor':
                return paper.impact_factor or 0
            elif criterion == 'title':
                return paper.title.lower()
            elif criterion == 'journal':
                return paper.journal.lower() if paper.journal else ''
            elif criterion == 'first_author':
                return paper.authors[0].lower() if paper.authors else ''
            elif criterion == 'relevance':
                # Use citation count as proxy for relevance
                return paper.citation_count or 0
            else:
                logger.warning(f"Unknown sort criteria: {criterion}. Using 0.")
                return 0
        
        # Create sort key function that handles multiple criteria
        def sort_key(paper):
            values = []
            for criterion, reverse in normalized_criteria:
                value = get_sort_value(paper, criterion)
                # For reverse sorting, negate numeric values
                # For strings, we'll handle reverse in the sorted() call
                if reverse and isinstance(value, (int, float)):
                    value = -value
                values.append(value)
            return tuple(values)
        
        # Sort papers
        # For string criteria with reverse=True, we need special handling
        # This is complex with multiple criteria, so we'll use a different approach
        # We'll build the sort key differently
        
        # Actually, let's use a cleaner approach with functools
        from functools import cmp_to_key
        
        def compare_papers(paper1, paper2):
            """Compare two papers based on multiple criteria."""
            for criterion, reverse in normalized_criteria:
                val1 = get_sort_value(paper1, criterion)
                val2 = get_sort_value(paper2, criterion)
                
                # Compare values
                if val1 < val2:
                    result = -1
                elif val1 > val2:
                    result = 1
                else:
                    result = 0
                
                # Apply reverse if needed
                if reverse:
                    result = -result
                
                # If not equal, return the result
                if result != 0:
                    return result
            
            # All criteria are equal
            return 0
        
        sorted_papers = sorted(self._papers, key=cmp_to_key(compare_papers))
        return Papers(sorted_papers, auto_deduplicate=False)
    
    def _calculate_completeness_score(self, paper: Paper, source_priority: List[str] = None) -> int:
        """
        Calculate a completeness score for a paper based on available data.
        Higher score = more complete data.
        
        Args:
            paper: The paper to score
            source_priority: List of sources in priority order (first = highest priority)
        """
        score = 0
        
        # Basic fields (1 point each)
        if paper.title: score += 1
        if paper.authors and len(paper.authors) > 0: score += 1
        if paper.abstract and len(paper.abstract) > 50: score += 2  # Abstract is valuable
        if paper.year: score += 1
        if paper.journal: score += 1
        
        # Identifiers (2 points each - very valuable for lookups)
        if paper.doi: score += 2
        if paper.pmid: score += 2
        if paper.arxiv_id: score += 2
        
        # Enriched data (1 point each)
        if paper.citation_count is not None: score += 1
        if paper.impact_factor is not None: score += 1
        if paper.keywords and len(paper.keywords) > 0: score += 1
        if paper.pdf_url: score += 1
        
        # Source priority bonus (higher bonus for sources listed first)
        if source_priority and paper.source in source_priority:
            # Give 10 points for first source, 9 for second, etc.
            priority_index = source_priority.index(paper.source)
            score += (10 - priority_index)
        
        return score
    
    def _merge_papers(self, paper1: Paper, paper2: Paper, source_priority: List[str] = None) -> Paper:
        """
        Merge two duplicate papers, keeping the best data from each.
        
        Args:
            paper1: First paper
            paper2: Second paper  
            source_priority: List of sources in priority order (first = highest priority)
        """
        # Determine which paper should be the base (higher completeness score)
        score1 = self._calculate_completeness_score(paper1, source_priority)
        score2 = self._calculate_completeness_score(paper2, source_priority)
        
        if score1 >= score2:
            base_paper, other_paper = paper1, paper2
        else:
            base_paper, other_paper = paper2, paper1
        
        # Merge all sources
        all_sources = list(set(getattr(base_paper, 'all_sources', [base_paper.source]) + 
                              getattr(other_paper, 'all_sources', [other_paper.source])))
        
        # Create merged paper starting from base
        merged = Paper(
            title=base_paper.title or other_paper.title,
            authors=base_paper.authors if base_paper.authors else other_paper.authors,
            abstract=base_paper.abstract if len(base_paper.abstract or '') >= len(other_paper.abstract or '') else other_paper.abstract,
            source=base_paper.source,  # Keep the base paper's source
            year=base_paper.year or other_paper.year,
            doi=base_paper.doi or other_paper.doi,
            pmid=base_paper.pmid or other_paper.pmid,
            arxiv_id=base_paper.arxiv_id or other_paper.arxiv_id,
            journal=base_paper.journal or other_paper.journal,
            keywords=list(set((base_paper.keywords or []) + (other_paper.keywords or []))),
            citation_count=max(base_paper.citation_count or 0, other_paper.citation_count or 0) if (base_paper.citation_count or other_paper.citation_count) else None,
            pdf_url=base_paper.pdf_url or other_paper.pdf_url,
            pdf_path=base_paper.pdf_path or other_paper.pdf_path,
            impact_factor=base_paper.impact_factor or other_paper.impact_factor,
            journal_quartile=base_paper.journal_quartile or other_paper.journal_quartile,
            journal_rank=base_paper.journal_rank or other_paper.journal_rank,
            h_index=base_paper.h_index or other_paper.h_index,
            metadata={**other_paper.metadata, **base_paper.metadata}  # Base paper metadata takes precedence
        )
        
        # Set all sources
        merged.all_sources = all_sources
        merged.metadata['all_sources'] = all_sources
        
        # Keep citation source from the paper that had the citation
        if base_paper.citation_count is not None:
            merged.citation_count_source = base_paper.citation_count_source
        elif other_paper.citation_count is not None:
            merged.citation_count_source = other_paper.citation_count_source
            
        # Keep impact factor source from the paper that had it
        if base_paper.impact_factor is not None:
            merged.impact_factor_source = base_paper.impact_factor_source
        elif other_paper.impact_factor is not None:
            merged.impact_factor_source = other_paper.impact_factor_source
            
        # Keep quartile source from the paper that had it
        if base_paper.journal_quartile is not None:
            merged.quartile_source = base_paper.quartile_source
        elif other_paper.journal_quartile is not None:
            merged.quartile_source = other_paper.quartile_source
        
        return merged
    
    def _deduplicate_in_place(self, threshold: float = 0.85, source_priority: List[str] = None) -> None:
        """
        Remove duplicate papers in-place based on similarity threshold.
        Intelligently merges data from duplicates.
        
        Args:
            threshold: Similarity threshold (0-1) above which papers are considered duplicates
            source_priority: List of sources in priority order (first = highest priority)
        """
        if not self._papers:
            return
        
        unique_papers = [self._papers[0]]
        
        for paper in self._papers[1:]:
            is_duplicate = False
            
            for i, unique_paper in enumerate(unique_papers):
                if paper.similarity_score(unique_paper) > threshold:
                    is_duplicate = True
                    # Merge the papers instead of just keeping one
                    merged_paper = self._merge_papers(unique_paper, paper, source_priority)
                    unique_papers[i] = merged_paper
                    logger.debug(f"Merged duplicate papers from sources: {merged_paper.all_sources}")
                    break
            
            if not is_duplicate:
                unique_papers.append(paper)
        
        if len(unique_papers) < len(self._papers):
            logger.info(f"Deduplicated {len(self._papers)} papers to {len(unique_papers)} unique papers")
            self._papers = unique_papers
    
    
    def to_dataframe(self) -> pd.DataFrame:
        """
        Convert collection to pandas DataFrame for analysis.
        
        Returns:
            DataFrame with paper metadata
        """
        if self._df_cache is not None:
            return self._df_cache
        
        # Import JCR year dynamically to include in column names
        from ._MetadataEnricher import JCR_YEAR
        
        data = []
        for paper in self._papers:
            row = {
                'title': paper.title,
                'first_author': paper.authors[0] if paper.authors else 'N/A',
                'num_authors': len(paper.authors),
                'year': int(paper.year) if paper.year and paper.year.isdigit() else None,
                'journal': paper.journal or 'N/A',
                'citation_count': paper.citation_count if paper.citation_count is not None else 'N/A',
                'citation_count_source': paper.citation_count_source or 'N/A',
                f'JCR_{JCR_YEAR}_impact_factor': paper.impact_factor if paper.impact_factor is not None else 'N/A',
                'impact_factor_source': paper.impact_factor_source or 'N/A',
                f'JCR_{JCR_YEAR}_quartile': paper.journal_quartile or 'N/A',
                'quartile_source': paper.quartile_source or 'N/A',
                'doi': paper.doi or 'N/A',
                'pmid': paper.pmid or 'N/A',
                'arxiv_id': paper.arxiv_id or 'N/A',
                'source': paper.source,
                'has_pdf': bool(paper.pdf_url or paper.pdf_path),
                'num_keywords': len(paper.keywords),
                'abstract_word_count': len(paper.abstract.split()) if paper.abstract else 0,
                'abstract': paper.abstract or 'N/A'
            }
            data.append(row)
        
        self._df_cache = pd.DataFrame(data)
        return self._df_cache
    
    def save(self, 
             output_path: Union[str, Path], 
             format: Optional[str] = None,
             include_enriched: bool = True) -> None:
        """
        Save collection to file. Format is auto-detected from extension if not specified.
        
        Simple save method like numpy.save() - just writes the file without extra features.
        For symlinks, verbose output, etc., use scitex.io.save() instead.
        
        Args:
            output_path: Output file path
            format: Output format ('bibtex', 'json', 'csv'). Auto-detected from extension if None.
            include_enriched: Include enriched metadata (for bibtex format)
            
        Examples:
            >>> # Save as BibTeX (auto-detected from extension)
            >>> papers_obj.save("/path/to/references.bib")
            
            >>> # Save as JSON
            >>> papers_obj.save("/path/to/papers.json")
            
            >>> # Save as CSV for data analysis
            >>> papers_obj.save("/path/to/papers.csv")
            
            >>> # Save BibTeX without enriched metadata
            >>> papers_obj.save("refs.bib", include_enriched=False)
            
            >>> # Explicitly specify format
            >>> papers_obj.save("myfile.txt", format="bibtex")
        """
        output_path = Path(output_path)
        
        # Auto-detect format from extension if not specified
        if format is None:
            ext = output_path.suffix.lower()
            if ext in ['.bib', '.bibtex']:
                format = 'bibtex'
            elif ext == '.json':
                format = 'json'
            elif ext == '.csv':
                format = 'csv'
            else:
                # Default to bibtex
                format = 'bibtex'
        
        # Ensure parent directory exists
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        if format.lower() == "bibtex":
            # Write BibTeX content directly
            bibtex_content = self._to_bibtex(include_enriched=include_enriched)
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(f"% BibTeX bibliography\n")
                f.write(f"% Generated by SciTeX Scholar on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"% Number of entries: {len(self._papers)}\n\n")
                f.write(bibtex_content)
        
        elif format.lower() == "json":
            # Write JSON directly
            import json
            data = {
                'metadata': {
                    'created': datetime.now().isoformat(),
                    'num_papers': len(self._papers),
                    'enriched': self._enriched
                },
                'papers': [p.to_dict() for p in self._papers]
            }
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=2, ensure_ascii=False)
        
        elif format.lower() == "csv":
            # Write CSV directly
            df = self.to_dataframe()
            df.to_csv(output_path, index=False)
        
        else:
            from ..errors import FileFormatError
            raise FileFormatError(
                filepath=str(filepath),
                expected_format="One of: bib, ris, json, csv, md, xlsx",
                actual_format=format
            )
    
    def download_pdfs(self, 
                     scholar=None, 
                     download_dir: Optional[Union[str, Path]] = None,
                     force: bool = False,
                     max_workers: int = 4,
                     show_progress: bool = True,
                     acknowledge_ethical_usage: Optional[bool] = None,
                     **kwargs) -> Dict[str, Any]:
        """
        Download PDFs for papers in this collection.
        
        Args:
            scholar: Scholar instance to use for downloading. If None, creates a new instance.
            download_dir: Directory to save PDFs (default: uses scholar's workspace)
            force: Force re-download even if files exist
            max_workers: Maximum concurrent downloads
            show_progress: Show download progress
            acknowledge_ethical_usage: Acknowledge ethical usage terms for Sci-Hub (default: from config)
            **kwargs: Additional arguments passed to downloader
            
        Returns:
            Dictionary with download results:
                - 'successful': Number of successful downloads
                - 'failed': Number of failed downloads
                - 'results': List of detailed results
                - 'downloaded_files': Dict mapping DOIs to file paths
            
        Examples:
            >>> papers = scholar.search("deep learning")
            >>> # Using existing scholar instance
            >>> results = papers.download_pdfs(scholar)
            >>> print(f"Downloaded {results['successful']} PDFs")
            
            >>> # Or create new scholar instance automatically
            >>> results = papers.download_pdfs(download_dir="./my_pdfs")
        """
        if scholar is None:
            from ._Scholar import Scholar
            scholar = Scholar()
        
        return scholar.download_pdfs(
            self, 
            download_dir=download_dir,
            force=force,
            max_workers=max_workers,
            show_progress=show_progress,
            acknowledge_ethical_usage=acknowledge_ethical_usage,
            **kwargs
        )
    
    def _to_bibtex_entries(self, include_enriched: bool) -> List[Dict[str, Any]]:
        """Convert collection to BibTeX entries format for scitex.io."""
        entries = []
        used_keys = set()
        
        for paper in self._papers:
            # Ensure unique keys
            paper._generate_bibtex_key()
            original_key = paper._bibtex_key
            
            counter = 1
            while paper._bibtex_key in used_keys:
                paper._bibtex_key = f"{original_key}{chr(ord('a') + counter - 1)}"
                counter += 1
            
            used_keys.add(paper._bibtex_key)
            
            # Create entry in scitex.io format
            entry = {
                'entry_type': self._determine_entry_type(paper),
                'key': paper._bibtex_key,
                'fields': self._paper_to_bibtex_fields(paper, include_enriched)
            }
            entries.append(entry)
        
        return entries
    
    def _determine_entry_type(self, paper: Paper) -> str:
        """Determine BibTeX entry type for a paper."""
        # Use original entry type if available
        if hasattr(paper, '_bibtex_entry_type') and paper._bibtex_entry_type:
            return paper._bibtex_entry_type
        
        # Otherwise determine based on paper properties
        if paper.arxiv_id:
            return 'misc'
        elif paper.journal:
            return 'article'
        else:
            return 'misc'
    
    def _paper_to_bibtex_fields(self, paper: Paper, include_enriched: bool) -> Dict[str, str]:
        """Convert paper to BibTeX fields dict."""
        fields = {}
        
        # If paper has original BibTeX fields, start with those
        if hasattr(paper, '_original_bibtex_fields'):
            fields = paper._original_bibtex_fields.copy()
        
        # Required fields (always override to ensure accuracy)
        fields['title'] = paper.title
        fields['author'] = ' and '.join(paper.authors) if paper.authors else 'Unknown'
        
        # Optional fields (only override if we have better data)
        if paper.year:
            fields['year'] = str(paper.year)
        
        if paper.journal:
            fields['journal'] = paper.journal
        
        if paper.doi:
            fields['doi'] = paper.doi
        
        if paper.arxiv_id:
            fields['eprint'] = paper.arxiv_id
            fields['archivePrefix'] = 'arXiv'
        
        if paper.abstract:
            fields['abstract'] = paper.abstract
        
        if paper.keywords:
            fields['keywords'] = ', '.join(paper.keywords)
        
        if paper.pdf_url:
            fields['url'] = paper.pdf_url
        
        # Volume and pages (from original or paper object)
        if hasattr(paper, 'volume') and paper.volume:
            fields['volume'] = str(paper.volume)
        if hasattr(paper, 'pages') and paper.pages:
            fields['pages'] = str(paper.pages)
        
        # Enriched metadata
        if include_enriched:
            # Get JCR year dynamically from enrichment module
            from ._MetadataEnricher import JCR_YEAR
            
            if paper.impact_factor is not None and paper.impact_factor > 0:
                fields[f'JCR_{JCR_YEAR}_impact_factor'] = str(paper.impact_factor)
                if paper.impact_factor_source:
                    fields['impact_factor_source'] = paper.impact_factor_source
            
            if paper.journal_quartile and paper.journal_quartile != 'Unknown':
                fields[f'JCR_{JCR_YEAR}_quartile'] = paper.journal_quartile
                if hasattr(paper, 'quartile_source') and paper.quartile_source:
                    fields['quartile_source'] = paper.quartile_source
            
            if paper.citation_count is not None:
                fields['citation_count'] = str(paper.citation_count)
                if paper.citation_count_source:
                    fields['citation_count_source'] = paper.citation_count_source
            
            # Add enrichment note
            enriched_info = []
            if paper.impact_factor is not None and paper.impact_factor > 0:
                enriched_info.append(f"IF={paper.impact_factor}")
            if paper.citation_count is not None:
                enriched_info.append(f"Citations={paper.citation_count}")
            
            if enriched_info:
                enrichment_note = f"[SciTeX Enhanced: {', '.join(enriched_info)}]"
                if 'note' in fields:
                    fields['note'] = f"{fields['note']}; {enrichment_note}"
                else:
                    fields['note'] = enrichment_note
        
        return fields
    
    def _to_json(self) -> str:
        """Convert collection to JSON format."""
        data = {
            'metadata': {
                'generated': datetime.now().isoformat(),
                'total_papers': len(self._papers),
                'enriched': self._enriched
            },
            'papers': [paper.to_dict() for paper in self._papers]
        }
        return json.dumps(data, indent=2, ensure_ascii=False)
    
    def summarize(self) -> None:
        """
        Print a summary of the paper collection.
        
        Displays key statistics about the collection including paper counts,
        year distribution, enrichment status, sources, and example papers.
        
        Returns:
            None (prints to stdout)
            
        Examples:
            >>> papers_obj.summarize()
            Paper Collection Summary
            ==================================================
            Total papers: 20
            Year range: 2020 - 2025
            ...
        """
        lines = [
            "Paper Collection Summary",
            "=" * 50,
            f"Total papers: {len(self._papers)}"
        ]
        
        if not self._papers:
            lines.append("(Empty collection)")
            print("\n".join(lines))
            return
        
        # Get year statistics
        years = [int(p.year) for p in self._papers if p.year and p.year.isdigit()]
        if years:
            year_counts = {}
            for year in years:
                year_counts[year] = year_counts.get(year, 0) + 1
            
            lines.append(f"Year range: {min(years)} - {max(years)}")
            # Show year distribution if varied
            if len(year_counts) > 1 and len(year_counts) <= 10:
                lines.append("\nYear distribution:")
                for year in sorted(year_counts.keys(), reverse=True)[:5]:
                    lines.append(f"  {year}: {year_counts[year]} papers")
                if len(year_counts) > 5:
                    lines.append(f"  ... and {len(year_counts) - 5} more years")
        
        # Enrichment statistics
        with_citations = sum(1 for p in self._papers if p.citation_count is not None)
        with_impact_factor = sum(1 for p in self._papers if p.impact_factor is not None)
        with_doi = sum(1 for p in self._papers if p.doi)
        with_pdf = sum(1 for p in self._papers if p.pdf_url or p.pdf_path)
        
        lines.append("\nEnrichment status:")
        if with_citations > 0:
            pct = (with_citations / len(self._papers)) * 100
            lines.append(f"  Citation data: {with_citations}/{len(self._papers)} ({pct:.0f}%)")
        if with_impact_factor > 0:
            pct = (with_impact_factor / len(self._papers)) * 100
            lines.append(f"  Impact factors: {with_impact_factor}/{len(self._papers)} ({pct:.0f}%)")
        if with_doi > 0:
            pct = (with_doi / len(self._papers)) * 100
            lines.append(f"  DOIs: {with_doi}/{len(self._papers)} ({pct:.0f}%)")
        if with_pdf > 0:
            pct = (with_pdf / len(self._papers)) * 100
            lines.append(f"  PDFs available: {with_pdf}/{len(self._papers)} ({pct:.0f}%)")
        
        # Source distribution
        sources = {}
        for p in self._papers:
            sources[p.source] = sources.get(p.source, 0) + 1
        
        if sources:
            lines.append("\nSources:")
            for source, count in sorted(sources.items(), key=lambda x: x[1], reverse=True):
                lines.append(f"  {source}: {count} papers")
        
        # Top journals if available
        journals = {}
        for p in self._papers:
            if p.journal:
                journals[p.journal] = journals.get(p.journal, 0) + 1
        
        if journals and len(journals) > 1:
            lines.append("\nTop journals:")
            for journal, count in sorted(journals.items(), key=lambda x: x[1], reverse=True)[:5]:
                if len(journal) > 50:
                    journal = journal[:47] + "..."
                lines.append(f"  {journal}: {count}")
            if len(journals) > 5:
                lines.append(f"  ... and {len(journals) - 5} more journals")
        
        # Show a few example papers
        if len(self._papers) > 0:
            lines.append("\nExample papers:")
            for i, paper in enumerate(self._papers[:3]):
                title = paper.title if len(paper.title) <= 60 else paper.title[:57] + "..."
                lines.append(f"  {i+1}. {title}")
                if paper.authors:
                    first_author = paper.authors[0] if len(paper.authors[0]) <= 20 else paper.authors[0][:17] + "..."
                    author_info = f"{first_author}"
                    if len(paper.authors) > 1:
                        author_info += f" et al. ({len(paper.authors)} authors)"
                    lines.append(f"     {author_info}, {paper.year}")
            if len(self._papers) > 3:
                lines.append(f"  ... and {len(self._papers) - 3} more papers")
        
        print("\n".join(lines))
    
    def _to_bibtex(self, include_enriched: bool = True) -> str:
        """
        Convert entire collection to BibTeX string.
        
        Args:
            include_enriched: Include enriched metadata (impact factor, etc.)
            
        Returns:
            BibTeX formatted string for all papers
        """
        bibtex_entries = []
        used_keys = set()
        
        for paper in self._papers:
            # Ensure unique keys
            paper._generate_bibtex_key()
            original_key = paper._bibtex_key
            
            counter = 1
            while paper._bibtex_key in used_keys:
                paper._bibtex_key = f"{original_key}{chr(ord('a') + counter - 1)}"
                counter += 1
            
            used_keys.add(paper._bibtex_key)
            bibtex_entries.append(paper._to_bibtex(include_enriched))
        
        return "\n\n".join(bibtex_entries)
    


# PaperEnricher functionality has been moved to enrichment.py


# Export all classes and functions
__all__ = ['Papers']

================================================================================
File: /home/ywatanabe/proj/SciTeX-Code/src/scitex/scholar/_PDFDownloader.py
Relative: ../../../../SciTeX-Code/src/scitex/scholar/_PDFDownloader.py
Size: 39857 bytes, Lines: 1088, Words: ~6642
================================================================================
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Timestamp: "2025-07-24 08:50:00 (ywatanabe)"
# File: /home/ywatanabe/proj/scitex_repo/src/scitex/scholar/_PDFDownloader.py
# ----------------------------------------
import os
__FILE__ = (
    "./src/scitex/scholar/_PDFDownloader.py"
)
__DIR__ = os.path.dirname(__FILE__)
# ----------------------------------------

"""
PDF downloader for SciTeX Scholar.

This module provides comprehensive PDF download functionality:
1. Direct publisher patterns (fastest)
2. Zotero translator support (most reliable)
3. Sci-Hub fallback (for paywalled content)
4. Web scraping with Playwright (last resort)
"""

import asyncio
import hashlib
import json
import logging
import re
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Set, Tuple, Union
from urllib.parse import quote, urljoin, urlparse

import aiohttp
import requests
from playwright.async_api import async_playwright

from ..errors import PDFDownloadError, ScholarError, warn_performance
from ._ethical_usage import check_ethical_usage, ETHICAL_USAGE_MESSAGE
from ._utils import normalize_filename
from ._ZoteroTranslatorRunner import ZoteroTranslatorRunner

logger = logging.getLogger(__name__)


class PDFDownloader:
    """
    PDF downloader with multiple strategies.
    
    Download priority:
    1. Check local cache
    2. Try direct publisher patterns
    3. Use Zotero translators if available
    4. Try Sci-Hub (with ethical acknowledgment)
    5. Use Playwright for JavaScript sites
    
    Features:
    - Concurrent downloads with progress tracking
    - Smart caching and deduplication
    - Automatic retry with exponential backoff
    - Publisher-specific optimizations
    - Ethical usage acknowledgment for Sci-Hub
    """
    
    # Sci-Hub mirrors (updated regularly)
    SCIHUB_MIRRORS = [
        "https://sci-hub.se",
        "https://sci-hub.st",
        "https://sci-hub.ru",
        "https://sci-hub.ren",
        "https://sci-hub.tw",
        "https://sci-hub.ee",
    ]
    
    def __init__(
        self,
        download_dir: Optional[Path] = None,
        use_translators: bool = True,
        use_scihub: bool = True,
        use_playwright: bool = True,
        use_openathens: bool = False,
        openathens_config: Optional[Dict[str, Any]] = None,
        timeout: int = 30,
        max_retries: int = 3,
        max_concurrent: int = 3,
        acknowledge_ethical_usage: Optional[bool] = None,
    ):
        """
        Initialize PDF downloader.
        
        Args:
            download_dir: Default download directory
            use_translators: Enable Zotero translator support
            use_scihub: Enable Sci-Hub fallback
            use_playwright: Enable Playwright for JS sites
            use_openathens: Enable OpenAthens authentication
            openathens_config: OpenAthens configuration dict
            timeout: Download timeout in seconds
            max_retries: Maximum retry attempts
            max_concurrent: Maximum concurrent downloads
            acknowledge_ethical_usage: Acknowledge ethical usage for Sci-Hub
        """
        self.download_dir = Path(download_dir or "./pdfs")
        self.use_translators = use_translators
        self.use_scihub = use_scihub
        self.use_playwright = use_playwright
        self.use_openathens = use_openathens
        self.timeout = timeout
        self.max_retries = max_retries
        self.max_concurrent = max_concurrent
        
        # Initialize components
        if use_translators:
            try:
                self.zotero_translator_runner = ZoteroTranslatorRunner()
            except Exception as e:
                logger.warning(f"Failed to initialize Zotero translator runner: {e}")
                self.zotero_translator_runner = None
                self.use_translators = False
        else:
            self.zotero_translator_runner = None
            
        # Initialize OpenAthens authenticator
        # TODO: Future authentication methods (EZProxy, Lean Library, Shibboleth)
        # will be added here following the same pattern
        self.openathens_authenticator = None
        if use_openathens and openathens_config:
            try:
                from ._OpenAthensAuthenticator import OpenAthensAuthenticator
                self.openathens_authenticator = OpenAthensAuthenticator(
                    email=openathens_config.get('email'),
                    timeout=timeout,
                )
            except Exception as e:
                logger.warning(f"Failed to initialize OpenAthens: {e}")
                self.use_openathens = False
            
        # Track downloads to avoid duplicates
        self._active_downloads: Set[str] = set()
        self._download_cache: Dict[str, Path] = {}
        
        # Ethical usage for Sci-Hub
        self._ethical_acknowledged = acknowledge_ethical_usage
        
    async def download_pdf(
        self,
        identifier: str,
        output_dir: Optional[Path] = None,
        filename: Optional[str] = None,
        force: bool = False,
        metadata: Optional[Dict[str, Any]] = None,
    ) -> Optional[Path]:
        """
        Download PDF using best available method.
        
        Args:
            identifier: DOI, URL, or other identifier
            output_dir: Output directory (uses default if None)
            filename: Custom filename (auto-generated if None)
            force: Force re-download even if exists
            metadata: Additional metadata for filename generation
            
        Returns:
            Path to downloaded PDF or None
        """
        # Normalize identifier
        identifier = identifier.strip()
        
        # Check if already downloading
        if identifier in self._active_downloads:
            logger.info(f"Already downloading: {identifier}")
            # Wait for existing download
            while identifier in self._active_downloads:
                await asyncio.sleep(0.5)
            return self._download_cache.get(identifier)
            
        # Mark as active
        self._active_downloads.add(identifier)
        
        try:
            # Determine output path
            output_dir = output_dir or self.download_dir
            output_dir.mkdir(parents=True, exist_ok=True)
            
            if not filename:
                filename = self._generate_filename(identifier, metadata)
            output_path = output_dir / filename
            
            # Check cache
            if not force and output_path.exists() and output_path.stat().st_size > 1000:
                logger.info(f"PDF already exists: {output_path}")
                self._download_cache[identifier] = output_path
                return output_path
                
            # Try download strategies in order
            pdf_path = None
            
            # Strategy 1: Direct download if URL
            if identifier.startswith('http'):
                pdf_path = await self._try_direct_url_download(
                    identifier, output_path
                )
                
            # Strategy 2: DOI resolution and patterns
            elif self._is_doi(identifier):
                pdf_path = await self._download_from_doi(
                    identifier, output_path
                )
                
            # Strategy 3: Try as URL even without http
            else:
                # Maybe it's a partial URL
                for prefix in ['https://', 'http://']:
                    test_url = prefix + identifier
                    if await self._is_valid_url(test_url):
                        pdf_path = await self._try_direct_url_download(
                            test_url, output_path
                        )
                        if pdf_path:
                            break
                            
            # Cache result
            if pdf_path:
                self._download_cache[identifier] = pdf_path
                
            return pdf_path
            
        finally:
            # Remove from active downloads
            self._active_downloads.discard(identifier)
            
    async def _download_from_doi(
        self,
        doi: str,
        output_path: Path
    ) -> Optional[Path]:
        """Download PDF from DOI using multiple strategies."""
        # Resolve DOI to URL
        resolved_url = await self._resolve_doi(doi)
        if not resolved_url:
            logger.error(f"Failed to resolve DOI: {doi}")
            return None
            
        logger.info(f"Resolved {doi} to {resolved_url}")
        
        # Try strategies in order
        strategies = [
            ("Direct patterns", self._try_direct_patterns),
            ("OpenAthens", self._try_openathens),
            ("Zotero translators", self._try_zotero_translator),
            ("Sci-Hub", self._try_scihub),
            ("Playwright", self._try_playwright),
        ]
        
        for name, strategy in strategies:
            if not self._should_use_strategy(name):
                continue
                
            logger.info(f"Trying {name} for {doi}")
            
            try:
                pdf_path = await strategy(doi, resolved_url, output_path)
                if pdf_path:
                    logger.info(f"Success with {name}: {pdf_path}")
                    return pdf_path
            except Exception as e:
                logger.debug(f"{name} failed for {doi}: {e}")
                
        logger.error(f"All strategies failed for {doi}")
        return None
        
    def _should_use_strategy(self, strategy: str) -> bool:
        """Check if strategy should be used."""
        if strategy == "OpenAthens":
            return self.use_openathens and self.openathens_authenticator is not None
        elif strategy == "Zotero translators":
            return self.use_translators and self.zotero_translator_runner is not None
        elif strategy == "Sci-Hub":
            return self.use_scihub
        elif strategy == "Playwright":
            return self.use_playwright
        return True
        
    async def _try_openathens(
        self,
        doi: str,
        url: str,
        output_path: Path
    ) -> Optional[Path]:
        """Try download using OpenAthens authentication."""
        if not self.openathens_authenticator:
            return None
            
        try:
            # Ensure authenticated
            if not await self.openathens_authenticator.is_authenticated():
                logger.info("Authenticating with OpenAthens...")
                success = await self.openathens_authenticator.authenticate()
                if not success:
                    logger.warning("OpenAthens authentication failed")
                    return None
            
            # Try direct download with auth
            result = await self.openathens_authenticator.download_with_auth(
                url, output_path
            )
            
            if result:
                return result
                
            # Try with authenticated URL
            auth_url = await self.openathens_authenticator.get_authenticated_url(url)
            if await self._download_file(auth_url, output_path):
                return output_path
                
        except Exception as e:
            logger.debug(f"OpenAthens download failed: {e}")
            
        return None
        
    async def _try_direct_patterns(
        self,
        doi: str,
        url: str,
        output_path: Path
    ) -> Optional[Path]:
        """Try direct download using publisher patterns."""
        pdf_urls = self._get_publisher_pdf_urls(url, doi)
        
        for pdf_url in pdf_urls:
            logger.debug(f"Trying direct download: {pdf_url}")
            if await self._download_file(pdf_url, output_path, referer=url):
                return output_path
                
        return None
        
    async def _try_zotero_translator(
        self,
        doi: str,
        url: str,
        output_path: Path
    ) -> Optional[Path]:
        """Try download using Zotero translator."""
        if not self.zotero_translator_runner:
            return None
            
        # Find and run translator
        translator = self.zotero_translator_runner.find_translator_for_url(url)
        if not translator:
            return None
            
        # Extract PDF URLs using translator
        pdf_urls = await self.zotero_translator_runner.extract_pdf_urls(url)
        
        for pdf_url in pdf_urls:
            logger.info(f"Trying translator PDF: {pdf_url}")
            if await self._download_file(pdf_url, output_path, referer=url):
                return output_path
                
        return None
        
    async def _try_scihub(
        self,
        doi: str,
        url: str,
        output_path: Path
    ) -> Optional[Path]:
        """Try download using Sci-Hub."""
        # Check ethical acknowledgment
        if not self._ethical_acknowledged:
            self._ethical_acknowledged = check_ethical_usage(
                self._ethical_acknowledged
            )
            if not self._ethical_acknowledged:
                logger.info("Sci-Hub download skipped (ethical usage not acknowledged)")
                return None
                
        # Try each Sci-Hub mirror
        for mirror in self.SCIHUB_MIRRORS:
            try:
                # Sci-Hub accepts DOIs directly
                scihub_url = f"{mirror}/{doi}"
                
                # Get the PDF URL from Sci-Hub
                pdf_url = await self._get_scihub_pdf_url(scihub_url)
                if pdf_url:
                    logger.info(f"Found PDF on Sci-Hub: {mirror}")
                    if await self._download_file(pdf_url, output_path):
                        return output_path
                        
            except Exception as e:
                logger.debug(f"Sci-Hub mirror {mirror} failed: {e}")
                continue
                
        return None
        
    async def _get_scihub_pdf_url(self, scihub_url: str) -> Optional[str]:
        """Extract PDF URL from Sci-Hub page."""
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(
                    scihub_url,
                    timeout=aiohttp.ClientTimeout(total=self.timeout),
                    headers={'User-Agent': 'Mozilla/5.0'}
                ) as response:
                    if response.status != 200:
                        return None
                        
                    html = await response.text()
                    
                    # Look for PDF embed/iframe
                    patterns = [
                        r'<iframe.*?src=["\']([^"\']*\.pdf[^"\']*)["\']',
                        r'<embed.*?src=["\']([^"\']*\.pdf[^"\']*)["\']',
                        r'<iframe.*?src=["\']([^"\']*)["\'].*?pdf',
                        r'window\.location\.href\s*=\s*["\']([^"\']*\.pdf[^"\']*)["\']',
                    ]
                    
                    for pattern in patterns:
                        match = re.search(pattern, html, re.IGNORECASE)
                        if match:
                            pdf_url = match.group(1)
                            # Make absolute URL
                            if not pdf_url.startswith('http'):
                                if pdf_url.startswith('//'):
                                    pdf_url = 'https:' + pdf_url
                                else:
                                    pdf_url = urljoin(scihub_url, pdf_url)
                            return pdf_url
                            
        except Exception as e:
            logger.debug(f"Failed to get Sci-Hub PDF URL: {e}")
            
        return None
        
    async def _try_playwright(
        self,
        doi: str,
        url: str,
        output_path: Path
    ) -> Optional[Path]:
        """Try download using Playwright for JS-heavy sites."""
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            page = await browser.new_page()
            
            try:
                # Navigate and wait for content
                await page.goto(url, wait_until='networkidle')
                await page.wait_for_timeout(3000)
                
                # Look for PDF links
                pdf_urls = await page.evaluate('''
                    () => {
                        const urls = new Set();
                        
                        // Check all links
                        document.querySelectorAll('a').forEach(a => {
                            const href = a.href;
                            if (href && (href.includes('.pdf') || 
                                        href.includes('/pdf/') ||
                                        a.textContent.match(/PDF|Download/i))) {
                                urls.add(href);
                            }
                        });
                        
                        // Check iframes
                        document.querySelectorAll('iframe').forEach(iframe => {
                            if (iframe.src && iframe.src.includes('pdf')) {
                                urls.add(iframe.src);
                            }
                        });
                        
                        return Array.from(urls);
                    }
                ''')
                
                # Try downloading PDFs
                for pdf_url in pdf_urls:
                    if await self._download_file(pdf_url, output_path, referer=url):
                        return output_path
                        
            finally:
                await browser.close()
                
        return None
        
    def _get_publisher_pdf_urls(self, url: str, doi: str) -> List[str]:
        """Generate PDF URLs based on publisher patterns."""
        domain = urlparse(url).netloc
        pdf_urls = []
        
        # Comprehensive publisher patterns
        patterns = {
            # Nature Publishing Group
            'nature.com': [
                lambda: url.replace('/articles/', '/articles/') + '.pdf',
                lambda: re.sub(r'(/articles/[^/]+).*', r'\1.pdf', url),
            ],
            
            # Science/AAAS
            'science.org': [
                lambda: url.replace('/doi/', '/doi/pdf/'),
                lambda: url.replace('/content/', '/content/') + '.full.pdf',
            ],
            
            # Cell Press/Elsevier
            'cell.com': [
                lambda: url.replace('/fulltext/', '/pdf/'),
                lambda: url.replace('/article/', '/action/showPdf?pii='),
            ],
            'sciencedirect.com': [
                lambda: self._sciencedirect_pdf_url(url),
            ],
            
            # Springer
            'springer.com': [
                lambda: url.replace('/article/', '/content/pdf/') + '.pdf',
                lambda: url.replace('/chapter/', '/content/pdf/') + '.pdf',
            ],
            'link.springer.com': [
                lambda: url.replace('/article/', '/content/pdf/') + '.pdf',
                lambda: url.replace('/chapter/', '/content/pdf/') + '.pdf',
            ],
            
            # Wiley
            'wiley.com': [
                lambda: url.replace('/abs/', '/pdf/'),
                lambda: url.replace('/full/', '/pdfdirect/'),
                lambda: url.replace('/doi/', '/doi/pdf/'),
                lambda: re.sub(r'/doi/([^/]+)/([^/]+)/(.+)', r'/doi/pdf/\1/\2/\3', url),
            ],
            
            # IEEE
            'ieee.org': [
                lambda: self._ieee_pdf_url(url),
                lambda: url.replace('/document/', '/stamp/stamp.jsp?tp=&arnumber='),
            ],
            
            # ACS Publications
            'acs.org': [
                lambda: url.replace('/doi/', '/doi/pdf/'),
                lambda: url.replace('/abs/', '/pdf/'),
            ],
            
            # RSC Publishing
            'rsc.org': [
                lambda: url.replace('/en/content/', '/en/content/articlepdf/'),
                lambda: url + '/pdf',
            ],
            
            # IOP Science
            'iop.org': [
                lambda: url.replace('/article/', '/article/') + '/pdf',
                lambda: url.replace('/meta', '/pdf'),
            ],
            
            # Taylor & Francis
            'tandfonline.com': [
                lambda: url.replace('/doi/full/', '/doi/pdf/'),
                lambda: url.replace('/doi/abs/', '/doi/pdf/'),
            ],
            
            # PLOS
            'plos.org': [
                lambda: self._plos_pdf_url(url),
                lambda: url.replace('/article?', '/article/file?') + '&type=printable',
            ],
            
            # PNAS
            'pnas.org': [
                lambda: url.replace('/content/', '/content/') + '.full.pdf',
                lambda: url.replace('/doi/', '/doi/pdf/'),
            ],
            
            # Oxford Academic
            'oup.com': [
                lambda: url.replace('/article/', '/article-pdf/'),
                lambda: url + '/pdf',
            ],
            'academic.oup.com': [
                lambda: url.replace('/article/', '/article-pdf/'),
            ],
            
            # BMJ
            'bmj.com': [
                lambda: url.replace('/content/', '/content/') + '.full.pdf',
                lambda: url + '.full.pdf',
            ],
            
            # Frontiers
            'frontiersin.org': [
                lambda: url.replace('/articles/', '/articles/') + '/pdf',
                lambda: url.replace('/full', '/pdf'),
            ],
            
            # MDPI
            'mdpi.com': [
                lambda: url.replace('/htm', '/pdf'),
                lambda: url + '/pdf',
            ],
            
            # arXiv
            'arxiv.org': [
                lambda: url.replace('/abs/', '/pdf/') + '.pdf',
                lambda: url.replace('arxiv.org', 'export.arxiv.org').replace('/abs/', '/pdf/') + '.pdf',
            ],
            
            # bioRxiv/medRxiv
            'biorxiv.org': [
                lambda: url + '.full.pdf',
                lambda: url.replace('/content/', '/content/') + '.full.pdf',
            ],
            'medrxiv.org': [
                lambda: url + '.full.pdf',
                lambda: url.replace('/content/', '/content/') + '.full.pdf',
            ],
            
            # SSRN
            'ssrn.com': [
                lambda: self._ssrn_pdf_url(url),
            ],
            
            # JSTOR
            'jstor.org': [
                lambda: url.replace('/stable/', '/stable/pdf/') + '.pdf',
            ],
            
            # Project MUSE
            'muse.jhu.edu': [
                lambda: url.replace('/article/', '/article/') + '/pdf',
            ],
            
            # APS Physics
            'aps.org': [
                lambda: url.replace('/abstract/', '/pdf/'),
            ],
            'physics.aps.org': [
                lambda: url.replace('/abstract/', '/pdf/'),
            ],
        }
        
        # Try all matching patterns
        for domain_pattern, url_generators in patterns.items():
            if domain_pattern in domain:
                for generator in url_generators:
                    try:
                        pdf_url = generator()
                        if pdf_url:
                            pdf_urls.append(pdf_url)
                    except:
                        continue
                        
        # Add generic DOI resolver
        pdf_urls.append(f"https://doi.org/{doi}?format=pdf")
        
        # Remove duplicates while preserving order
        seen = set()
        unique_urls = []
        for url in pdf_urls:
            if url not in seen:
                seen.add(url)
                unique_urls.append(url)
                
        return unique_urls
        
    def _sciencedirect_pdf_url(self, url: str) -> Optional[str]:
        """Generate ScienceDirect PDF URL."""
        # Extract PII (Publication Item Identifier)
        pii_match = re.search(r'/pii/([A-Z0-9]+)', url)
        if pii_match:
            pii = pii_match.group(1)
            return f"https://www.sciencedirect.com/science/article/pii/{pii}/pdfft"
            
        # Try article pattern
        article_match = re.search(r'/article/abs/pii/([A-Z0-9]+)', url)
        if article_match:
            pii = article_match.group(1)
            return f"https://www.sciencedirect.com/science/article/pii/{pii}/pdfft"
            
        return None
        
    def _ieee_pdf_url(self, url: str) -> Optional[str]:
        """Generate IEEE PDF URL."""
        # Extract document number
        doc_match = re.search(r'/document/(\d+)', url)
        if doc_match:
            doc_num = doc_match.group(1)
            return f"https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber={doc_num}"
            
        # Try arnumber pattern
        arn_match = re.search(r'arnumber=(\d+)', url)
        if arn_match:
            doc_num = arn_match.group(1)
            return f"https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber={doc_num}"
            
        return None
        
    def _plos_pdf_url(self, url: str) -> Optional[str]:
        """Generate PLOS PDF URL."""
        # Extract article ID
        id_match = re.search(r'id=([^&]+)', url)
        if id_match:
            article_id = id_match.group(1)
            return f"https://journals.plos.org/plosone/article/file?id={article_id}&type=printable"
            
        # Try DOI pattern
        doi_match = re.search(r'journal\.p[^.]+\.(\d+)', url)
        if doi_match:
            return url.replace('/article?', '/article/file?') + '&type=printable'
            
        return None
        
    def _ssrn_pdf_url(self, url: str) -> Optional[str]:
        """Generate SSRN PDF URL."""
        # Extract abstract ID
        abstract_match = re.search(r'abstract=(\d+)', url)
        if abstract_match:
            abstract_id = abstract_match.group(1)
            return f"https://papers.ssrn.com/sol3/Delivery.cfm/SSRN_ID{abstract_id}_code.pdf?abstractid={abstract_id}"
            
        return None
        
    async def _resolve_doi(self, doi: str) -> Optional[str]:
        """Resolve DOI to publisher URL."""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (compatible; SciTeX/1.0)',
                'Accept': 'text/html,application/xhtml+xml',
            }
            
            # Clean DOI
            doi = doi.strip()
            if not doi.startswith('10.'):
                doi = '10.' + doi.split('10.')[-1]
                
            async with aiohttp.ClientSession() as session:
                async with session.get(
                    f"https://doi.org/{doi}",
                    headers=headers,
                    allow_redirects=True,
                    timeout=aiohttp.ClientTimeout(total=10)
                ) as response:
                    return str(response.url)
                    
        except Exception as e:
            logger.error(f"DOI resolution failed for {doi}: {e}")
            return None
            
    async def _download_file(
        self,
        url: str,
        output_path: Path,
        referer: Optional[str] = None
    ) -> bool:
        """Download file with retry logic."""
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'application/pdf,*/*',
        }
        
        if referer:
            headers['Referer'] = referer
            
        for attempt in range(self.max_retries):
            try:
                async with aiohttp.ClientSession() as session:
                    async with session.get(
                        url,
                        headers=headers,
                        timeout=aiohttp.ClientTimeout(total=self.timeout),
                        allow_redirects=True
                    ) as response:
                        if response.status == 200:
                            content = await response.read()
                            
                            # Verify it's a PDF
                            if content.startswith(b'%PDF'):
                                output_path.parent.mkdir(parents=True, exist_ok=True)
                                output_path.write_bytes(content)
                                logger.info(f"Downloaded PDF to {output_path}")
                                return True
                            else:
                                logger.debug(f"Content is not PDF from {url}")
                                
                        elif response.status == 404:
                            logger.debug(f"404 Not Found: {url}")
                            return False
                        else:
                            logger.debug(f"HTTP {response.status} from {url}")
                            
            except asyncio.TimeoutError:
                logger.warning(f"Timeout on attempt {attempt + 1} for {url}")
            except Exception as e:
                logger.debug(f"Download error on attempt {attempt + 1}: {e}")
                
            # Exponential backoff
            if attempt < self.max_retries - 1:
                await asyncio.sleep(2 ** attempt)
                
        return False
        
    async def _try_direct_url_download(
        self,
        url: str,
        output_path: Path
    ) -> Optional[Path]:
        """Try downloading directly from URL."""
        # Check if it's already a PDF URL
        if '.pdf' in url.lower() or '/pdf/' in url:
            if await self._download_file(url, output_path):
                return output_path
                
        # Try appending .pdf
        if not url.endswith('.pdf'):
            pdf_url = url + '.pdf'
            if await self._download_file(pdf_url, output_path):
                return output_path
                
        return None
        
    def _is_doi(self, identifier: str) -> bool:
        """Check if identifier is a DOI."""
        # DOI regex pattern
        doi_pattern = r'^10\.\d{4,}/[-._;()/:\w]+$'
        return bool(re.match(doi_pattern, identifier))
        
    async def _is_valid_url(self, url: str) -> bool:
        """Check if URL is valid and accessible."""
        try:
            parsed = urlparse(url)
            return bool(parsed.scheme and parsed.netloc)
        except:
            return False
            
    def _generate_filename(
        self,
        identifier: str,
        metadata: Optional[Dict[str, Any]] = None
    ) -> str:
        """Generate descriptive filename."""
        if metadata:
            parts = []
            
            # Add first author
            if 'authors' in metadata and metadata['authors']:
                first_author = metadata['authors'][0].split(',')[0].strip()
                first_author = re.sub(r'[^\w\s-]', '', first_author)
                parts.append(first_author.replace(' ', '_'))
                
            # Add year
            if 'year' in metadata:
                parts.append(str(metadata['year']))
                
            # Add short title
            if 'title' in metadata:
                title_words = metadata['title'].split()[:5]
                short_title = '_'.join(title_words)
                short_title = re.sub(r'[^\w\s-]', '', short_title)
                parts.append(short_title)
                
            if parts:
                filename = '_'.join(parts) + '.pdf'
                # Limit length
                if len(filename) > 100:
                    filename = filename[:96] + '.pdf'
                return filename
                
        # Fallback: use identifier
        clean_id = re.sub(r'[^\w.-]', '_', identifier)
        return clean_id + '.pdf'
        
    async def batch_download(
        self,
        identifiers: List[str],
        output_dir: Optional[Path] = None,
        organize_by_year: bool = False,
        metadata_list: Optional[List[Dict[str, Any]]] = None,
        progress_callback: Optional[callable] = None,
    ) -> Dict[str, Optional[Path]]:
        """
        Download multiple PDFs concurrently.
        
        Args:
            identifiers: List of DOIs/URLs
            output_dir: Output directory
            organize_by_year: Create year subdirectories
            metadata_list: Metadata for each identifier
            progress_callback: Callback(completed, total, identifier)
            
        Returns:
            Dictionary mapping identifier to downloaded path
        """
        if len(identifiers) > 10:
            warn_performance(
                "PDF Download",
                f"Downloading {len(identifiers)} PDFs. This may take time."
            )
            
        output_dir = output_dir or self.download_dir
        results = {}
        completed = 0
        total = len(identifiers)
        
        # Prepare download tasks
        tasks = []
        for i, identifier in enumerate(identifiers):
            metadata = metadata_list[i] if metadata_list else None
            
            # Determine output directory
            if organize_by_year and metadata and 'year' in metadata:
                item_dir = output_dir / str(metadata['year'])
            else:
                item_dir = output_dir
                
            tasks.append({
                'identifier': identifier,
                'output_dir': item_dir,
                'metadata': metadata
            })
            
        # Download with concurrency limit
        semaphore = asyncio.Semaphore(self.max_concurrent)
        
        async def download_with_limit(task: Dict) -> Tuple[str, Optional[Path]]:
            nonlocal completed
            
            async with semaphore:
                path = await self.download_pdf(
                    identifier=task['identifier'],
                    output_dir=task['output_dir'],
                    metadata=task['metadata']
                )
                
                completed += 1
                if progress_callback:
                    progress_callback(completed, total, task['identifier'])
                    
                return task['identifier'], path
                
        # Execute downloads
        download_results = await asyncio.gather(
            *[download_with_limit(task) for task in tasks],
            return_exceptions=True
        )
        
        # Process results
        for result in download_results:
            if isinstance(result, Exception):
                logger.error(f"Download failed: {result}")
            else:
                identifier, path = result
                results[identifier] = path
                
        # Summary
        success_count = sum(1 for p in results.values() if p is not None)
        logger.info(f"Downloaded {success_count}/{total} PDFs successfully")
        
        return results


# Convenience functions

async def download_pdf(
    identifier: str,
    output_dir: Optional[Path] = None,
    use_scihub: bool = True,
    acknowledge_ethical_usage: Optional[bool] = None,
) -> Optional[Path]:
    """
    Simple function to download a single PDF.
    
    Args:
        identifier: DOI or URL
        output_dir: Output directory
        use_scihub: Enable Sci-Hub fallback
        acknowledge_ethical_usage: Acknowledge ethical usage
        
    Returns:
        Path to downloaded PDF or None
    """
    downloader = PDFDownloader(
        use_scihub=use_scihub,
        acknowledge_ethical_usage=acknowledge_ethical_usage
    )
    return await downloader.download_pdf(identifier, output_dir)


async def download_pdfs(
    identifiers: List[str],
    output_dir: Optional[Path] = None,
    max_concurrent: int = 3,
    use_scihub: bool = True,
    acknowledge_ethical_usage: Optional[bool] = None,
    progress_callback: Optional[callable] = None,
) -> Dict[str, Optional[Path]]:
    """
    Download multiple PDFs concurrently.
    
    Args:
        identifiers: List of DOIs/URLs
        output_dir: Output directory
        max_concurrent: Maximum concurrent downloads
        use_scihub: Enable Sci-Hub fallback
        acknowledge_ethical_usage: Acknowledge ethical usage
        progress_callback: Optional progress callback
        
    Returns:
        Dictionary mapping identifier to path
    """
    downloader = PDFDownloader(
        max_concurrent=max_concurrent,
        use_scihub=use_scihub,
        acknowledge_ethical_usage=acknowledge_ethical_usage
    )
    return await downloader.batch_download(
        identifiers,
        output_dir,
        progress_callback=progress_callback
    )


if __name__ == "__main__":
    # Example usage
    import sys
    
    async def test_unified_downloader():
        # Test identifiers
        test_cases = [
            # DOIs
            "10.1038/s41586-021-03819-2",  # Nature
            "10.1126/science.abg5298",      # Science
            "10.1016/j.cell.2021.07.015",   # Cell
            "10.1103/PhysRevLett.127.067401",  # APS
            "10.1021/acs.nanolett.1c02400",    # ACS
            
            # Direct URLs
            "https://arxiv.org/abs/2103.14030",
            "https://www.biorxiv.org/content/10.1101/2021.07.15.452479v1",
            
            # Paywalled (will try Sci-Hub)
            "10.1038/s41586-020-2649-2",
        ]
        
        if len(sys.argv) > 1:
            test_cases = [sys.argv[1]]
            
        output_dir = Path("./test_unified_pdfs")
        output_dir.mkdir(exist_ok=True)
        
        print("Testing PDF Downloader")
        print("=" * 50)
        
        # Test single download
        if len(test_cases) == 1:
            identifier = test_cases[0]
            print(f"\nDownloading: {identifier}")
            
            path = await download_pdf(
                identifier,
                output_dir,
                use_scihub=True,
                acknowledge_ethical_usage=True
            )
            
            if path:
                print(f"✓ Success: {path}")
                print(f"  Size: {path.stat().st_size / 1024:.1f} KB")
            else:
                print(f"✗ Failed to download {identifier}")
                
        else:
            # Test batch download
            print(f"\nBatch downloading {len(test_cases)} PDFs...")
            
            def progress(completed, total, identifier):
                print(f"Progress: {completed}/{total} - {identifier}")
                
            results = await download_pdfs(
                test_cases,
                output_dir,
                max_concurrent=2,
                use_scihub=True,
                acknowledge_ethical_usage=True,
                progress_callback=progress
            )
            
            print("\n\nResults:")
            print("-" * 50)
            
            for identifier, path in results.items():
                if path:
                    size_kb = path.stat().st_size / 1024
                    print(f"✓ {identifier}")
                    print(f"  → {path.name} ({size_kb:.1f} KB)")
                else:
                    print(f"✗ {identifier} - Failed")
                    
            # Summary
            success = sum(1 for p in results.values() if p)
            print(f"\nSuccess rate: {success}/{len(test_cases)} ({success/len(test_cases)*100:.0f}%)")
            
    asyncio.run(test_unified_downloader())

# EOF

================================================================================
File: /home/ywatanabe/proj/SciTeX-Code/src/scitex/scholar/_PDFParser.py
Relative: ../../../../SciTeX-Code/src/scitex/scholar/_PDFParser.py
Size: 8349 bytes, Lines: 224, Words: ~1391
================================================================================
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Timestamp: "2025-07-23 13:54:54 (ywatanabe)"
# File: /home/ywatanabe/proj/scitex_repo/src/scitex/scholar/_PDFParser.py
# ----------------------------------------
import os
__FILE__ = (
    "./src/scitex/scholar/_PDFParser.py"
)
__DIR__ = os.path.dirname(__FILE__)
# ----------------------------------------

"""
PDF text extraction functionality for downstream AI integration.

This module provides clean text extraction from scientific PDFs,
with section awareness and format handling.
"""

import logging
import re
from pathlib import Path
from typing import Dict, List, Tuple

logger = logging.getLogger(__name__)


class PDFParser:
    """Extract text and structure from scientific PDFs."""

    # Common section headers in scientific papers
    SECTION_PATTERNS = [
        r"^\s*abstract\s*$",
        r"^\s*introduction\s*$",
        r"^\s*background\s*$",
        r"^\s*methods?\s*$",
        r"^\s*materials?\s+and\s+methods?\s*$",
        r"^\s*results?\s*$",
        r"^\s*discussion\s*$",
        r"^\s*conclusions?\s*$",
        r"^\s*references?\s*$",
        r"^\s*acknowledgments?\s*$",
        r"^\s*supplementary\s*",
        r"^\s*\d+\.?\s+\w+",  # Numbered sections like "1. Introduction"
    ]

    def __init__(self):
        self._fitz_available = self._check_fitz()
        if not self._fitz_available:
            logger.warning(
                "PyMuPDF (fitz) not installed. PDF text extraction will be limited. "
                "Install with: pip install PyMuPDF"
            )

    def _check_fitz(self) -> bool:
        """Check if PyMuPDF is available."""
        try:
            import fitz

            return True
        except ImportError:
            return False

    def _extract_text(self, pdf_path: Path) -> str:
        """
        Extract all text from PDF.

        Args:
            pdf_path: Path to PDF file

        Returns:
            Extracted text as string
        """
        if not pdf_path.exists():
            raise FileNotFoundError(f"PDF not found: {pdf_path}")

        if self._fitz_available:
            return self._extract_with_fitz(pdf_path)
        else:
            return self._extract_fallback(pdf_path)

    def _extract_sections(self, pdf_path: Path) -> Dict[str, str]:
        """
        Extract text organized by sections.

        Args:
            pdf_path: Path to PDF file

        Returns:
            Dictionary mapping section names to text
        """
        if not pdf_path.exists():
            raise FileNotFoundError(f"PDF not found: {pdf_path}")

        if self._fitz_available:
            return self._extract_sections_with_fitz(pdf_path)
        else:
            # Fallback: return all text as "content"
            text = self._extract_fallback(pdf_path)
            return {"content": text}

    def _extract_with_fitz(self, pdf_path: Path) -> str:
        """Extract text using PyMuPDF."""
        import fitz

        try:
            doc = fitz.open(pdf_path)
            text_parts = []

            for page_num, page in enumerate(doc):
                text = page.get_text()
                if text.strip():
                    text_parts.append(text)

            doc.close()
            return "\n".join(text_parts)

        except Exception as e:
            logger.error(f"Error extracting text from {pdf_path}: {e}")
            return ""

    def _extract_sections_with_fitz(self, pdf_path: Path) -> Dict[str, str]:
        """Extract text by sections using PyMuPDF."""
        import fitz

        try:
            doc = fitz.open(pdf_path)

            # Extract all text with page numbers
            pages_text = []
            for page_num, page in enumerate(doc):
                text = page.get_text()
                if text.strip():
                    pages_text.append((page_num, text))

            doc.close()

            # Parse sections
            sections = self._parse_sections(pages_text)
            return sections

        except Exception as e:
            logger.error(f"Error extracting sections from {pdf_path}: {e}")
            return {"error": str(e)}

    def _parse_sections(
        self, pages_text: List[Tuple[int, str]]
    ) -> Dict[str, str]:
        """Parse text into sections based on headers."""
        sections = {}
        current_section = "header"
        current_text = []

        for page_num, page_text in pages_text:
            lines = page_text.split("\n")

            for line in lines:
                line_lower = line.lower().strip()

                # Check if this line is a section header
                is_header = False
                for pattern in self.SECTION_PATTERNS:
                    if re.match(pattern, line_lower, re.IGNORECASE):
                        # Save previous section
                        if current_text:
                            sections[current_section] = "\n".join(current_text)

                        # Start new section
                        current_section = line_lower.replace(".", "").strip()
                        current_text = []
                        is_header = True
                        break

                if not is_header:
                    current_text.append(line)

        # Save last section
        if current_text:
            sections[current_section] = "\n".join(current_text)

        return sections

    def _extract_fallback(self, pdf_path: Path) -> str:
        """Fallback text extraction without PyMuPDF."""
        # This is a placeholder - in production you might use:
        # - pdfplumber
        # - PyPDF2
        # - subprocess call to pdftotext
        logger.warning(f"Using fallback extraction for {pdf_path}")
        return f"[PDF text extraction requires PyMuPDF: {pdf_path}]"

    def extract_metadata(self, pdf_path: Path) -> Dict[str, any]:
        """
        Extract PDF metadata.

        Args:
            pdf_path: Path to PDF file

        Returns:
            Dictionary with metadata (title, author, subject, etc.)
        """
        if not self._fitz_available:
            return {}

        import fitz

        try:
            doc = fitz.open(pdf_path)
            metadata = doc.metadata
            doc.close()
            return metadata
        except Exception as e:
            logger.error(f"Error extracting metadata from {pdf_path}: {e}")
            return {}

    def _extract_for_ai(self, pdf_path: Path) -> Dict[str, any]:
        """
        Extract comprehensive data for AI processing.

        Args:
            pdf_path: Path to PDF file

        Returns:
            Dictionary with:
            - full_text: Complete text
            - sections: Text by section
            - metadata: PDF metadata
            - stats: Word count, page count, etc.
        """
        result = {
            "pdf_path": str(pdf_path),
            "filename": pdf_path.name,
            "full_text": "",
            "sections": {},
            "metadata": {},
            "stats": {},
        }

        try:
            # Extract text
            result["full_text"] = self._extract_text(pdf_path)

            # Extract sections
            result["sections"] = self._extract_sections(pdf_path)

            # Extract metadata
            result["metadata"] = self.extract_metadata(pdf_path)

            # Calculate stats
            result["stats"] = {
                "total_chars": len(result["full_text"]),
                "total_words": len(result["full_text"].split()),
                "num_sections": len(result["sections"]),
            }

            # Add page count if available
            if self._fitz_available:
                import fitz

                doc = fitz.open(pdf_path)
                result["stats"]["num_pages"] = len(doc)
                doc.close()

        except Exception as e:
            logger.error(f"Error in _extract_for_ai: {e}")
            result["error"] = str(e)

        return result


# Convenience function
def _extract_text(pdf_path: Path) -> str:
    """Extract text from PDF file."""
    extractor = PDFParser()
    return extractor._extract_text(pdf_path)


def _extract_for_ai(pdf_path: Path) -> Dict[str, any]:
    """Extract comprehensive PDF data for AI processing."""
    extractor = PDFParser()
    return extractor._extract_for_ai(pdf_path)

# EOF

================================================================================
File: /home/ywatanabe/proj/SciTeX-Code/src/scitex/scholar/README.md
Relative: ../../../../SciTeX-Code/src/scitex/scholar/README.md
Size: 16941 bytes, Lines: 443, Words: ~2823
================================================================================
<!-- ---
!-- Timestamp: 2025-07-24 14:29:08
!-- Author: ywatanabe
!-- File: /home/ywatanabe/proj/scitex_repo/src/scitex/scholar/README.md
!-- --- -->

# SciTeX Scholar

A comprehensive Python library for scientific literature management with automatic enrichment of journal impact factors and citation counts.

## 🌟 Key Features

### Literature Search & Management
- **Multi-Source Search**: Unified search across PubMed, arXiv, and Semantic Scholar
- **Automatic Enrichment**: Journal impact factors (2024 JCR data) and citation counts
- **Smart Deduplication**: Intelligent merging of results from multiple sources
- **Advanced Filtering**: By citations, impact factor, year, journal quartile, etc.
- **Multiple Export Formats**: BibTeX, RIS, JSON, CSV, and Markdown

### PDF Management
- **OpenAthens Authentication**: Institutional access to paywalled papers
- **Multi-Strategy Downloads**: Direct links, Zotero translators, browser automation
- **Local PDF Library**: Index and search your existing PDF collection
- **Text Extraction**: Extract full text and sections for AI/NLP processing
- **Secure Cookie Storage**: Encrypted session management with explicit storage location

### Data Analysis & Integration
- **Pandas Integration**: Convert results to DataFrames for analysis
- **Batch Operations**: Process hundreds of papers efficiently
- **Vector Similarity**: Find related papers using embeddings
- **Statistics & Summaries**: Built-in analysis tools
- **Zotero Integration**: Import/export with Zotero libraries

## Installation

```bash
# Install SciTeX
pip install -e ~/proj/scitex_repo

# Install optional dependencies for enhanced functionality
pip install impact-factor  # For real 2024 JCR impact factors
pip install PyMuPDF       # For PDF text extraction
pip install sentence-transformers  # For vector similarity search
pip install selenium webdriver-manager  # For PDF downloading from Sci-Hub

git clone git@github.com:zotero/translators.git zotero_translators
```

## Quick Start

```python
from scitex.scholar import Scholar, ScholarConfig

# Simple usage with defaults (reads from environment variables)
scholar = Scholar()

# Or customize with ScholarConfig
config = ScholarConfig(
    semantic_scholar_api_key="your-api-key",
    enable_auto_enrich=True,  # Auto-enrich with IF & citations
    use_impact_factor_package=True,  # Use real 2024 JCR data
    default_search_limit=50,
    pdf_dir="~/.scitex/scholar",  # Where to store PDFs
    acknowledge_scihub_ethical_usage=True,
)
scholar = Scholar(config)

# Search for papers - automatically enriched with impact factors & citations
papers = scholar.search(
    query="epilepsy detection machine learning",
    limit=50,
    sources=["pubmed"],  # or ["pubmed", "arxiv", "semantic_scholar"]
    year_min=2020,
    year_max=2024
)

print(f"Found {len(papers)} papers")

# Papers are automatically enriched!
for paper in papers[:3]:
    print(f"{paper.title}")
    print(f"  Journal: {paper.journal} (IF: {paper.impact_factor})")
    print(f"  Citations: {paper.citation_count}")
    print(f"  Year: {paper.year}")
    print()

# Download PDFs for high-impact papers
high_impact = papers.filter(impact_factor_min=5.0)
downloaded = scholar.download_pdfs(high_impact, acknowledge_ethical_usage=True)
print(f"Download Status:\n{downloaded}")

# Access PDFs for processing
for paper in high_impact:
    if paper.pdf_path and paper.pdf_path.exists():
        text = scholar._extract_text(paper.pdf_path)
        print(f"Extracted {len(text)} characters from {paper.title}")

# Disable auto-enrichment for faster searches
config = ScholarConfig(enable_auto_enrich=False)
scholar = Scholar(config)
papers = scholar.search("deep learning")  # No enrichment
```

## Configuration

SciTeX Scholar uses a flexible configuration system with three priority levels:

1. **Direct parameters** (highest priority)
2. **YAML config file** 
3. **Environment variables** (lowest priority)

### Configuration Priority Order

```python
# Method 1: Direct parameters (highest priority)
config = ScholarConfig(
    semantic_scholar_api_key="your-key",
    enable_auto_enrich=True,
    pdf_dir="./my_pdfs"
)
scholar = Scholar(config)

# Method 2: YAML config file
scholar = Scholar("./config.yaml")  # Loads from YAML file

# Method 3: Environment variables (lowest priority)
# Set environment variables with SCITEX_ prefix
# Then just create Scholar without arguments
scholar = Scholar()  # Uses env vars as defaults
```

### Using YAML Configuration File

Create a config file (e.g., `~/.scitex/scholar/config.yaml`):

```yaml
# API Keys and Authentication
semantic_scholar_api_key: "your-api-key-here"
crossref_api_key: "optional-crossref-key"
pubmed_email: "your.email@example.com"
crossref_email: "your.email@example.com"

# Feature Settings
enable_auto_enrich: true
use_impact_factor_package: true
enable_auto_download: false  # Auto-download PDFs during search
acknowledge_scihub_ethical_usage: false  # Must be true to use Sci-Hub

# Search Defaults
default_search_sources:
  - pubmed
  - arxiv
  - semantic_scholar
default_search_limit: 50

# PDF Management
pdf_dir: "~/.scitex/scholar/pdfs"
enable_pdf_extraction: true
max_parallel_downloads: 3
download_timeout: 30

# Performance
max_parallel_requests: 3
request_timeout: 30
cache_size: 1000
```

### Environment Variables

All settings can be configured via environment variables with `SCITEX_SCHOLAR_` prefix:

```bash
# API Keys
export SCITEX_SCHOLAR_SEMANTIC_SCHOLAR_API_KEY="your-key"
export SCITEX_SCHOLAR_CROSSREF_API_KEY="your-key"

# Email addresses (required for PubMed)
export SCITEX_SCHOLAR_PUBMED_EMAIL="your.email@example.com"
export SCITEX_SCHOLAR_CROSSREF_EMAIL="your.email@example.com"

# Feature toggles
export SCITEX_SCHOLAR_AUTO_ENRICH="true"
export SCITEX_SCHOLAR_USE_IMPACT_FACTOR_PACKAGE="true"
export SCITEX_SCHOLAR_AUTO_DOWNLOAD="false"
export SCITEX_SCHOLAR_ACKNOWLEDGE_SCIHUB_ETHICAL_USAGE="false"  # Must be true for Sci-Hub

# OpenAthens institutional access
export SCITEX_SCHOLAR_OPENATHENS_ENABLED="true"
export SCITEX_SCHOLAR_OPENATHENS_EMAIL="your.email@institution.edu"

# PDF directory
export SCITEX_SCHOLAR_PDF_DIR="~/.scitex/scholar/pdfs"

# Config file location (optional)
export SCITEX_SCHOLAR_CONFIG="~/.scitex/scholar/config.yaml"
```

### Configuration Best Practices

1. **For personal use**: Use environment variables in your shell profile
2. **For projects**: Use a YAML config file checked into version control
3. **For scripts**: Pass ScholarConfig directly for explicit control

```python
# Example: Script with explicit config
from scitex.scholar import Scholar, ScholarConfig

# Explicit configuration for reproducibility
config = ScholarConfig(
    enable_auto_enrich=True,
    pdf_dir="./project_pdfs",
    default_search_limit=100
)

scholar = Scholar(config)
papers = scholar.search("your query")
```

## Paper Collection Operations

```python
# Filter papers by various criteria
high_impact = papers.filter(
    min_citations=50,
    impact_factor_min=5.0,
    year_min=2022,
    has_pdf=True
)

# Sort by multiple criteria (descending by default)
sorted_papers = papers.sort_by('impact_factor', 'citation_count')

# Sort with custom order (ascending year, descending citations)
sorted_papers = papers.sort_by(
    ('year', False),        # Ascending year
    ('citation_count', True) # Descending citations
)

# Available sort criteria:
# 'citations', 'citation_count', 'year', 'impact_factor', 
# 'title', 'journal', 'first_author', 'relevance'

# Export to various formats (auto-detected from extension)
papers.save("my_papers.bib")    # BibTeX
papers.save("my_papers.json")   # JSON
papers.save("my_papers.csv")    # CSV for analysis

# Get summary statistics
papers.summarize()  # Prints detailed summary
stats = papers.summary  # Returns dict with basic stats

# Convert to pandas DataFrame for analysis
df = papers.to_dataframe()
print(df.columns)  # See available columns
```

## Individual Paper Access

```python
# Access individual papers
paper = papers[0]

# Basic metadata (always available)
print(paper.title)
print(paper.authors)      # List of author names
print(paper.abstract)
print(paper.year)
print(paper.journal)
print(paper.source)       # "pubmed", "arxiv", etc.

# Identifiers (when available)
print(paper.doi)
print(paper.pmid)         # PubMed ID
print(paper.arxiv_id)     # arXiv ID

# Enriched data (automatically added)
print(paper.impact_factor)    # From impact_factor package (2024 JCR)
print(paper.citation_count)   # From Semantic Scholar/CrossRef
print(paper.journal_quartile) # Q1, Q2, Q3, Q4

# Additional metadata
print(paper.keywords)     # List of keywords
print(paper.pdf_url)      # URL to PDF (when available)
print(paper.pdf_path)     # Local PDF path (when downloaded)

# Methods
similarity = paper.similarity_score(other_paper)
bibtex = paper.to_bibtex()
dict_data = paper.to_dict()
identifier = paper.get_identifier()  # Primary ID (DOI/PMID/etc.)
```

## Enrich an existing BibTeX file

``` python
enriched_papers = scholar.enrich_bibtex(
    bibtex_path="/path/to/original.bib",
    output_path="/path/to/enriched.bib",  # Optional, defaults to overwriting input
    backup=True,                          # Create backup before overwriting
    preserve_original_fields=True,        # Keep all original BibTeX fields
    add_missing_abstracts=True,           # Fetch missing abstracts
    add_missing_urls=True                 # Fetch missing URLs
)
```


## Advanced Features

### PDF Download Features

SciTeX Scholar provides multiple ways to download PDFs:

#### 1. Automatic PDF Downloads During Search

```python
# Enable auto-download in config
config = ScholarConfig(
    enable_auto_download=True,  # Download open-access PDFs automatically
    pdf_dir="~/.scitex/scholar/pdfs"
)
scholar = Scholar(config)

# PDFs are downloaded automatically during search
papers = scholar.search("machine learning", limit=10)
# Open-access PDFs are downloaded in the background
```

#### 2. Manual PDF Downloads

```python
# NEW: Unified download API - accepts multiple input types

# Download from DOI strings
downloaded = scholar.download_pdfs(["10.1234/doi1", "10.5678/doi2"])
print(f"Downloaded {downloaded['successful']} PDFs")

# Download from single DOI
downloaded = scholar.download_pdfs("10.1234/example")

# Download from Papers collection
papers = scholar.search("deep learning")
downloaded = scholar.download_pdfs(papers)

# Download with Papers convenience method
downloaded = papers.download_pdfs()  # Creates Scholar instance if needed

# Advanced options
downloaded = scholar.download_pdfs(
    papers,
    download_dir="./my_pdfs",
    max_workers=4,
    show_progress=True,
    acknowledge_ethical_usage=True  # Required for Sci-Hub
)

# Access downloaded PDF paths
for doi, path in downloaded['downloaded_files'].items():
    print(f"{doi}: {path}")
```

#### 3. Sci-Hub Integration (Use Responsibly)

For papers behind paywalls, SciTeX provides Sci-Hub integration:

**Note**: This feature requires `selenium` and `webdriver-manager`. Install with:
```bash
pip install selenium webdriver-manager
```

```python
from scitex.scholar import dois_to_local_pdfs, dois_to_local_pdfs_async

# You must acknowledge ethical usage terms to use Sci-Hub
# Either set in config or pass directly:

# Extract DOIs from papers
dois = [paper.doi for paper in papers if paper.doi]

# Synchronous download (simpler)
downloaded_paths = dois_to_local_pdfs(
    dois,
    download_dir="./pdfs",
    max_workers=4,  # Parallel downloads
    acknowledge_ethical_usage=True  # Required!
)

# Asynchronous download (faster for many papers)
import asyncio
downloaded_paths = asyncio.run(
    dois_to_local_pdfs_async(
        dois, 
        download_dir="./pdfs",
        acknowledge_ethical_usage=True  # Required!
    )
)
```

**⚖️ IMPORTANT**: This notice applies ONLY to the Sci-Hub PDF download feature. All other SciTeX Scholar features are completely legitimate research tools.

Sci-Hub access may be restricted in your jurisdiction. Please:
- Check your local laws and institutional policies
- Ensure you have proper access rights to the papers
- Use this feature responsibly for legitimate academic purposes only
- See `docs/SCIHUB_ETHICAL_USAGE.md` for detailed guidelines

#### 4. OpenAthens Institutional Access (Recommended)

OpenAthens provides legitimate access to paywalled papers through your institutional subscriptions:

```python
# Configure OpenAthens (one-time setup)
scholar.configure_openathens(
    email="your.email@institution.edu"  # Your institutional email
)

# Or via environment variables
export SCITEX_SCHOLAR_OPENATHENS_EMAIL="your.email@institution.edu"
export SCITEX_SCHOLAR_OPENATHENS_ENABLED="true"
```

**First-time authentication:**
```python
# Authenticate (opens browser for manual login)
await scholar.authenticate_openathens()
# Log in with your institutional credentials
# Session is saved for ~8 hours
```

**Download papers with institutional access:**
```python
# Download specific papers by DOI
dois = ["10.1038/s41586-019-1666-5", "10.1126/science.abj8754"]
results = scholar.download_pdfs(dois, output_dir="./pdfs")

# Download from search results
papers = scholar.search("deep learning", limit=20)
results = scholar.download_pdfs(papers)

# The system automatically uses your saved OpenAthens session
print(f"Downloaded {results['successful']} papers")
```

**Session management:**
```python
# Check if authenticated
if await scholar.is_openathens_authenticated():
    print("Session active")
    
# Force re-authentication if needed
await scholar.authenticate_openathens(force=True)
```

**Supported publishers:**
- Nature Publishing Group
- Science/AAAS
- Cell Press
- Annual Reviews
- Elsevier journals
- Wiley
- Springer Nature
- And many more...

**Security features:**
- Session cookies are encrypted at rest using Fernet encryption
- Machine-specific salt for key derivation (PBKDF2-HMAC-SHA256)
- Restricted file permissions (0600)
- Sessions stored in `~/.scitex/scholar/openathens_sessions/`
- Automatic migration from unencrypted to encrypted format

See `docs/HOW_TO_USE_OPENATHENS.md` for setup instructions and `docs/OPENATHENS_SECURITY.md` for security details.

#### 5. Local PDF Library Management

```python
# Index your existing PDF collection
scholar._index_local_pdfs(
    directory="/path/to/your/pdfs",
    recursive=True  # Search subdirectories
)

# Search within your local PDFs
local_papers = scholar.search_local("neural networks", limit=20)

# Get library statistics
stats = scholar.get_library_stats()
print(f"Total PDFs: {stats['total_files']}")
print(f"Indexed papers: {stats['indexed_count']}")
```

#### Download Configuration Options

```yaml
# In config.yaml
pdf_dir: "~/.scitex/scholar/pdfs"  # Where to store PDFs
enable_auto_download: true          # Auto-download during search
enable_pdf_extraction: true         # Extract text from PDFs
max_parallel_downloads: 3           # Concurrent download limit
download_timeout: 30                # Timeout per download (seconds)

# Sci-Hub settings (optional)
scihub_mirrors:                     # Custom mirror list
  - "https://sci-hub.se/"
  - "https://sci-hub.st/"
scihub_max_retries: 3               # Retry attempts per paper
```

### Text Extraction for AI/NLP

```python
# Extract text from individual PDF
text = scholar._extract_text("/path/to/paper.pdf")

# Extract structured sections
sections = scholar._extract_sections("/path/to/paper.pdf")
# Returns: {"abstract": "...", "introduction": "...", "methods": "..."}

# Comprehensive extraction for AI processing
ai_data = scholar._extract_for_ai("/path/to/paper.pdf")
# Returns: {"full_text": "...", "sections": {...}, "metadata": {...}}

# Batch extract from multiple papers
extracted = scholar.extract_text_from_papers(papers)
for item in extracted:
    print(f"Paper: {item['paper']['title']}")
    print(f"Text length: {len(item['full_text'])} chars")
```

## Environment Variables

Set these for enhanced functionality:

```bash
# Required for PubMed API (any valid email)
export SCITEX_PUBMED_EMAIL="your.email@example.com"

# Optional: For CrossRef API (any valid email)
export SCITEX_CROSSREF_EMAIL="your.email@example.com"

# Optional: For Semantic Scholar API (free at https://www.semanticscholar.org/product/api)
export SCITEX_SEMANTIC_SCHOLAR_API_KEY="your-api-key"

# Optional: For CrossRef API higher rate limits
export SCITEX_CROSSREF_API_KEY="your-api-key"
```

## TODO
- [ ] Add support for EZproxy
- [ ] Add support for Shibboleth

## Citation

If you use SciTeX Scholar in your research, please cite:

```bibtex
@software{scitex_scholar,
  title = {SciTeX Scholar: Scientific Literature Management with Automatic Enrichment},
  author = {Watanabe, Yusuke},
  year = {2025},
  url = {https://github.com/ywatanabe1989/scitex}
}
```

## License

MIT

## Contact

Yusuke Watanabe (ywatanabe@scitex.ai)

<!-- EOF -->

================================================================================
File: /home/ywatanabe/proj/SciTeX-Code/src/scitex/scholar/_Scholar.py
Relative: ../../../../SciTeX-Code/src/scitex/scholar/_Scholar.py
Size: 44000 bytes, Lines: 1097, Words: ~7333
================================================================================
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Timestamp: "2025-07-23 15:52:28 (ywatanabe)"
# File: /home/ywatanabe/proj/scitex_repo/src/scitex/scholar/_Scholar.py
# ----------------------------------------
import os
__FILE__ = (
    "./src/scitex/scholar/_Scholar.py"
)
__DIR__ = os.path.dirname(__FILE__)
# ----------------------------------------

"""
Unified Scholar class for scientific literature management.

This is the main entry point for all scholar functionality, providing:
- Simple, intuitive API
- Smart defaults
- Method chaining
- Progressive disclosure of advanced features
"""

import asyncio
import logging
import re
import warnings
from pathlib import Path
from typing import Any, Dict, List, Optional, Union

# PDF extraction is now handled by scitex.io
from ..errors import ConfigurationError, SciTeXWarning, BibTeXEnrichmentError, ScholarError
from ..io import load
from ._Config import ScholarConfig
from ._DOIResolver import BatchDOIResolver, DOIResolver
from ._Paper import Paper
from ._Papers import Papers
from ._PDFDownloader import PDFDownloader
from ._SearchEngines import UnifiedSearcher, get_scholar_dir
from ._MetadataEnricher import MetadataEnricher

logger = logging.getLogger(__name__)


class Scholar:
    """
    Main interface for SciTeX Scholar - scientific literature management made simple.

    By default, papers are automatically enriched with:
    - Journal impact factors from impact_factor package (2024 JCR data)
    - Citation counts from Semantic Scholar (via DOI/title matching)

    Example usage:
        # Basic search with automatic enrichment
        scholar = Scholar()
        papers = scholar.search("deep learning neuroscience")
        # Papers now have impact_factor and citation_count populated
        papers.save("my_papers.bib")

        # Disable automatic enrichment if needed
        config = ScholarConfig(enable_auto_enrich=False)
        scholar = Scholar(config=config)

        # Search specific source
        papers = scholar.search("transformer models", sources='arxiv')

        # Advanced workflow
        papers = scholar.search("transformer models", year_min=2020) \\
                      .filter(min_citations=50) \\
                      .sort_by("impact_factor") \\
                      .save("transformers.bib")

        # Local library
        scholar._index_local_pdfs("./my_papers")
        local_papers = scholar.search_local("attention mechanism")
    """

    def __init__(
        self,
        config: Optional[Union[ScholarConfig, str, Path]] = None,
    ):
        """
        Initialize Scholar with configuration.

        Args:
            config: Can be:
                   - ScholarConfig instance
                   - Path to YAML config file (str or Path) 
                   - None (uses ScholarConfig.load() to find config)
        """
        # Handle different config input types
        if config is None:
            self.config = ScholarConfig.load()  # Auto-detect config
        elif isinstance(config, (str, Path)):
            self.config = ScholarConfig.from_yaml(config)
        elif isinstance(config, ScholarConfig):
            self.config = config
        else:
            raise TypeError(f"Invalid config type: {type(config)}")
        
        # Set workspace directory
        if self.config.pdf_dir:
            self.workspace_dir = Path(self.config.pdf_dir)
        else:
            self.workspace_dir = get_scholar_dir()
        self.workspace_dir.mkdir(parents=True, exist_ok=True)
        
        # Warn if citations enabled but no API key
        if self.config.enable_auto_enrich and not self.config.semantic_scholar_api_key:
            warnings.warn(
                "SCITEX_SCHOLAR_SEMANTIC_SCHOLAR_API_KEY not found. "
                "Citation counts will use CrossRef (works without key). "
                "For additional citation sources, get a free API key at: "
                "https://www.semanticscholar.org/product/api",
                SciTeXWarning,
                stacklevel=2,
            )

        # Initialize components
        self._searcher = UnifiedSearcher(
            email=self.config.pubmed_email,
            semantic_scholar_api_key=self.config.semantic_scholar_api_key,
        )

        self._enricher = MetadataEnricher(
            semantic_scholar_api_key=self.config.semantic_scholar_api_key,
            crossref_api_key=self.config.crossref_api_key,
            email=self.config.crossref_email,
            use_impact_factor_package=self.config.use_impact_factor_package,
        )

        # Prepare OpenAthens config if enabled
        openathens_config = None
        if self.config.openathens_enabled:
            openathens_config = {
                'email': self.config.openathens_email,
            }
        
        self._pdf_downloader = PDFDownloader(
            download_dir=self.workspace_dir / "pdfs",
            use_scihub=True,
            acknowledge_ethical_usage=self.config.acknowledge_scihub_ethical_usage,
            use_openathens=self.config.openathens_enabled,
            openathens_config=openathens_config
        )

        # Initialize DOI resolver
        self._doi_resolver = DOIResolver(
            email=self.config.crossref_email
        )
        self._batch_resolver = BatchDOIResolver(
            email=self.config.crossref_email,
            max_workers=self.config.max_parallel_requests,
        )

        logger.info(f"Scholar initialized (workspace: {self.workspace_dir})")
        
        # Print configuration summary
        self._print_config_summary()

    def search(
        self,
        query: str,
        limit: int = 100,
        sources: Union[str, List[str]] = "pubmed",
        year_min: Optional[int] = None,
        year_max: Optional[int] = None,
        **kwargs,
    ) -> Papers:
        """
        Search for papers from one or more sources.

        Args:
            query: Search query
            limit: Maximum results (default 100)
            sources: Source(s) to search - can be a string or list of strings
                    ('pubmed', 'semantic_scholar', 'arxiv')
            year_min: Minimum publication year
            year_max: Maximum publication year
            search_mode: Search mode - 'strict' (all terms required) or 'flexible' (any terms)
            **kwargs: Additional search parameters

        Returns:
            Papers with results
        """
        # Ensure sources is a list
        if isinstance(sources, str):
            sources = [sources]

        # Run async search in sync context
        coro = self._searcher.search(
            query=query,
            sources=sources,
            limit=limit,
            year_min=year_min,
            year_max=year_max,
            **kwargs,
        )
        logger.debug(f"Searching with sources: {sources}")
        papers = self._run_async(coro)
        logger.debug(f"Search returned {len(papers)} papers")

        # Create collection (deduplication is automatic)
        # Pass source priority for intelligent deduplication
        collection = Papers(papers, source_priority=sources)

        # Log search results
        if not papers:
            logger.info(f"No results found for query: '{query}'")
            # Suggest alternative sources if default source was used
            if "semantic_scholar" in sources:
                logger.info(
                    "Try searching with different sources or check your internet connection"
                )
        else:
            logger.info(f"Found {len(papers)} papers for query: '{query}'")

        # Auto-enrich if enabled
        if self.config.enable_auto_enrich and papers:
            logger.info("Auto-enriching papers...")
            self._enricher.enrich_all(
                papers,
                enrich_impact_factors=self.config.use_impact_factor_package,
                enrich_citations=True,
                enrich_journal_metrics=self.config.use_impact_factor_package,
            )
            collection._enriched = True

        # Auto-download if enabled
        if self.config.enable_auto_download and papers:
            open_access = [p for p in papers if p.pdf_url]
            if open_access:
                logger.info(
                    f"Auto-downloading {len(open_access)} open-access PDFs..."
                )
                # Download PDFs for open access papers
                dois = [p.doi for p in open_access if p.doi]
                if dois:
                    self.download_pdfs(dois, show_progress=False)

        return collection

    def search_local(self, query: str, limit: int = 20) -> Papers:
        """
        Search local PDF library.

        Args:
            query: Search query
            limit: Maximum results

        Returns:
            Papers with local results
        """
        # Use the UnifiedSearcher with 'local' source
        papers = self._run_async(
            self._searcher.search(query, sources=['local'], limit=limit)
        )
        return Papers(papers)

    def _index_local_pdfs(
        self, directory: Union[str, Path], recursive: bool = True
    ) -> Dict[str, Any]:
        """
        Index local PDF files for searching.

        Args:
            directory: Directory containing PDFs
            recursive: Search subdirectories

        Returns:
            Indexing statistics
        """
        # Build local index using the searcher
        return self._searcher.build_local_index([directory])

    def download_pdfs(
        self, 
        items: Union[List[str], List[Paper], Papers, str, Paper],
        download_dir: Optional[Union[str, Path]] = None,
        force: bool = False,
        max_workers: int = 4,
        show_progress: bool = True,
        acknowledge_ethical_usage: Optional[bool] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        Download PDFs for DOIs or papers.

        This is the main entry point for downloading PDFs. It accepts various input types
        and delegates to the appropriate downloader.

        Args:
            items: Can be:
                - List of DOI strings
                - Single DOI string
                - List of Paper objects
                - Single Paper object
                - Papers collection
            download_dir: Directory to save PDFs (default: workspace_dir/pdfs)
            force: Force re-download even if files exist
            max_workers: Maximum concurrent downloads
            show_progress: Show download progress
            acknowledge_ethical_usage: Acknowledge ethical usage terms for Sci-Hub (default: from config)
            **kwargs: Additional arguments passed to downloader

        Returns:
            Dictionary with download results:
                - 'successful': Number of successful downloads
                - 'failed': Number of failed downloads
                - 'results': List of detailed results
                - 'downloaded_files': Dict mapping DOIs to file paths

        Examples:
            >>> # Download from DOIs
            >>> scholar.download_pdfs(["10.1234/doi1", "10.5678/doi2"])
            
            >>> # Download from Papers collection
            >>> papers = scholar.search("deep learning")
            >>> scholar.download_pdfs(papers)
            
            >>> # Download single DOI
            >>> scholar.download_pdfs("10.1234/example")
        """
        # Use the integrated PDFDownloader instead of standalone SciHubDownloader
        
        # Set default download directory
        if download_dir is None:
            download_dir = self.workspace_dir / "pdfs"
        
        # Normalize input to list
        if isinstance(items, str):
            # Single DOI string
            items = [items]
        elif isinstance(items, Paper):
            # Single Paper object
            items = [items]
        elif isinstance(items, Papers):
            # Papers collection
            items = items.papers
        
        # Determine if we have DOIs or Papers
        if items and isinstance(items[0], str):
            # List of DOI strings
            dois = items
        else:
            # List of Paper objects - extract DOIs
            dois = []
            for paper in items:
                if paper.doi:
                    dois.append(paper)
                else:
                    logger.warning(f"Paper '{paper.title}' has no DOI, skipping download")
        
        if not dois:
            return {
                'successful': 0,
                'failed': 0,
                'results': [],
                'downloaded_files': {}
            }
        
        # Update PDFDownloader settings
        self._pdf_downloader.acknowledge_ethical_usage = acknowledge_ethical_usage
        self._pdf_downloader.max_concurrent = max_workers
        
        # Download PDFs using the integrated downloader
        async def download_batch():
            # Extract DOIs and metadata
            identifiers = []
            metadata_list = []
            
            for item in dois:
                if isinstance(item, str):
                    identifiers.append(item)
                    metadata_list.append(None)
                elif isinstance(item, Paper):
                    identifiers.append(item.doi)
                    metadata_list.append({
                        'title': item.title,
                        'authors': item.authors,
                        'year': item.year
                    })
            
            return await self._pdf_downloader.batch_download(
                identifiers=identifiers,
                output_dir=download_dir,
                metadata_list=metadata_list,
                progress_callback=lambda c, t, _: logger.info(f"Downloaded {c}/{t}") if show_progress else None
            )
        
        # Run async function
        try:
            import asyncio
            loop = asyncio.get_event_loop()
            if loop.is_running():
                # If already in async context
                import concurrent.futures
                with concurrent.futures.ThreadPoolExecutor() as executor:
                    future = executor.submit(asyncio.run, download_batch())
                    results = future.result()
            else:
                results = loop.run_until_complete(download_batch())
        except RuntimeError:
            # No event loop
            results = asyncio.run(download_batch())
        
        # Convert results to match old format for backward compatibility
        successful = sum(1 for path in results.values() if path is not None)
        failed = len(results) - successful
        
        return {
            'successful': successful,
            'failed': failed,
            'results': list(results.keys()),
            'downloaded_files': {doi: str(path) for doi, path in results.items() if path}
        }

    def _enrich_papers(
        self,
        papers: Union[List[Paper], Papers],
        impact_factors: bool = True,
        citations: bool = True,
        journal_metrics: bool = True,
    ) -> Union[List[Paper], Papers]:
        """
        Enrich papers with all available metadata.

        Args:
            papers: Papers to enrich
            impact_factors: Add journal impact factors
            citations: Add citation counts
            journal_metrics: Add quartiles, rankings

        Returns:
            Enriched papers (same type as input)
        """
        if isinstance(papers, Papers):
            self._enricher.enrich_all(
                papers.papers,
                enrich_impact_factors=impact_factors,
                enrich_citations=citations,
                enrich_journal_metrics=journal_metrics,
            )
            papers._enriched = True
            return papers
        else:
            return self._enricher.enrich_all(
                papers,
                enrich_impact_factors=impact_factors,
                enrich_citations=citations,
                enrich_journal_metrics=journal_metrics,
            )

    def enrich_bibtex(
        self,
        bibtex_path: Union[str, Path],
        output_path: Optional[Union[str, Path]] = None,
        backup: bool = True,
        preserve_original_fields: bool = True,
        add_missing_abstracts: bool = True,
        add_missing_urls: bool = True,
    ) -> Papers:
        """
        Enrich an existing BibTeX file with impact factors, citations, and missing fields.

        Args:
            bibtex_path: Path to input BibTeX file
            output_path: Path for enriched output (defaults to input path)
            backup: Create backup of original file before overwriting
            preserve_original_fields: Keep all original BibTeX fields
            add_missing_abstracts: Fetch abstracts for entries without them
            add_missing_urls: Fetch URLs for entries without them

        Returns:
            Papers with enriched papers
        """
        bibtex_path = Path(bibtex_path)
        if not bibtex_path.exists():
            from ..errors import PathNotFoundError
            raise PathNotFoundError(str(bibtex_path))

        # Set output path
        if output_path is None:
            output_path = bibtex_path
        else:
            output_path = Path(output_path)

        # Create backup if needed
        if backup and output_path == bibtex_path:
            backup_path = bibtex_path.with_suffix(".bib.bak")
            import shutil

            shutil.copy2(bibtex_path, backup_path)
            logger.info(f"Created backup: {backup_path}")

        # Load existing BibTeX entries
        logger.info(f"Loading BibTeX file: {bibtex_path}")
        try:
            entries = load(str(bibtex_path))
        except Exception as e:
            raise BibTeXEnrichmentError(
                str(bibtex_path),
                f"Failed to load BibTeX file: {str(e)}"
            )

        # Convert BibTeX entries to Paper objects
        papers = []
        original_fields_map = {}

        for entry in entries:
            paper = self._bibtex_entry_to_paper(entry)
            if paper:
                papers.append(paper)
                # Store original fields for preservation
                if preserve_original_fields:
                    original_fields_map[paper.get_identifier()] = entry[
                        "fields"
                    ]

        logger.info(f"Parsed {len(papers)} papers from BibTeX file")

        # Create collection
        collection = Papers(papers)

        # Enrich papers with impact factors and citations
        if papers:
            logger.info(
                "Enriching papers with impact factors and citations..."
            )
            self._enricher.enrich_all(
                papers,
                enrich_impact_factors=self.config.use_impact_factor_package,
                enrich_citations=True,
                enrich_journal_metrics=self.config.use_impact_factor_package,
            )

            # Always fetch missing DOIs, and optionally abstracts/URLs
            logger.info(
                "Fetching missing DOIs and other information from online sources..."
            )
            self._fetch_missing_fields(
                papers, add_missing_abstracts, add_missing_urls
            )

        # Merge original fields if preserving
        if preserve_original_fields:
            for paper in papers:
                paper_id = paper.get_identifier()
                if paper_id in original_fields_map:
                    paper._original_bibtex_fields = original_fields_map[
                        paper_id
                    ]

        # Save enriched BibTeX
        collection.save(str(output_path))
        logger.info(f"Saved enriched BibTeX to: {output_path}")

        return collection

    def enrich_bibtex(self, *args, **kwargs) -> Papers:
        """
        Backward compatibility alias for enrich_bibtex.

        .. deprecated::
            Use enrich_bibtex() instead.
        """
        warnings.warn(
            "enrich_bibtex() is deprecated, use enrich_bibtex() instead",
            DeprecationWarning,
            stacklevel=2,
        )
        return self.enrich_bibtex(*args, **kwargs)

    def _bibtex_entry_to_paper(self, entry: Dict[str, Any]) -> Optional[Paper]:
        """
        Convert a parsed BibTeX entry to a Paper object.

        Args:
            entry: Parsed BibTeX entry dictionary

        Returns:
            Paper object or None if conversion fails
        """
        try:
            fields = entry.get("fields", {})

            # Extract authors
            authors_str = fields.get("author", "")
            authors = self._parse_bibtex_authors(authors_str)

            # Extract Semantic Scholar Corpus ID if URL is from api.semanticscholar.org
            url = fields.get("url", "")
            semantic_scholar_id = None
            if "api.semanticscholar.org/CorpusId:" in url:
                # Extract corpus ID from URL
                match = re.search(r"CorpusId:(\d+)", url)
                if match:
                    semantic_scholar_id = match.group(1)

            # Create Paper object with available fields
            paper = Paper(
                title=fields.get("title", "").strip(),
                authors=authors,
                year=(
                    int(fields.get("year", 0))
                    if fields.get("year", "").isdigit()
                    else None
                ),
                journal=fields.get("journal", fields.get("booktitle", "")),
                doi=fields.get("doi", ""),
                pmid=fields.get("pmid", ""),
                arxiv_id=fields.get("arxiv", ""),
                abstract=fields.get("abstract", ""),
                pdf_url=fields.get("url", ""),
                keywords=self._parse_bibtex_keywords(
                    fields.get("keywords", "")
                ),
                source=f"bibtex:{entry.get('key', 'unknown')}",
            )

            # Store Semantic Scholar ID for later use
            if semantic_scholar_id:
                paper._semantic_scholar_corpus_id = semantic_scholar_id

            # Add volume, pages if available
            if "volume" in fields:
                paper.volume = fields["volume"]
            if "pages" in fields:
                paper.pages = fields["pages"]

            # Store entry type and key
            paper._bibtex_entry_type = entry.get("entry_type", "article")
            paper._bibtex_key = entry.get("key", "")

            return paper

        except Exception as e:
            logger.warning(f"Failed to convert BibTeX entry: {e}")
            return None

    def _parse_bibtex_authors(self, authors_str: str) -> List[str]:
        """Parse BibTeX author string into list of author names."""
        if not authors_str:
            return []

        # Split by 'and'
        authors = []
        for author in authors_str.split(" and "):
            author = author.strip()
            if author:
                # Handle "Last, First" format
                if "," in author:
                    parts = author.split(",", 1)
                    author = f"{parts[1].strip()} {parts[0].strip()}"
                authors.append(author)

        return authors

    def _parse_bibtex_keywords(self, keywords_str: str) -> List[str]:
        """Parse BibTeX keywords string into list."""
        if not keywords_str:
            return []

        # Split by comma or semicolon
        keywords = []
        for kw in re.split(r"[,;]", keywords_str):
            kw = kw.strip()
            if kw:
                keywords.append(kw)

        return keywords

    def _fetch_missing_fields(
        self, papers: List[Paper], fetch_abstracts: bool, fetch_urls: bool
    ):
        """
        Fetch missing DOIs, abstracts and URLs from online sources.
        Uses batch processing for efficiency when handling multiple papers.

        Args:
            papers: List of Paper objects
            fetch_abstracts: Whether to fetch missing abstracts
            fetch_urls: Whether to fetch missing URLs
        """
        papers_to_update = []

        for paper in papers:
            needs_update = False

            # Always try to get DOI if missing
            if not paper.doi:
                needs_update = True
            if fetch_abstracts and not paper.abstract:
                needs_update = True
            if fetch_urls and not paper.pdf_url:
                needs_update = True

            if needs_update:
                papers_to_update.append(paper)

        if not papers_to_update:
            return

        logger.info(
            f"Fetching missing fields for {len(papers_to_update)} papers..."
        )

        # Use batch processing for efficiency
        if len(papers_to_update) > 1:
            logger.info(
                "Using batch processing for efficient DOI resolution..."
            )

            # Process all papers in batch
            enhanced_data = self._batch_resolver.enhance_papers_parallel(
                papers_to_update, show_progress=True
            )

            # Update URLs if needed
            for paper in papers_to_update:
                if (
                    paper.doi
                    and fetch_urls
                    and "api.semanticscholar.org" in (paper.pdf_url or "")
                ):
                    paper.pdf_url = f"https://doi.org/{paper.doi}"
                    logger.info(
                        f"  ✓ Updated URL to DOI link for: {paper.title[:50]}..."
                    )

        else:
            # Single paper - use regular resolver
            paper = papers_to_update[0]
            logger.debug(f"Processing single paper: {paper.title[:50]}...")

            # Try to get DOI
            if not paper.doi:
                # First try URL resolution if available
                if paper.pdf_url:
                    doi = self._doi_resolver.resolve_from_url(paper.pdf_url)
                    if doi:
                        paper.doi = doi
                        logger.info(f"  ✓ Found DOI from URL: {doi}")

                # If still no DOI, try title-based search
                if not paper.doi:
                    authors_tuple = (
                        tuple(paper.authors) if paper.authors else None
                    )

                    doi = self._doi_resolver.title_to_doi(
                        title=paper.title,
                        year=paper.year,
                        authors=authors_tuple,
                    )

                    if doi:
                        paper.doi = doi
                        logger.info(f"  ✓ Found DOI from title: {doi}")

                # Update URL if needed
                if (
                    paper.doi
                    and fetch_urls
                    and "api.semanticscholar.org" in (paper.pdf_url or "")
                ):
                    paper.pdf_url = f"https://doi.org/{paper.doi}"
                    logger.info(f"  ✓ Updated URL to DOI link")

            # Get abstract if needed
            if paper.doi and fetch_abstracts and not paper.abstract:
                abstract = self._doi_resolver.get_abstract(paper.doi)
                if abstract:
                    paper.abstract = abstract
                    logger.info(f"  ✓ Found abstract")

    def resolve_doi(
        self,
        title: str,
        year: Optional[int] = None,
        authors: Optional[List[str]] = None,
    ) -> Optional[str]:
        """
        Resolve DOI from paper title using multiple sources.

        This method uses CrossRef, PubMed, and OpenAlex to find DOIs,
        avoiding rate-limited services like Semantic Scholar.

        Args:
            title: Paper title
            year: Publication year (optional but improves accuracy)
            authors: List of author names (optional but improves accuracy)

        Returns:
            DOI string if found, None otherwise

        Example:
            doi = scholar.resolve_doi(
                "The functional role of cross-frequency coupling",
                year=2010
            )
            # Returns: "10.1016/j.tins.2010.09.001"
        """
        # Convert authors to tuple for caching if provided
        authors_tuple = tuple(authors) if authors else None
        return self._doi_resolver.title_to_doi(title, year, authors_tuple)

    def _search_crossref_by_title(
        self, title: str, authors: List[str] = None
    ) -> List[Dict[str, Any]]:
        """
        Search CrossRef API by title to find DOI.

        Args:
            title: Paper title
            authors: List of author names (optional)

        Returns:
            List of matching papers from CrossRef
        """
        try:
            from urllib.parse import quote

            import requests

            # Build query
            query = quote(title)

            # Add author to query if available
            if authors and len(authors) > 0:
                first_author = authors[0]
                # Extract last name
                last_name = first_author.split()[-1] if first_author else ""
                if last_name:
                    query += f"+{quote(last_name)}"

            # CrossRef API URL
            url = f"https://api.crossref.org/works"
            params = {
                "query": title,  # Use unquoted title for query parameter
                "rows": 5,
                "select": "DOI,title,author,abstract,published-print,type",
            }

            # Add email if configured for polite access
            if self._email_crossref:
                params["mailto"] = self._email_crossref

            response = requests.get(url, params=params, timeout=10)

            if response.status_code == 200:
                data = response.json()
                items = data.get("message", {}).get("items", [])

                # Filter results by title similarity
                results = []
                for item in items:
                    crossref_title = item.get("title", [""])[0]
                    # Simple similarity check
                    if (
                        crossref_title
                        and self._title_similarity(title, crossref_title) > 0.8
                    ):
                        results.append(item)

                return results
            else:
                logger.debug(f"CrossRef API returned {response.status_code}")
                return []

        except Exception as e:
            logger.debug(f"CrossRef search error: {e}")
            return []

    def _title_similarity(self, title1: str, title2: str) -> float:
        """
        Calculate similarity between two titles (simple approach).

        Args:
            title1: First title
            title2: Second title

        Returns:
            Similarity score between 0 and 1
        """
        # Normalize titles
        t1 = title1.lower().strip()
        t2 = title2.lower().strip()

        # Remove punctuation
        import string

        translator = str.maketrans("", "", string.punctuation)
        t1 = t1.translate(translator)
        t2 = t2.translate(translator)

        # Split into words
        words1 = set(t1.split())
        words2 = set(t2.split())

        # Calculate Jaccard similarity
        if not words1 or not words2:
            return 0.0

        intersection = len(words1 & words2)
        union = len(words1 | words2)

        return intersection / union if union > 0 else 0.0

    def configure_openathens(
        self,
        email: Optional[str] = None,
        save_to_env: bool = False
    ):
        """
        Configure OpenAthens authentication.
        
        Args:
            email: Institutional email address (e.g., 'user@institution.edu')
            save_to_env: Save configuration to environment variables
        
        Note:
            Uses the unified MyAthens interface. Authentication is done
            manually in the browser when you call authenticate_openathens().
        """
        import getpass
        
        # Update configuration
        self.config.openathens_enabled = True
        if email:
            self.config.openathens_email = email
            
        # Save to environment if requested
        if save_to_env:
            import os
            os.environ["SCITEX_SCHOLAR_OPENATHENS_ENABLED"] = "true"
            if email:
                os.environ["SCITEX_SCHOLAR_OPENATHENS_EMAIL"] = email
            
        # Reinitialize PDF downloader with OpenAthens
        openathens_config = {
            'email': self.config.openathens_email,
        }
        
        self._pdf_downloader = PDFDownloader(
            download_dir=self.workspace_dir / "pdfs",
            use_scihub=True,
            acknowledge_ethical_usage=self.config.acknowledge_scihub_ethical_usage,
            use_openathens=True,
            openathens_config=openathens_config
        )
        
        logger.info("OpenAthens configured")
        
    async def authenticate_openathens(self, force: bool = False) -> bool:
        """
        Manually trigger OpenAthens authentication.
        
        Args:
            force: Force re-authentication even if session exists
        
        Returns:
            True if authentication successful
        """
        if not self.config.openathens_enabled:
            raise ScholarError("OpenAthens not configured. Call configure_openathens() first.")
            
        if not self._pdf_downloader.openathens_authenticator:
            raise ScholarError("OpenAthens authenticator not initialized")
            
        return await self._pdf_downloader.openathens_authenticator.authenticate(force=force)
    
    def get_library_stats(self) -> Dict[str, Any]:
        """Get statistics about local PDF library."""
        # Get stats from the local search engine
        pdf_dir = self.workspace_dir / "pdfs"
        if not pdf_dir.exists():
            return {"total_pdfs": 0, "indexed": 0}
        
        # Count PDF files
        pdf_files = list(pdf_dir.rglob("*.pdf"))
        return {
            "total_pdfs": len(pdf_files),
            "pdf_directory": str(pdf_dir),
            "indexed": len(pdf_files)  # Assume all PDFs are indexed
        }

    def search_quick(self, query: str, top_n: int = 5) -> List[str]:
        """
        Quick search returning just paper titles.

        Args:
            query: Search query
            top_n: Number of results

        Returns:
            List of paper titles
        """
        papers = self.search(query, limit=top_n)
        return [p.title for p in papers]

    def find_similar(self, paper_title: str, limit: int = 10) -> Papers:
        """
        Find papers similar to a given paper.

        Args:
            paper_title: Title of reference paper
            limit: Number of similar papers

        Returns:
            Papers with similar papers
        """
        # First find the paper
        reference = self.search(paper_title, limit=1)
        if not reference:
            logger.warning(f"Could not find paper: {paper_title}")
            return Papers([])

        # Search for similar topics
        ref_paper = reference[0]

        # Build query from title and keywords
        query_parts = [ref_paper.title]
        if ref_paper.keywords:
            query_parts.extend(ref_paper.keywords[:3])

        query = " ".join(query_parts)

        # Search and filter out the reference paper
        similar = self.search(query, limit=limit + 1)
        similar_papers = [
            p
            for p in similar.papers
            if p.get_identifier() != ref_paper.get_identifier()
        ]

        return Papers(similar_papers[:limit])

    def _extract_text(self, pdf_path: Union[str, Path]) -> str:
        """
        Extract text from PDF file for downstream AI processing.

        Args:
            pdf_path: Path to PDF file

        Returns:
            Extracted text as string
        """
        # Use scitex.io for PDF text extraction
        from ..io import load

        return load(str(pdf_path), mode="text")

    def _extract_sections(self, pdf_path: Union[str, Path]) -> Dict[str, str]:
        """
        Extract text organized by sections.

        Args:
            pdf_path: Path to PDF file

        Returns:
            Dictionary mapping section names to text
        """
        # Use scitex.io for section extraction
        from ..io import load

        return load(str(pdf_path), mode="sections")

    def _extract_for_ai(self, pdf_path: Union[str, Path]) -> Dict[str, Any]:
        """
        Extract comprehensive data from PDF for AI processing.

        Args:
            pdf_path: Path to PDF file

        Returns:
            Dictionary with:
            - full_text: Complete text
            - sections: Text by section
            - metadata: PDF metadata
            - stats: Word count, page count, etc.
        """
        # Use scitex.io for comprehensive extraction
        from ..io import load

        return load(str(pdf_path), mode="full")

    def extract_text_from_papers(
        self, papers: Union[List[Paper], Papers]
    ) -> List[Dict[str, Any]]:
        """
        Extract text from multiple papers for AI processing.

        Args:
            papers: Papers to extract text from

        Returns:
            List of extraction results with paper metadata
        """
        if isinstance(papers, Papers):
            papers = papers.papers

        results = []
        for paper in papers:
            if paper.pdf_path and paper.pdf_path.exists():
                extraction = self._extract_for_ai(paper.pdf_path)
                extraction["paper"] = {
                    "title": paper.title,
                    "authors": paper.authors,
                    "year": paper.year,
                    "doi": paper.doi,
                    "journal": paper.journal,
                }
                results.append(extraction)
            else:
                # Include paper even without PDF
                results.append(
                    {
                        "paper": {
                            "title": paper.title,
                            "authors": paper.authors,
                            "year": paper.year,
                            "doi": paper.doi,
                            "journal": paper.journal,
                        },
                        "full_text": paper.abstract or "",
                        "error": "No PDF available",
                    }
                )

        return results

    def _print_config_summary(self):
        """Print configuration summary on initialization."""
        print("\n" + "="*60)
        print("SciTeX Scholar v2.0 - Configuration Summary")
        print("="*60)
        
        # Helper function to mask sensitive data
        def mask_sensitive(value, show_first=4):
            """Mask sensitive data showing only first few characters."""
            if not value:
                return None
            if len(str(value)) > show_first + 3:
                return f"{str(value)[:show_first]}{'*' * (len(str(value)) - show_first)}"
            else:
                return "*" * len(str(value))
        
        # API Keys status
        print("\n📚 API Keys:")
        if self.config.semantic_scholar_api_key:
            masked_key = mask_sensitive(self.config.semantic_scholar_api_key)
            print(f"  • Semantic Scholar: ✓ Configured ({masked_key})")
        else:
            print(f"  • Semantic Scholar: ✗ Not set (citations via CrossRef only)")
            
        if self.config.crossref_api_key:
            masked_key = mask_sensitive(self.config.crossref_api_key)
            print(f"  • CrossRef: ✓ Configured ({masked_key})")
        else:
            print(f"  • CrossRef: ✗ Not set (works without key)")
            
        if self.config.pubmed_email:
            # Mask email but show domain
            parts = self.config.pubmed_email.split('@')
            if len(parts) == 2:
                masked_email = f"{mask_sensitive(parts[0], 2)}@{parts[1]}"
            else:
                masked_email = mask_sensitive(self.config.pubmed_email)
            print(f"  • PubMed Email: ✓ Set ({masked_email})")
        else:
            print(f"  • PubMed Email: ✗ Not set (required for PubMed)")
        
        # Features
        print("\n⚙️  Features:")
        print(f"  • Auto-enrichment: {'✓ Enabled' if self.config.enable_auto_enrich else '✗ Disabled'}")
        print(f"  • Impact factors: {'✓ Using JCR package' if self.config.use_impact_factor_package else '✗ Using built-in data'}")
        print(f"  • Auto-download PDFs: {'✓ Enabled' if self.config.enable_auto_download else '✗ Disabled'}")
        print(f"  • Sci-Hub access: {'✓ Acknowledged' if self.config.acknowledge_scihub_ethical_usage else '✗ Requires acknowledgment'}")
        
        # OpenAthens status
        if self.config.openathens_enabled:
            print(f"  • OpenAthens: ✓ Enabled ({self.config.openathens_org_id})")
            if self.config.openathens_username:
                masked_user = mask_sensitive(self.config.openathens_username, 3)
                print(f"    - Username: {masked_user}")
            if self.config.openathens_idp_url:
                print(f"    - IdP URL: {self.config.openathens_idp_url}")
        else:
            print(f"  • OpenAthens: ✗ Disabled")
        
        # Settings
        print("\n📁 Settings:")
        print(f"  • Workspace: {self.workspace_dir}")
        print(f"  • Default search limit: {self.config.default_search_limit}")
        print(f"  • Default sources: {', '.join(self.config.default_search_sources)}")
        
        print("\n💡 Tip: Configure with environment variables or YAML file")
        print("  See: stx.scholar.ScholarConfig.show_env_vars()")
        print("="*60 + "\n")
    
    def _run_async(self, coro):
        """Run async coroutine in sync context."""
        # Simplified approach - always create new event loop
        return asyncio.run(coro)

    # Context manager support
    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        pass

    # Async context manager support
    async def __aenter__(self):
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        pass


# Convenience functions for quick use
def search(query: str, **kwargs) -> Papers:
    """Quick search without creating Scholar instance."""
    scholar = Scholar()
    return scholar.search(query, **kwargs)


def search_quick(query: str, top_n: int = 5) -> List[str]:
    """Quick search returning just titles."""
    scholar = Scholar()
    return scholar.search_quick(query, top_n)


def enrich_bibtex(
    bibtex_path: Union[str, Path],
    output_path: Optional[Union[str, Path]] = None,
) -> Papers:
    """
    Quick function to enrich a BibTeX file with impact factors and citations.

    This is the easiest way to enrich your bibliography with:
    - Journal impact factors (2024 JCR data)
    - Citation counts from CrossRef and Semantic Scholar
    - Missing DOIs

    Args:
        bibtex_path: Path to BibTeX file to enrich
        output_path: Optional output path (defaults to overwriting input with backup)

    Returns:
        Papers collection with enriched data

    Example:
        >>> from scitex.scholar import enrich_bibtex
        >>> enrich_bibtex("my_papers.bib")
        >>> # Or save to new file:
        >>> enrich_bibtex("my_papers.bib", "my_papers_enriched.bib")
    """
    scholar = Scholar()
    return scholar.enrich_bibtex(bibtex_path, output_path)


# Export main class and convenience functions
__all__ = ["Scholar", "search", "search_quick", "enrich_bibtex"]

# EOF

================================================================================
File: /home/ywatanabe/proj/SciTeX-Code/src/scitex/scholar/_SearchEngines.py
Relative: ../../../../SciTeX-Code/src/scitex/scholar/_SearchEngines.py
Size: 34047 bytes, Lines: 882, Words: ~5674
================================================================================
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Timestamp: "2025-07-19 11:10:00 (ywatanabe)"
# File: ./src/scitex/scholar/_search_unified.py
# ----------------------------------------
import os
__FILE__ = (
    "./src/scitex/scholar/_search_unified.py"
)
__DIR__ = os.path.dirname(__FILE__)
# ----------------------------------------

"""
Unified search functionality for SciTeX Scholar.

This module consolidates:
- Web search (Semantic Scholar, PubMed, arXiv)
- Local PDF search
- Vector similarity search
- Search result ranking and merging
"""

import asyncio
import logging
import json
import pickle
from pathlib import Path
from typing import List, Dict, Any, Optional, Union, Tuple
from datetime import datetime
import aiohttp
import xml.etree.ElementTree as ET
from urllib.parse import quote_plus

from ._Paper import Paper
from ..errors import SearchError

logger = logging.getLogger(__name__)


class SearchEngine:
    """Base class for all search engines."""
    
    def __init__(self, name: str):
        self.name = name
        self.rate_limit = 0.1  # seconds between requests
        self._last_request = 0
    
    async def search(self, query: str, limit: int = 20, **kwargs) -> List[Paper]:
        """Search for papers. Must be implemented by subclasses."""
        raise NotImplementedError
    
    async def _rate_limit(self):
        """Enforce rate limiting."""
        import time
        now = time.time()
        elapsed = now - self._last_request
        if elapsed < self.rate_limit:
            await asyncio.sleep(self.rate_limit - elapsed)
        self._last_request = time.time()


class SemanticScholarEngine(SearchEngine):
    """Semantic Scholar search engine."""
    
    def __init__(self, api_key: Optional[str] = None):
        super().__init__("semantic_scholar")
        self.api_key = api_key
        self.base_url = "https://api.semanticscholar.org/graph/v1"
        self.rate_limit = 0.1 if api_key else 1.0  # Faster with API key
        
    
    async def search(self, query: str, limit: int = 20, **kwargs) -> List[Paper]:
        """Search Semantic Scholar for papers."""
        await self._rate_limit()
        
        # Check if query is for a specific paper ID
        if query.startswith('CorpusId:'):
            corpus_id = query.replace('CorpusId:', '').strip()
            paper = await self._fetch_paper_by_id(f"CorpusId:{corpus_id}")
            return [paper] if paper else []
        
        headers = {}
        if self.api_key:
            headers['x-api-key'] = self.api_key
        
        params = {
            'query': query,
            'limit': min(limit, 100),
            'fields': 'title,authors,abstract,year,citationCount,journal,paperId,venue,fieldsOfStudy,isOpenAccess,url,tldr,doi,externalIds'
        }
        
        # Add year filters if provided
        if 'year_min' in kwargs:
            params['year'] = f"{kwargs['year_min']}-"
        if 'year_max' in kwargs:
            if 'year' in params:
                params['year'] = f"{kwargs['year_min']}-{kwargs['year_max']}"
            else:
                params['year'] = f"-{kwargs['year_max']}"
        
        papers = []
        
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(
                    f"{self.base_url}/paper/search",
                    params=params,
                    headers=headers
                ) as response:
                    if response.status == 200:
                        data = await response.json()
                        
                        for item in data.get('data', []):
                            paper = self._parse_semantic_scholar_paper(item)
                            if paper:
                                papers.append(paper)
                    else:
                        error_msg = await response.text()
                        # Don't print error message directly - just log it
                        logger.debug(f"Semantic Scholar API returned {response.status}: {error_msg}")
                        # Return empty list to let other sources handle the search
                        return []
                        
        except Exception as e:
            logger.debug(f"Semantic Scholar search error: {e}")
            # Return empty list instead of raising to allow fallback to other sources
            return []
        
        return papers
    
    async def _fetch_paper_by_id(self, paper_id: str) -> Optional[Paper]:
        """Fetch a specific paper by its ID (CorpusId, DOI, arXiv ID, etc.)."""
        await self._rate_limit()
        
        headers = {}
        if self.api_key:
            headers['x-api-key'] = self.api_key
        
        # Build URL for fetching paper by ID
        url = f"{self.base_url}/paper/{paper_id}"
        params = {
            'fields': 'title,authors,abstract,year,citationCount,journal,paperId,venue,fieldsOfStudy,isOpenAccess,url,tldr,externalIds'
        }
        
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(url, params=params, headers=headers) as response:
                    if response.status == 200:
                        data = await response.json()
                        return self._parse_semantic_scholar_paper(data)
                    else:
                        logger.debug(f"Failed to fetch paper {paper_id}: {response.status}")
                        return None
                        
        except Exception as e:
            logger.debug(f"Error fetching paper {paper_id}: {e}")
            return None
    
    def _parse_semantic_scholar_paper(self, data: Dict[str, Any]) -> Optional[Paper]:
        """Parse Semantic Scholar paper data."""
        if not data or not isinstance(data, dict):
            logger.warning("Received None or non-dict data for Semantic Scholar paper")
            return None
            
        try:
            # Extract authors
            authors = []
            for author_data in data.get('authors', []):
                name = author_data.get('name', '')
                if name:
                    authors.append(name)
            
            # Get PDF URL if available
            pdf_url = None
            if data.get('isOpenAccess'):
                pdf_url = data.get('url')
            
            # Extract journal/venue
            journal_data = data.get('journal')
            journal = ''
            if journal_data and isinstance(journal_data, dict):
                journal = journal_data.get('name', '')
            if not journal:
                journal = data.get('venue', '')
            
            # Extract DOI from externalIds if not directly available
            doi = data.get('doi')
            if not doi and data.get('externalIds'):
                doi = data.get('externalIds', {}).get('DOI')
            
            # Create paper
            paper = Paper(
                title=data.get('title', ''),
                authors=authors,
                abstract=data.get('abstract', '') or (data.get('tldr', {}) or {}).get('text', ''),
                source='semantic_scholar',
                year=data.get('year'),
                doi=doi,
                journal=journal,
                keywords=data.get('fieldsOfStudy', []),
                citation_count=data.get('citationCount', 0),
                pdf_url=pdf_url,
                metadata={
                    'semantic_scholar_paper_id': data.get('paperId'),
                    'fields_of_study': data.get('fieldsOfStudy', []),
                    'is_open_access': data.get('isOpenAccess', False),
                    'citation_count_source': 'Semantic Scholar' if data.get('citationCount') is not None else None,
                    'external_ids': data.get('externalIds', {})
                }
            )
            
            return paper
            
        except Exception as e:
            logger.warning(f"Failed to parse Semantic Scholar paper: {e}")
            return None


class PubMedEngine(SearchEngine):
    """PubMed search engine using E-utilities."""
    
    def __init__(self, email: Optional[str] = None):
        super().__init__("pubmed")
        self.email = email or "research@example.com"
        self.base_url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
        self.rate_limit = 0.4  # NCBI rate limit
    
    async def search(self, query: str, limit: int = 20, **kwargs) -> List[Paper]:
        """Search PubMed for papers."""
        await self._rate_limit()
        
        # First, search for IDs
        search_params = {
            'db': 'pubmed',
            'term': query,
            'retmax': limit,
            'retmode': 'json',
            'email': self.email,
            'sort': 'relevance'  # Sort by relevance instead of date to get diverse years
        }
        
        # Add date filters
        year_min = kwargs.get('year_min')
        year_max = kwargs.get('year_max')
        if year_min is not None or year_max is not None:
            min_date = f"{year_min or 1900}/01/01"
            max_date = f"{year_max or datetime.now().year}/12/31"
            search_params['mindate'] = min_date
            search_params['maxdate'] = max_date
            search_params['datetype'] = 'pdat'  # Publication date
        else:
            # When no date range specified, search last 20 years to avoid only getting current year
            current_year = datetime.now().year
            search_params['mindate'] = f"{current_year - 20}/01/01"
            search_params['maxdate'] = f"{current_year}/12/31"
            search_params['datetype'] = 'pdat'
        
        papers = []
        
        try:
            async with aiohttp.ClientSession() as session:
                # Search for IDs
                logger.info(f"PubMed API URL: {self.base_url}/esearch.fcgi")
                logger.info(f"PubMed search params: {search_params}")
                async with session.get(
                    f"{self.base_url}/esearch.fcgi",
                    params=search_params
                ) as response:
                    if response.status == 200:
                        data = await response.json()
                        pmids = data.get('esearchresult', {}).get('idlist', [])
                        logger.info(f"PubMed search returned {len(pmids)} PMIDs")
                        
                        if pmids:
                            # Fetch details
                            papers = await self._fetch_pubmed_details(session, pmids)
                    else:
                        logger.error(f"PubMed search failed: {response.status}")
                        
        except Exception as e:
            logger.error(f"PubMed search error: {type(e).__name__}: {e}")
            import traceback
            logger.error(traceback.format_exc())
            # Return empty list instead of raising to allow other sources
            return []
        
        return papers
    
    async def _fetch_pubmed_details(self, session: aiohttp.ClientSession, pmids: List[str]) -> List[Paper]:
        """Fetch detailed information for PubMed IDs."""
        await self._rate_limit()
        
        fetch_params = {
            'db': 'pubmed',
            'id': ','.join(pmids),
            'retmode': 'xml',
            'email': self.email
        }
        
        papers = []
        
        async with session.get(
            f"{self.base_url}/efetch.fcgi",
            params=fetch_params
        ) as response:
            if response.status == 200:
                xml_data = await response.text()
                papers = self._parse_pubmed_xml(xml_data)
            else:
                logger.error(f"PubMed fetch failed: {response.status}")
        
        return papers
    
    def _parse_pubmed_xml(self, xml_data: str) -> List[Paper]:
        """Parse PubMed XML response."""
        papers = []
        
        try:
            root = ET.fromstring(xml_data)
            
            for article_elem in root.findall('.//PubmedArticle'):
                try:
                    # Extract article data
                    medline = article_elem.find('.//MedlineCitation')
                    if medline is None:
                        continue
                    
                    # Title
                    title_elem = medline.find('.//ArticleTitle')
                    title = title_elem.text if title_elem is not None else ''
                    
                    # Authors
                    authors = []
                    for author_elem in medline.findall('.//Author'):
                        last_name = author_elem.findtext('LastName', '')
                        first_name = author_elem.findtext('ForeName', '')
                        if last_name:
                            name = f"{last_name}, {first_name}" if first_name else last_name
                            authors.append(name)
                    
                    # Abstract
                    abstract_parts = []
                    for abstract_elem in medline.findall('.//AbstractText'):
                        text = abstract_elem.text or ''
                        abstract_parts.append(text)
                    abstract = ' '.join(abstract_parts)
                    
                    # Year
                    year_elem = medline.find('.//PubDate/Year')
                    year = year_elem.text if year_elem is not None else None
                    
                    # Journal
                    journal_elem = medline.find('.//Journal/Title')
                    journal = journal_elem.text if journal_elem is not None else ''
                    
                    # PMID
                    pmid_elem = medline.find('.//PMID')
                    pmid = pmid_elem.text if pmid_elem is not None else ''
                    
                    # DOI
                    doi = None
                    for id_elem in article_elem.findall('.//ArticleId'):
                        if id_elem.get('IdType') == 'doi':
                            doi = id_elem.text
                            break
                    
                    # Keywords
                    keywords = []
                    for kw_elem in medline.findall('.//MeshHeading/DescriptorName'):
                        if kw_elem.text:
                            keywords.append(kw_elem.text)
                    
                    paper = Paper(
                        title=title,
                        authors=authors,
                        abstract=abstract,
                        source='pubmed',
                        year=year,
                        doi=doi,
                        pmid=pmid,
                        journal=journal,
                        keywords=keywords
                    )
                    
                    papers.append(paper)
                    
                except Exception as e:
                    logger.warning(f"Failed to parse PubMed article: {e}")
                    continue
                    
        except Exception as e:
            logger.error(f"Failed to parse PubMed XML: {e}")
        
        return papers


class ArxivEngine(SearchEngine):
    """arXiv search engine."""
    
    def __init__(self):
        super().__init__("arxiv")
        self.base_url = "http://export.arxiv.org/api/query"
        self.rate_limit = 0.5
    
    async def search(self, query: str, limit: int = 20, **kwargs) -> List[Paper]:
        """Search arXiv for papers."""
        await self._rate_limit()
        
        params = {
            'search_query': f'all:{query}',
            'start': 0,
            'max_results': limit,
            'sortBy': 'relevance',
            'sortOrder': 'descending'
        }
        
        papers = []
        
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(self.base_url, params=params) as response:
                    if response.status == 200:
                        xml_data = await response.text()
                        papers = self._parse_arxiv_xml(xml_data)
                    else:
                        logger.error(f"arXiv search failed: {response.status}")
                        
        except Exception as e:
            logger.error(f"arXiv search error: {e}")
            raise SearchError(query, "arXiv", str(e))
        
        return papers
    
    def _parse_arxiv_xml(self, xml_data: str) -> List[Paper]:
        """Parse arXiv XML response."""
        papers = []
        
        try:
            # Parse XML with namespace
            root = ET.fromstring(xml_data)
            ns = {'atom': 'http://www.w3.org/2005/Atom'}
            
            for entry in root.findall('atom:entry', ns):
                try:
                    # Title
                    title_elem = entry.find('atom:title', ns)
                    title = title_elem.text.strip() if title_elem is not None else ''
                    
                    # Authors
                    authors = []
                    for author_elem in entry.findall('atom:author', ns):
                        name_elem = author_elem.find('atom:name', ns)
                        if name_elem is not None and name_elem.text:
                            authors.append(name_elem.text)
                    
                    # Abstract
                    summary_elem = entry.find('atom:summary', ns)
                    abstract = summary_elem.text.strip() if summary_elem is not None else ''
                    
                    # Year
                    published_elem = entry.find('atom:published', ns)
                    year = None
                    if published_elem is not None and published_elem.text:
                        year = published_elem.text[:4]
                    
                    # arXiv ID
                    id_elem = entry.find('atom:id', ns)
                    arxiv_id = None
                    pdf_url = None
                    if id_elem is not None and id_elem.text:
                        # Extract ID from URL
                        arxiv_id = id_elem.text.split('/')[-1]
                        pdf_url = f"https://arxiv.org/pdf/{arxiv_id}.pdf"
                    
                    # Categories (as keywords)
                    keywords = []
                    for cat_elem in entry.findall('atom:category', ns):
                        term = cat_elem.get('term')
                        if term:
                            keywords.append(term)
                    
                    paper = Paper(
                        title=title,
                        authors=authors,
                        abstract=abstract,
                        source='arxiv',
                        year=year,
                        arxiv_id=arxiv_id,
                        keywords=keywords,
                        pdf_url=pdf_url
                    )
                    
                    papers.append(paper)
                    
                except Exception as e:
                    logger.warning(f"Failed to parse arXiv entry: {e}")
                    continue
                    
        except Exception as e:
            logger.error(f"Failed to parse arXiv XML: {e}")
        
        return papers


class LocalSearchEngine(SearchEngine):
    """Search engine for local PDF files."""
    
    def __init__(self, index_path: Optional[Path] = None):
        super().__init__("local")
        self.index_path = index_path or get_scholar_dir() / "local_index.json"
        self.index = self._load_index()
    
    async def search(self, query: str, limit: int = 20, **kwargs) -> List[Paper]:
        """Search local PDF collection."""
        # Local search is synchronous, wrap in async
        return await asyncio.to_thread(self._search_sync, query, limit, kwargs)
    
    def _search_sync(self, query: str, limit: int, kwargs: dict) -> List[Paper]:
        """Synchronous local search implementation."""
        if not self.index:
            return []
        
        # Simple keyword matching
        query_terms = query.lower().split()
        scored_papers = []
        
        for paper_data in self.index.values():
            # Calculate relevance score
            score = 0
            searchable_text = f"{paper_data.get('title', '')} {paper_data.get('abstract', '')} {' '.join(paper_data.get('keywords', []))}".lower()
            
            for term in query_terms:
                score += searchable_text.count(term)
            
            if score > 0:
                # Create Paper object
                paper = Paper(
                    title=paper_data.get('title', 'Unknown Title'),
                    authors=paper_data.get('authors', []),
                    abstract=paper_data.get('abstract', ''),
                    source='local',
                    year=paper_data.get('year'),
                    keywords=paper_data.get('keywords', []),
                    pdf_path=Path(paper_data.get('pdf_path', ''))
                )
                scored_papers.append((score, paper))
        
        # Sort by score and return top results
        scored_papers.sort(key=lambda x: x[0], reverse=True)
        return [paper for score, paper in scored_papers[:limit]]
    
    def _load_index(self) -> Dict[str, Any]:
        """Load local search index."""
        if self.index_path.exists():
            try:
                with open(self.index_path, 'r') as f:
                    return json.load(f)
            except Exception as e:
                logger.warning(f"Failed to load local index: {e}")
        return {}
    
    def build_index(self, pdf_dirs: List[Path]) -> Dict[str, Any]:
        """Build search index from PDF directories."""
        logger.info(f"Building local index from {len(pdf_dirs)} directories")
        
        index = {}
        stats = {'files_indexed': 0, 'errors': 0}
        
        for pdf_dir in pdf_dirs:
            if not pdf_dir.exists():
                continue
                
            for pdf_path in pdf_dir.rglob("*.pdf"):
                try:
                    # Extract text and metadata
                    paper_data = self._extract_pdf_metadata(pdf_path)
                    if paper_data:
                        index[str(pdf_path)] = paper_data
                        stats['files_indexed'] += 1
                except Exception as e:
                    logger.warning(f"Failed to index {pdf_path}: {e}")
                    stats['errors'] += 1
        
        # Save index
        self.index = index
        self._save_index()
        
        logger.info(f"Indexed {stats['files_indexed']} files with {stats['errors']} errors")
        return stats
    
    def _extract_pdf_metadata(self, pdf_path: Path) -> Optional[Dict[str, Any]]:
        """Extract metadata from PDF file."""
        # This is a placeholder - in real implementation would use PyPDF2 or similar
        return {
            'title': pdf_path.stem.replace('_', ' ').title(),
            'authors': [],
            'abstract': '',
            'year': None,
            'keywords': [],
            'pdf_path': str(pdf_path)
        }
    
    def _save_index(self) -> None:
        """Save index to disk."""
        self.index_path.parent.mkdir(parents=True, exist_ok=True)
        with open(self.index_path, 'w') as f:
            json.dump(self.index, f, indent=2)


class VectorSearchEngine(SearchEngine):
    """Vector similarity search using sentence embeddings."""
    
    def __init__(self, index_path: Optional[Path] = None, model_name: str = "all-MiniLM-L6-v2"):
        super().__init__("vector")
        self.index_path = index_path or get_scholar_dir() / "vector_index.pkl"
        self.model_name = model_name
        self._model = None
        self._papers = []
        self._embeddings = None
        self._load_index()
    
    async def search(self, query: str, limit: int = 20, **kwargs) -> List[Paper]:
        """Search using vector similarity."""
        # Vector search is CPU-bound, use thread
        return await asyncio.to_thread(self._search_sync, query, limit)
    
    def _search_sync(self, query: str, limit: int) -> List[Paper]:
        """Synchronous vector search implementation."""
        if not self._embeddings or not self._papers:
            return []
        
        # Lazy load model
        if self._model is None:
            self._load_model()
        
        # Encode query
        query_embedding = self._model.encode([query])[0]
        
        # Calculate similarities
        import numpy as np
        similarities = np.dot(self._embeddings, query_embedding)
        
        # Get top results
        top_indices = np.argsort(similarities)[-limit:][::-1]
        
        results = []
        for idx in top_indices:
            if idx < len(self._papers):
                results.append(self._papers[idx])
        
        return results
    
    def add_papers(self, papers: List[Paper]) -> None:
        """Add papers to vector index."""
        if self._model is None:
            self._load_model()
        
        # Create searchable text for each paper
        texts = []
        for paper in papers:
            text = f"{paper.title} {paper.abstract}"
            texts.append(text)
        
        # Encode papers
        new_embeddings = self._model.encode(texts)
        
        # Add to index
        import numpy as np
        if self._embeddings is None:
            self._embeddings = new_embeddings
            self._papers = papers
        else:
            self._embeddings = np.vstack([self._embeddings, new_embeddings])
            self._papers.extend(papers)
        
        # Save index
        self._save_index()
    
    def _load_model(self) -> None:
        """Load sentence transformer model."""
        try:
            from sentence_transformers import SentenceTransformer
            self._model = SentenceTransformer(self.model_name)
        except ImportError:
            logger.warning("sentence-transformers not installed. Vector search disabled.")
            self._model = None
    
    def _load_index(self) -> None:
        """Load vector index from disk."""
        if self.index_path.exists():
            try:
                with open(self.index_path, 'rb') as f:
                    data = pickle.load(f)
                    self._papers = data.get('papers', [])
                    self._embeddings = data.get('embeddings')
            except Exception as e:
                logger.warning(f"Failed to load vector index: {e}")
    
    def _save_index(self) -> None:
        """Save vector index to disk."""
        self.index_path.parent.mkdir(parents=True, exist_ok=True)
        data = {
            'papers': self._papers,
            'embeddings': self._embeddings
        }
        with open(self.index_path, 'wb') as f:
            pickle.dump(data, f)


class UnifiedSearcher:
    """
    Unified searcher that combines results from multiple engines.
    """
    
    def __init__(self, 
                 email: Optional[str] = None,
                 semantic_scholar_api_key: Optional[str] = None):
        """Initialize unified searcher with all engines."""
        self.email = email
        self.semantic_scholar_api_key = semantic_scholar_api_key
        self._engines = {}  # Lazy-loaded engines
    
    @property
    def engines(self):
        """Lazy-load engines as needed."""
        return self._engines
    
    def _get_engine(self, source: str):
        """Get or create engine for a source."""
        if source not in self._engines:
            if source == 'semantic_scholar':
                self._engines[source] = SemanticScholarEngine(self.semantic_scholar_api_key)
            elif source == 'pubmed':
                self._engines[source] = PubMedEngine(self.email)
            elif source == 'arxiv':
                self._engines[source] = ArxivEngine()
            elif source == 'local':
                self._engines[source] = LocalSearchEngine()
            elif source == 'vector':
                self._engines[source] = VectorSearchEngine()
            else:
                raise ValueError(f"Unknown source: {source}")
        return self._engines[source]
    
    async def search(self,
                    query: str,
                    sources: List[str] = None,
                    limit: int = 20,
                    deduplicate: bool = True,
                    **kwargs) -> List[Paper]:
        """
        Search across multiple sources and merge results.
        
        Args:
            query: Search query
            sources: List of sources to search (default: all web sources)
            limit: Maximum results per source
            deduplicate: Remove duplicate papers
            **kwargs: Additional parameters for engines
            
        Returns:
            Merged and ranked list of papers
        """
        if sources is None:
            sources = ['pubmed']  # Default to PubMed only
        
        # Filter to valid sources
        valid_sources = ['semantic_scholar', 'pubmed', 'arxiv', 'local', 'vector']
        sources = [s for s in sources if s in valid_sources]
        
        if not sources:
            logger.warning("No valid search sources specified")
            return []
        
        # Search all sources concurrently
        tasks = []
        for source in sources:
            try:
                engine = self._get_engine(source)
                task = engine.search(query, limit, **kwargs)
                tasks.append(task)
            except Exception as e:
                logger.debug(f"Failed to initialize {source} engine: {e}")
        
        logger.debug(f"Searching {len(tasks)} sources: {sources}")
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Merge results
        all_papers = []
        for source, result in zip(sources, results):
            if isinstance(result, Exception):
                logger.debug(f"Search failed for {source}: {result}")
            else:
                logger.debug(f"{source} returned {len(result)} papers")
                all_papers.extend(result)
        
        # Deduplicate if requested
        if deduplicate:
            all_papers = self._deduplicate_papers(all_papers)
        
        # Sort by relevance (using citation count as proxy)
        all_papers.sort(key=lambda p: p.citation_count or 0, reverse=True)
        
        return all_papers[:limit]
    
    def _deduplicate_papers(self, papers: List[Paper]) -> List[Paper]:
        """Remove duplicate papers based on similarity."""
        if not papers:
            return []
        
        unique_papers = [papers[0]]
        
        for paper in papers[1:]:
            is_duplicate = False
            
            for unique_paper in unique_papers:
                if paper.similarity_score(unique_paper) > 0.85:
                    is_duplicate = True
                    # Keep the one with more information
                    if (paper.citation_count or 0) > (unique_paper.citation_count or 0):
                        unique_papers.remove(unique_paper)
                        unique_papers.append(paper)
                    break
            
            if not is_duplicate:
                unique_papers.append(paper)
        
        return unique_papers
    
    def build_local_index(self, pdf_dirs: List[Union[str, Path]]) -> Dict[str, Any]:
        """Build local search index."""
        pdf_dirs = [Path(d) for d in pdf_dirs]
        return self.engines['local'].build_index(pdf_dirs)
    
    def add_to_vector_index(self, papers: List[Paper]) -> None:
        """Add papers to vector search index."""
        self.engines['vector'].add_papers(papers)


# Convenience functions
def get_scholar_dir() -> Path:
    """Get SciTeX scholar directory."""
    scholar_dir = Path.home() / ".scitex" / "scholar"
    scholar_dir.mkdir(parents=True, exist_ok=True)
    return scholar_dir


async def search(query: str,
                sources: List[str] = None,
                limit: int = 20,
                email: Optional[str] = None,
                semantic_scholar_api_key: Optional[str] = None,
                **kwargs) -> List[Paper]:
    """
    Async convenience function for searching papers.
    """
    searcher = UnifiedSearcher(email=email, semantic_scholar_api_key=semantic_scholar_api_key)
    return await searcher.search(query, sources, limit, **kwargs)


def search_sync(query: str,
               sources: List[str] = None,
               limit: int = 20,
               email: Optional[str] = None,
               semantic_scholar_api_key: Optional[str] = None,
               **kwargs) -> List[Paper]:
    """
    Synchronous convenience function for searching papers.
    """
    return asyncio.run(search(query, sources, limit, email, semantic_scholar_api_key, **kwargs))


def build_index(paths: List[Union[str, Path]],
               vector_index: bool = True) -> Dict[str, Any]:
    """
    Build local search indices.
    
    Args:
        paths: Directories containing PDFs
        vector_index: Also build vector similarity index
        
    Returns:
        Statistics about indexing
    """
    searcher = UnifiedSearcher()
    stats = searcher.build_local_index(paths)
    
    if vector_index:
        # Add papers to vector index
        papers = searcher.engines['local']._search_sync("*", 9999, {})
        if papers:
            searcher.add_to_vector_index(papers)
            stats['vector_indexed'] = len(papers)
    
    return stats


# Export all classes and functions
__all__ = [
    'SearchEngine',
    'SemanticScholarEngine', 
    'PubMedEngine',
    'ArxivEngine',
    'LocalSearchEngine',
    'VectorSearchEngine',
    'UnifiedSearcher',
    'get_scholar_dir',
    'search',
    'search_sync',
    'build_index'
]

================================================================================
File: /home/ywatanabe/proj/SciTeX-Code/src/scitex/scholar/_utils.py
Relative: ../../../../SciTeX-Code/src/scitex/scholar/_utils.py
Size: 13533 bytes, Lines: 508, Words: ~2255
================================================================================
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Timestamp: "2025-07-19 11:22:00 (ywatanabe)"
# File: ./src/scitex/scholar/_utils.py
# ----------------------------------------
import os
__FILE__ = (
    "./src/scitex/scholar/_utils.py"
)
__DIR__ = os.path.dirname(__FILE__)
# ----------------------------------------

"""
Utility functions for SciTeX Scholar.

This module provides:
- Format converters (BibTeX, RIS, EndNote)
- Text processing utilities
- Validation functions
- Helper functions
"""

import re
import logging
from typing import List, Dict, Any, Optional, Union
from pathlib import Path
import json
from datetime import datetime

from ._Paper import Paper
from ._Papers import Papers

logger = logging.getLogger(__name__)


# Format converters
def papers_to_bibtex(papers: List[Paper], 
                    include_enriched: bool = True,
                    add_header: bool = True) -> str:
    """
    Convert papers to BibTeX format.
    
    Args:
        papers: List of papers
        include_enriched: Include enriched metadata
        add_header: Add header comments
        
    Returns:
        BibTeX formatted string
    """
    lines = []
    
    if add_header:
        lines.extend([
            f"% SciTeX Scholar Bibliography",
            f"% Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            f"% Total papers: {len(papers)}",
            ""
        ])
    
    # Track used keys to ensure uniqueness
    used_keys = set()
    
    for paper in papers:
        # Get BibTeX with unique key
        paper._generate_bibtex_key()
        original_key = paper._bibtex_key
        
        # Ensure unique key
        counter = 1
        while paper._bibtex_key in used_keys:
            paper._bibtex_key = f"{original_key}{chr(ord('a') + counter - 1)}"
            counter += 1
        
        used_keys.add(paper._bibtex_key)
        
        # Add entry
        bibtex = paper.to_bibtex(include_enriched)
        lines.append(bibtex)
        lines.append("")  # Empty line between entries
    
    return '\n'.join(lines)


def papers_to_ris(papers: List[Paper]) -> str:
    """
    Convert papers to RIS format (for EndNote, Mendeley, etc).
    
    Args:
        papers: List of papers
        
    Returns:
        RIS formatted string
    """
    lines = []
    
    for paper in papers:
        # Determine reference type
        if paper.journal:
            lines.append("TY  - JOUR")
        else:
            lines.append("TY  - GEN")
        
        # Title
        lines.append(f"TI  - {paper.title}")
        
        # Authors
        for author in paper.authors:
            lines.append(f"AU  - {author}")
        
        # Year
        if paper.year:
            lines.append(f"PY  - {paper.year}")
        
        # Journal
        if paper.journal:
            lines.append(f"JO  - {paper.journal}")
        
        # Abstract
        if paper.abstract:
            # RIS format requires line wrapping
            abstract_lines = _wrap_text(paper.abstract, 70)
            for i, line in enumerate(abstract_lines):
                if i == 0:
                    lines.append(f"AB  - {line}")
                else:
                    lines.append(f"      {line}")
        
        # Keywords
        for keyword in paper.keywords:
            lines.append(f"KW  - {keyword}")
        
        # DOI
        if paper.doi:
            lines.append(f"DO  - {paper.doi}")
        
        # End record
        lines.append("ER  - ")
        lines.append("")  # Empty line between records
    
    return '\n'.join(lines)


def papers_to_json(papers: List[Paper], 
                  indent: int = 2,
                  include_metadata: bool = True) -> str:
    """
    Convert papers to JSON format.
    
    Args:
        papers: List of papers
        indent: JSON indentation
        include_metadata: Include generation metadata
        
    Returns:
        JSON formatted string
    """
    data = {
        'papers': [paper.to_dict() for paper in papers]
    }
    
    if include_metadata:
        data['metadata'] = {
            'generated': datetime.now().isoformat(),
            'generator': 'SciTeX Scholar',
            'total_papers': len(papers)
        }
    
    return json.dumps(data, indent=indent, ensure_ascii=False)


def papers_to_markdown(papers: List[Paper],
                      group_by: Optional[str] = None) -> str:
    """
    Convert papers to Markdown format for documentation.
    
    Args:
        papers: List of papers
        group_by: Group by 'year', 'journal', or None
        
    Returns:
        Markdown formatted string
    """
    lines = ["# Bibliography\n"]
    
    if group_by == 'year':
        # Group by year
        from collections import defaultdict
        by_year = defaultdict(list)
        
        for paper in papers:
            year = paper.year or 'Unknown'
            by_year[year].append(paper)
        
        # Sort years descending
        for year in sorted(by_year.keys(), reverse=True):
            lines.append(f"## {year}\n")
            for paper in by_year[year]:
                lines.append(_paper_to_markdown_entry(paper))
                lines.append("")
    
    elif group_by == 'journal':
        # Group by journal
        from collections import defaultdict
        by_journal = defaultdict(list)
        
        for paper in papers:
            journal = paper.journal or 'Preprint'
            by_journal[journal].append(paper)
        
        # Sort journals alphabetically
        for journal in sorted(by_journal.keys()):
            lines.append(f"## {journal}\n")
            for paper in by_journal[journal]:
                lines.append(_paper_to_markdown_entry(paper))
                lines.append("")
    
    else:
        # No grouping
        for paper in papers:
            lines.append(_paper_to_markdown_entry(paper))
            lines.append("")
    
    return '\n'.join(lines)


def _paper_to_markdown_entry(paper: Paper) -> str:
    """Convert single paper to Markdown entry."""
    # Authors
    if len(paper.authors) > 3:
        authors_str = f"{paper.authors[0]} et al."
    else:
        authors_str = ", ".join(paper.authors)
    
    # Basic entry
    entry = f"- **{paper.title}**  \n  {authors_str}"
    
    # Add journal/year
    if paper.journal and paper.year:
        entry += f"  \n  *{paper.journal}* ({paper.year})"
    elif paper.year:
        entry += f" ({paper.year})"
    
    # Add metrics
    metrics = []
    if paper.citation_count:
        metrics.append(f"Citations: {paper.citation_count}")
    if paper.impact_factor:
        metrics.append(f"IF: {paper.impact_factor}")
    
    if metrics:
        entry += f"  \n  {' | '.join(metrics)}"
    
    # Add DOI link
    if paper.doi:
        entry += f"  \n  [DOI: {paper.doi}](https://doi.org/{paper.doi})"
    
    return entry


# Text processing utilities
def normalize_filename(filename: str, max_length: int = 100) -> str:
    """
    Normalize a filename for safe filesystem usage.
    
    Args:
        filename: Original filename
        max_length: Maximum length for the filename
        
    Returns:
        Safe filename
    """
    # Remove/replace unsafe characters
    safe_name = re.sub(r'[<>:"/\\|?*]', '_', filename)
    
    # Replace multiple spaces/underscores with single underscore
    safe_name = re.sub(r'[\s_]+', '_', safe_name)
    
    # Remove leading/trailing spaces and underscores
    safe_name = safe_name.strip('_ ')
    
    # Limit length
    if len(safe_name) > max_length:
        # Keep extension if present
        if '.' in safe_name:
            name, ext = safe_name.rsplit('.', 1)
            safe_name = name[:max_length - len(ext) - 1] + '.' + ext
        else:
            safe_name = safe_name[:max_length]
    
    return safe_name


def normalize_author_name(name: str) -> str:
    """
    Normalize author name format.
    
    Args:
        name: Author name in various formats
        
    Returns:
        Normalized name in "Last, First M." format
    """
    name = name.strip()
    
    # Handle "Last, First" format
    if ',' in name:
        parts = name.split(',', 1)
        last = parts[0].strip()
        first = parts[1].strip()
    else:
        # Handle "First Last" format
        parts = name.split()
        if len(parts) >= 2:
            first = ' '.join(parts[:-1])
            last = parts[-1]
        else:
            return name
    
    # Abbreviate first/middle names
    first_parts = first.split()
    abbreviated = []
    
    for part in first_parts:
        if len(part) > 1 and part[1] != '.':
            # Full name - abbreviate
            abbreviated.append(f"{part[0].upper()}.")
        else:
            # Already abbreviated
            abbreviated.append(part)
    
    first = ' '.join(abbreviated)
    
    return f"{last}, {first}"


def clean_title(title: str) -> str:
    """
    Clean paper title.
    
    Args:
        title: Raw title
        
    Returns:
        Cleaned title
    """
    # Remove excessive whitespace
    title = ' '.join(title.split())
    
    # Remove trailing dots (unless it's an abbreviation)
    if title.endswith('.') and not title[-3:].isupper():
        title = title[:-1]
    
    # Fix common encoding issues
    replacements = {
        'â€™': "'",
        'â€"': "—",
        'â€"': "–",
        'â€œ': '"',
        'â€ ': '"',
    }
    
    for old, new in replacements.items():
        title = title.replace(old, new)
    
    return title


def extract_year_from_text(text: str) -> Optional[str]:
    """
    Extract year from text.
    
    Args:
        text: Text potentially containing a year
        
    Returns:
        Four-digit year string or None
    """
    # Look for 4-digit years between 1900 and current year + 1
    import re
    current_year = datetime.now().year
    
    pattern = r'\b(19\d{2}|20\d{2})\b'
    matches = re.findall(pattern, text)
    
    valid_years = []
    for match in matches:
        year = int(match)
        if 1900 <= year <= current_year + 1:
            valid_years.append(match)
    
    # Return most recent valid year
    return max(valid_years) if valid_years else None


def validate_doi(doi: str) -> bool:
    """
    Validate DOI format.
    
    Args:
        doi: DOI string
        
    Returns:
        True if valid DOI format
    """
    # DOI regex pattern
    pattern = r'^10\.\d{4,}\/[-._;()\/:a-zA-Z0-9]+$'
    return bool(re.match(pattern, doi))


def validate_pmid(pmid: str) -> bool:
    """
    Validate PubMed ID.
    
    Args:
        pmid: PMID string
        
    Returns:
        True if valid PMID
    """
    try:
        pmid_int = int(pmid)
        return 1 <= pmid_int <= 999999999
    except ValueError:
        return False


def validate_arxiv_id(arxiv_id: str) -> bool:
    """
    Validate arXiv ID format.
    
    Args:
        arxiv_id: arXiv ID
        
    Returns:
        True if valid arXiv ID
    """
    # New format: YYMM.NNNNN
    new_pattern = r'^\d{4}\.\d{4,5}(v\d+)?$'
    # Old format: category/YYMMNNN
    old_pattern = r'^[a-z-]+(\.[A-Z]{2})?\/\d{7}(v\d+)?$'
    
    return bool(re.match(new_pattern, arxiv_id) or re.match(old_pattern, arxiv_id))


# Helper functions
def _wrap_text(text: str, width: int = 70) -> List[str]:
    """
    Wrap text to specified width.
    
    Args:
        text: Text to wrap
        width: Line width
        
    Returns:
        List of wrapped lines
    """
    import textwrap
    return textwrap.wrap(text, width=width)


def merge_papers(papers_list: List[List[Paper]], 
                deduplicate: bool = True) -> List[Paper]:
    """
    Merge multiple paper lists.
    
    Args:
        papers_list: List of paper lists
        deduplicate: Remove duplicates
        
    Returns:
        Merged list of papers
    """
    all_papers = []
    for papers in papers_list:
        all_papers.extend(papers)
    
    if not deduplicate:
        return all_papers
    
    # Deduplicate based on identifiers
    seen_ids = set()
    unique_papers = []
    
    for paper in all_papers:
        paper_id = paper.get_identifier()
        if paper_id not in seen_ids:
            seen_ids.add(paper_id)
            unique_papers.append(paper)
    
    return unique_papers


def filter_papers_by_regex(papers: List[Paper],
                          pattern: str,
                          fields: List[str] = None) -> List[Paper]:
    """
    Filter papers using regex pattern.
    
    Args:
        papers: List of papers
        pattern: Regex pattern
        fields: Fields to search in (default: title, abstract)
        
    Returns:
        Filtered papers
    """
    if fields is None:
        fields = ['title', 'abstract']
    
    regex = re.compile(pattern, re.IGNORECASE)
    filtered = []
    
    for paper in papers:
        for field in fields:
            value = getattr(paper, field, '')
            if value and regex.search(value):
                filtered.append(paper)
                break
    
    return filtered


# Export all functions
__all__ = [
    # Format converters
    'papers_to_bibtex',
    'papers_to_ris',
    'papers_to_json',
    'papers_to_markdown',
    
    # Text processing
    'normalize_author_name',
    'clean_title',
    'extract_year_from_text',
    
    # Validation
    'validate_doi',
    'validate_pmid',
    'validate_arxiv_id',
    
    # Helpers
    'merge_papers',
    'filter_papers_by_regex'
]

================================================================================
File: /home/ywatanabe/proj/SciTeX-Code/src/scitex/scholar/_ZoteroTranslatorRunner.py
Relative: ../../../../SciTeX-Code/src/scitex/scholar/_ZoteroTranslatorRunner.py
Size: 18181 bytes, Lines: 528, Words: ~3030
================================================================================
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Timestamp: "2025-07-24 08:30:00 (ywatanabe)"
# File: /home/ywatanabe/proj/scitex_repo/src/scitex/scholar/_ZoteroTranslatorRunner.py
# ----------------------------------------
import os
__FILE__ = (
    "./src/scitex/scholar/_ZoteroTranslatorRunner.py"
)
__DIR__ = os.path.dirname(__FILE__)
# ----------------------------------------

"""
Zotero translator runner for executing JavaScript translators.

This module provides a proper execution environment for Zotero translators,
handling the complex JavaScript environment they expect.
"""

import asyncio
import json
import logging
import re
import subprocess
import tempfile
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

from playwright.async_api import async_playwright, Page

from ..errors import TranslatorError

logger = logging.getLogger(__name__)


class ZoteroTranslatorRunner:
    """
    Executes Zotero translators to extract bibliographic data and PDF URLs.
    
    Uses Playwright to provide a real browser environment with proper DOM
    and JavaScript APIs that translators expect.
    """
    
    def __init__(self, translator_dir: Optional[Path] = None):
        """
        Initialize translator runner.
        
        Args:
            translator_dir: Path to Zotero translators directory
        """
        self.translator_dir = translator_dir or (
            Path(__file__).parent / "zotero_translators"
        )
        
        # Load translator metadata
        self._translators = self._load_translator_metadata()
        
        # Create Zotero API shim
        self._zotero_shim = self._create_zotero_shim()
        
    def _load_translator_metadata(self) -> Dict[str, Dict]:
        """Load metadata from all translators."""
        translators = {}
        
        for js_file in self.translator_dir.glob("*.js"):
            if js_file.name.startswith('_'):
                continue
                
            try:
                with open(js_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                    
                # Extract metadata from first JSON block
                metadata_match = re.search(
                    r'^(\{[^}]+\})',
                    content,
                    re.MULTILINE | re.DOTALL
                )
                
                if metadata_match:
                    # Fix the JSON (some translators have trailing commas)
                    metadata_str = re.sub(r',(\s*})', r'\1', metadata_match.group(1))
                    metadata = json.loads(metadata_str)
                    
                    if 'target' in metadata and metadata.get('translatorType', 0) & 4:
                        # Type 4 = Web translator
                        translators[js_file.stem] = {
                            'path': js_file,
                            'metadata': metadata,
                            'target_regex': metadata['target'],
                            'label': metadata.get('label', js_file.stem),
                            'content': content
                        }
            except Exception as e:
                logger.debug(f"Failed to load translator {js_file.name}: {e}")
                
        logger.info(f"Loaded {len(translators)} web translators")
        return translators
        
    def _create_zotero_shim(self) -> str:
        """Create JavaScript shim for Zotero API."""
        return '''
// Zotero API shim for translators
window.Zotero = {
    // Item types
    itemTypes: {
        journalArticle: "journalArticle",
        book: "book",
        bookSection: "bookSection",
        conferencePaper: "conferencePaper",
        thesis: "thesis",
        webpage: "webpage",
        report: "report",
        patent: "patent",
        preprint: "preprint"
    },
    
    // Debug logging
    debug: function(msg) {
        console.log("[Zotero]", msg);
    },
    
    // Item constructor
    Item: function(type) {
        this.itemType = type || "journalArticle";
        this.creators = [];
        this.tags = [];
        this.attachments = [];
        this.notes = [];
        this.seeAlso = [];
        this.complete = function() {
            window._zoteroItems.push(this);
        };
    },
    
    // Utilities
    Utilities: {
        // HTTP utilities
        requestDocument: async function(url, callback) {
            try {
                const response = await fetch(url);
                const text = await response.text();
                const parser = new DOMParser();
                const doc = parser.parseFromString(text, "text/html");
                callback(doc);
            } catch (e) {
                console.error("requestDocument failed:", e);
                callback(null);
            }
        },
        
        requestText: async function(url, callback) {
            try {
                const response = await fetch(url);
                const text = await response.text();
                callback(text);
            } catch (e) {
                console.error("requestText failed:", e);
                callback(null);
            }
        },
        
        // DOM utilities
        xpath: function(element, xpath) {
            const doc = element.ownerDocument || element;
            const result = doc.evaluate(xpath, element, null, 
                XPathResult.ANY_TYPE, null);
            const items = [];
            let item;
            while (item = result.iterateNext()) {
                items.push(item);
            }
            return items;
        },
        
        xpathText: function(element, xpath) {
            const nodes = this.xpath(element, xpath);
            return nodes.length ? nodes[0].textContent : null;
        },
        
        // Text utilities
        trimInternal: function(str) {
            return str ? str.trim().replace(/\\s+/g, ' ') : '';
        },
        
        cleanAuthor: function(author, type) {
            // Simple author cleaning
            return {
                firstName: author.split(' ').slice(0, -1).join(' '),
                lastName: author.split(' ').slice(-1)[0],
                creatorType: type || "author"
            };
        },
        
        // Other utilities
        strToISO: function(str) {
            // Simple date parsing
            return str;
        },
        
        processDocuments: async function(urls, callback) {
            for (const url of urls) {
                await this.requestDocument(url, callback);
            }
        }
    },
    
    // Done callback
    done: function() {
        console.log("Translation complete");
    },
    
    // Is running in Zotero
    isMLZ: false,
    isFx: false,
    isChrome: true,
    isStandalone: false,
    isConnector: true
};

// Global shortcuts
window.ZU = window.Zotero.Utilities;
window.Z = window.Zotero;

// Storage for completed items
window._zoteroItems = [];

// Helper functions used by translators
window.attr = function(element, selector, attribute) {
    const elem = typeof element === 'string' ? 
        document.querySelector(element) : 
        element.querySelector ? element.querySelector(selector) : null;
    return elem ? elem.getAttribute(attribute) : null;
};

window.text = function(element, selector) {
    const elem = selector ? 
        (element.querySelector ? element.querySelector(selector) : null) : 
        element;
    return elem ? elem.textContent.trim() : null;
};

window.innerText = function(element, selector) {
    const elem = selector ? 
        (element.querySelector ? element.querySelector(selector) : null) : 
        element;
    return elem ? elem.innerText : null;
};
'''
        
    def find_translator_for_url(self, url: str) -> Optional[Dict]:
        """Find appropriate translator for URL."""
        for name, translator in self._translators.items():
            try:
                if re.match(translator['target_regex'], url):
                    logger.info(f"Found translator: {translator['label']} for {url}")
                    return translator
            except Exception as e:
                logger.debug(f"Regex match failed for {name}: {e}")
                continue
                
        return None
        
    async def run_translator(
        self,
        url: str,
        translator: Optional[Dict] = None
    ) -> Dict[str, Any]:
        """
        Run translator on URL to extract bibliographic data.
        
        Args:
            url: URL to process
            translator: Specific translator to use (auto-detect if None)
            
        Returns:
            Dictionary with items and status
        """
        # Find translator if not provided
        if not translator:
            translator = self.find_translator_for_url(url)
            if not translator:
                return {
                    'success': False,
                    'error': f'No translator found for {url}',
                    'items': []
                }
                
        logger.info(f"Running translator: {translator['label']}")
        
        async with async_playwright() as p:
            browser = await p.chromium.launch(
                headless=True,
                args=['--disable-blink-features=AutomationControlled']
            )
            
            try:
                page = await browser.new_page()
                
                # Inject Zotero shim before navigation
                await page.add_init_script(self._zotero_shim)
                
                # Navigate to URL
                await page.goto(url, wait_until='networkidle', timeout=30000)
                
                # Inject translator code
                translator_code = translator['content']
                
                # Execute translator
                result = await page.evaluate('''
                    async (translatorCode) => {
                        try {
                            // Reset items array
                            window._zoteroItems = [];
                            
                            // Inject translator code
                            eval(translatorCode);
                            
                            // Run detectWeb
                            if (typeof detectWeb === 'function') {
                                const itemType = detectWeb(document, window.location.href);
                                
                                if (itemType) {
                                    // Run doWeb
                                    if (typeof doWeb === 'function') {
                                        await doWeb(document, window.location.href);
                                        
                                        // Wait a bit for async operations
                                        await new Promise(resolve => setTimeout(resolve, 2000));
                                    }
                                }
                            }
                            
                            return {
                                success: true,
                                items: window._zoteroItems
                            };
                            
                        } catch (error) {
                            return {
                                success: false,
                                error: error.toString(),
                                items: []
                            };
                        }
                    }
                ''', translator_code)
                
                # Extract PDF URLs from items
                for item in result.get('items', []):
                    await self._enhance_item_with_pdf_urls(page, item)
                    
                return result
                
            finally:
                await browser.close()
                
    async def _enhance_item_with_pdf_urls(self, page: Page, item: Dict):
        """Enhance item with direct PDF URLs if available."""
        # Look for PDF links on the page
        pdf_urls = await page.evaluate('''
            () => {
                const urls = [];
                
                // Common PDF link selectors
                const selectors = [
                    'a[href*=".pdf"]',
                    'a[href*="/pdf/"]',
                    'a[href*="/full.pdf"]',
                    'a[href*="/download/"]',
                    'a:has-text("PDF")',
                    'a:has-text("Download")',
                    '.pdf-link',
                    '[data-pdf-url]'
                ];
                
                for (const selector of selectors) {
                    const links = document.querySelectorAll(selector);
                    for (const link of links) {
                        const href = link.href || link.getAttribute('data-pdf-url');
                        if (href && !urls.includes(href)) {
                            urls.push(href);
                        }
                    }
                }
                
                return urls;
            }
        ''')
        
        # Add PDF URLs to attachments if not already present
        if pdf_urls and not any(
            att.get('mimeType') == 'application/pdf' 
            for att in item.get('attachments', [])
        ):
            for pdf_url in pdf_urls[:1]:  # Take first PDF URL
                item.setdefault('attachments', []).append({
                    'title': 'Full Text PDF',
                    'mimeType': 'application/pdf',
                    'url': pdf_url
                })
                
    async def extract_pdf_urls(self, url: str) -> List[str]:
        """
        Extract PDF URLs from a webpage using translators.
        
        Args:
            url: URL to extract PDFs from
            
        Returns:
            List of PDF URLs found
        """
        result = await self.run_translator(url)
        
        pdf_urls = []
        for item in result.get('items', []):
            for attachment in item.get('attachments', []):
                if (attachment.get('mimeType') == 'application/pdf' and 
                    'url' in attachment):
                    pdf_urls.append(attachment['url'])
                    
        return pdf_urls
        
    async def batch_extract(
        self,
        urls: List[str],
        max_concurrent: int = 3
    ) -> Dict[str, Dict[str, Any]]:
        """
        Extract data from multiple URLs concurrently.
        
        Args:
            urls: List of URLs to process
            max_concurrent: Maximum concurrent browser instances
            
        Returns:
            Dictionary mapping URL to extraction results
        """
        semaphore = asyncio.Semaphore(max_concurrent)
        
        async def extract_with_limit(url: str) -> Tuple[str, Dict]:
            async with semaphore:
                result = await self.run_translator(url)
                return url, result
                
        results = await asyncio.gather(
            *[extract_with_limit(url) for url in urls],
            return_exceptions=True
        )
        
        # Process results
        extracted = {}
        for result in results:
            if isinstance(result, Exception):
                logger.error(f"Extraction failed: {result}")
            else:
                url, data = result
                extracted[url] = data
                
        return extracted


# Convenience functions
async def extract_bibliography_from_url(url: str) -> List[Dict[str, Any]]:
    """
    Extract bibliographic data from URL using Zotero translators.
    
    Args:
        url: URL to extract from
        
    Returns:
        List of bibliographic items
    """
    runner = ZoteroTranslatorRunner()
    result = await runner.run_translator(url)
    return result.get('items', [])


async def find_pdf_urls(url: str) -> List[str]:
    """
    Find PDF URLs on a webpage using Zotero translators.
    
    Args:
        url: URL to search
        
    Returns:
        List of PDF URLs
    """
    runner = ZoteroTranslatorRunner()
    return await runner.extract_pdf_urls(url)


if __name__ == "__main__":
    # Example usage
    import sys
    
    async def test_translator():
        if len(sys.argv) > 1:
            url = sys.argv[1]
        else:
            # Default test URLs
            test_urls = [
                "https://arxiv.org/abs/2103.14030",
                "https://www.nature.com/articles/s41586-021-03819-2",
                "https://scholar.google.com/scholar?q=machine+learning",
            ]
            url = test_urls[0]
            
        print(f"\nTesting translator on: {url}")
        
        runner = ZoteroTranslatorRunner()
        
        # Find translator
        translator = runner.find_translator_for_url(url)
        if translator:
            print(f"Found translator: {translator['label']}")
        else:
            print("No translator found")
            return
            
        # Run translator
        result = await runner.run_translator(url)
        
        if result['success']:
            print(f"\nExtracted {len(result['items'])} items:")
            for i, item in enumerate(result['items']):
                print(f"\n{i+1}. {item.get('title', 'No title')}")
                print(f"   Type: {item.get('itemType')}")
                if 'DOI' in item:
                    print(f"   DOI: {item['DOI']}")
                    
                # Show attachments
                for att in item.get('attachments', []):
                    if att.get('mimeType') == 'application/pdf':
                        print(f"   PDF: {att.get('url', 'No URL')}")
        else:
            print(f"Extraction failed: {result.get('error')}")
            
        # Test PDF extraction
        print("\n\nExtracting PDF URLs...")
        pdf_urls = await runner.extract_pdf_urls(url)
        print(f"Found {len(pdf_urls)} PDFs:")
        for pdf_url in pdf_urls:
            print(f"  - {pdf_url}")
            
    asyncio.run(test_translator())

# EOF

