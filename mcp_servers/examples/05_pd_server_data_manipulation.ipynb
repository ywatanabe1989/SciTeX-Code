{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SciTeX PD Server - Enhanced Pandas Operations\n",
    "\n",
    "This notebook demonstrates the enhanced pandas functionality provided by the SciTeX PD MCP server.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The PD server enhances pandas with:\n",
    "- Automatic type inference and conversion\n",
    "- Statistical analysis helpers\n",
    "- Data cleaning utilities\n",
    "- Time series enhancements\n",
    "- Multi-dataframe operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Enhanced DataFrame Creation and Type Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard pandas approach\n",
    "standard_pandas = '''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create dataframe with mixed types\n",
    "data = {\n",
    "    'subject_id': ['S001', 'S002', 'S003', 'S004', 'S005'],\n",
    "    'age': ['25', '30', '28', '35', '32'],  # Strings that should be numeric\n",
    "    'group': ['Control', 'Treatment', 'Control', 'Treatment', 'Control'],\n",
    "    'score': [85.5, 92.3, 78.9, 88.2, 91.1],\n",
    "    'date': ['2024-01-15', '2024-01-16', '2024-01-15', '2024-01-17', '2024-01-16']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Manual type conversions\n",
    "df['age'] = pd.to_numeric(df['age'])\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df['group'] = pd.Categorical(df['group'])\n",
    "\n",
    "# Calculate statistics manually\n",
    "mean_by_group = df.groupby('group')['score'].mean()\n",
    "std_by_group = df.groupby('group')['score'].std()\n",
    "'''\n",
    "\n",
    "print(\"STANDARD PANDAS APPROACH:\")\n",
    "print(standard_pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SciTeX enhanced approach\n",
    "scitex_pandas = '''\n",
    "import scitex as stx\n",
    "import numpy as np\n",
    "\n",
    "def enhanced_dataframe_operations():\n",
    "    \"\"\"Demonstrate enhanced pandas operations.\"\"\"\n",
    "    # Create dataframe with automatic type inference\n",
    "    data = {\n",
    "        'subject_id': ['S001', 'S002', 'S003', 'S004', 'S005'],\n",
    "        'age': ['25', '30', '28', '35', '32'],\n",
    "        'group': ['Control', 'Treatment', 'Control', 'Treatment', 'Control'],\n",
    "        'score': [85.5, 92.3, 78.9, 88.2, 91.1],\n",
    "        'date': ['2024-01-15', '2024-01-16', '2024-01-15', '2024-01-17', '2024-01-16'],\n",
    "        'response_time': [250, 180, 320, 195, 210],\n",
    "        'accuracy': [0.95, 0.98, 0.87, 0.92, 0.96]\n",
    "    }\n",
    "    \n",
    "    # Force DataFrame with automatic type optimization\n",
    "    df = stx.pd.force_df(\n",
    "        data,\n",
    "        infer_types=True,      # Automatically detect and convert types\n",
    "        parse_dates=True,      # Find and parse date columns\n",
    "        categorical_threshold=0.5,  # Convert to categorical if < 50% unique\n",
    "        optimize_memory=True   # Use optimal dtypes for memory efficiency\n",
    "    )\n",
    "    \n",
    "    # Enhanced statistical summary\n",
    "    summary = stx.pd.describe_enhanced(\n",
    "        df,\n",
    "        include_categorical=True,\n",
    "        include_missing=True,\n",
    "        include_outliers=True,\n",
    "        percentiles=[0.05, 0.25, 0.5, 0.75, 0.95],\n",
    "        by_group='group'  # Separate statistics by group\n",
    "    )\n",
    "    \n",
    "    # Find p-values for group comparisons\n",
    "    p_values = stx.pd.find_pval(\n",
    "        df,\n",
    "        value_col='score',\n",
    "        group_col='group',\n",
    "        test='auto',  # Automatically select appropriate test\n",
    "        correct_multiple=True,  # Apply multiple comparison correction\n",
    "        include_effect_size=True\n",
    "    )\n",
    "    \n",
    "    # Advanced groupby with multiple aggregations\n",
    "    grouped = stx.pd.groupby_enhanced(\n",
    "        df,\n",
    "        by='group',\n",
    "        agg={\n",
    "            'score': ['mean', 'std', 'sem', 'ci95'],\n",
    "            'response_time': ['median', 'iqr'],\n",
    "            'accuracy': ['mean', 'min', 'max']\n",
    "        },\n",
    "        add_counts=True,\n",
    "        add_missing=True\n",
    "    )\n",
    "    \n",
    "    # Pivot with enhanced features\n",
    "    pivoted = stx.pd.pivot_enhanced(\n",
    "        df,\n",
    "        index='subject_id',\n",
    "        columns='date',\n",
    "        values='score',\n",
    "        fill_method='interpolate',  # Smart filling of missing values\n",
    "        add_margins=True,  # Add row/column totals\n",
    "        add_statistics=True  # Add mean, std rows\n",
    "    )\n",
    "    \n",
    "    # Time series resampling with multiple methods\n",
    "    ts_data = stx.pd.resample_enhanced(\n",
    "        df.set_index('date'),\n",
    "        rule='D',  # Daily\n",
    "        agg_numeric='mean',\n",
    "        agg_categorical='mode',\n",
    "        interpolate_missing=True,\n",
    "        add_rolling_stats=True,\n",
    "        window=3\n",
    "    )\n",
    "    \n",
    "    # Save with enhanced options\n",
    "    stx.io.save(\n",
    "        df, \n",
    "        './data/experiment_data.csv',\n",
    "        include_metadata=True,  # Save data types and descriptions\n",
    "        compress=True,  # Automatic compression\n",
    "        symlink_from_cwd=True\n",
    "    )\n",
    "    \n",
    "    return df, summary, p_values, grouped\n",
    "'''\n",
    "\n",
    "print(\"SCITEX ENHANCED APPROACH:\")\n",
    "print(scitex_pandas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Data Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning with SciTeX\n",
    "data_cleaning = '''\n",
    "import scitex as stx\n",
    "import numpy as np\n",
    "\n",
    "def advanced_data_cleaning():\n",
    "    \"\"\"Demonstrate advanced data cleaning capabilities.\"\"\"\n",
    "    # Create messy data\n",
    "    messy_data = {\n",
    "        'id': ['001', '002', '003', '004', '005', '006'],\n",
    "        'value': [10.5, -999, 12.3, np.nan, 15.2, 1000],  # With missing and outliers\n",
    "        'category': ['A', 'B', 'A', 'C', None, 'B'],\n",
    "        'text': ['  Hello  ', 'WORLD', '  test\\\\n', 'Data', None, 'Analysis '],\n",
    "        'date': ['2024-01-01', '2024/01/02', '01-03-2024', 'invalid', '2024-01-05', None]\n",
    "    }\n",
    "    \n",
    "    df = stx.pd.DataFrame(messy_data)\n",
    "    \n",
    "    # Comprehensive cleaning\n",
    "    cleaned = stx.pd.clean_dataframe(\n",
    "        df,\n",
    "        # Missing value handling\n",
    "        missing_values=[-999, 'invalid', 'N/A'],  # Custom missing indicators\n",
    "        fill_strategy={\n",
    "            'numeric': 'interpolate',  # or 'mean', 'median', 'forward_fill'\n",
    "            'categorical': 'mode',\n",
    "            'datetime': 'forward_fill'\n",
    "        },\n",
    "        \n",
    "        # Outlier detection and handling\n",
    "        outlier_method='iqr',  # or 'zscore', 'isolation_forest'\n",
    "        outlier_threshold=1.5,\n",
    "        outlier_action='clip',  # or 'remove', 'nan'\n",
    "        \n",
    "        # Text cleaning\n",
    "        text_operations=[\n",
    "            'strip',           # Remove whitespace\n",
    "            'lowercase',       # Convert to lowercase\n",
    "            'remove_special',  # Remove special characters\n",
    "            'normalize_whitespace'  # Fix multiple spaces\n",
    "        ],\n",
    "        \n",
    "        # Date parsing\n",
    "        parse_dates=True,\n",
    "        date_formats=['%Y-%m-%d', '%Y/%m/%d', '%d-%m-%Y'],  # Try multiple formats\n",
    "        \n",
    "        # Duplicate handling\n",
    "        remove_duplicates=True,\n",
    "        duplicate_subset=None,  # Check all columns\n",
    "        keep='first'\n",
    "    )\n",
    "    \n",
    "    # Data validation\n",
    "    validation_report = stx.pd.validate_dataframe(\n",
    "        cleaned,\n",
    "        rules={\n",
    "            'id': {'type': 'string', 'unique': True, 'not_null': True},\n",
    "            'value': {'type': 'numeric', 'range': (0, 100)},\n",
    "            'category': {'type': 'categorical', 'values': ['A', 'B', 'C']},\n",
    "            'date': {'type': 'datetime', 'after': '2024-01-01'}\n",
    "        },\n",
    "        return_clean=True,  # Return only valid rows\n",
    "        report_format='detailed'  # Get detailed violation report\n",
    "    )\n",
    "    \n",
    "    # Feature engineering\n",
    "    engineered = stx.pd.feature_engineering(\n",
    "        cleaned,\n",
    "        operations=[\n",
    "            # Temporal features from date\n",
    "            {'type': 'extract_date_features', \n",
    "             'column': 'date',\n",
    "             'features': ['year', 'month', 'day', 'dayofweek', 'quarter']},\n",
    "            \n",
    "            # Categorical encoding\n",
    "            {'type': 'encode_categorical',\n",
    "             'column': 'category',\n",
    "             'method': 'onehot',  # or 'label', 'target', 'ordinal'\n",
    "             'handle_unknown': 'ignore'},\n",
    "            \n",
    "            # Numeric transformations\n",
    "            {'type': 'transform_numeric',\n",
    "             'column': 'value',\n",
    "             'methods': ['log1p', 'sqrt', 'reciprocal'],\n",
    "             'keep_original': True},\n",
    "            \n",
    "            # Interaction features\n",
    "            {'type': 'create_interactions',\n",
    "             'columns': ['value', 'category_A', 'category_B'],\n",
    "             'degree': 2}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Data quality report\n",
    "    quality_report = stx.pd.generate_quality_report(\n",
    "        original=df,\n",
    "        cleaned=cleaned,\n",
    "        include_plots=True,\n",
    "        save_path='./reports/data_quality_report.html'\n",
    "    )\n",
    "    \n",
    "    print(\"Data Cleaning Summary:\")\n",
    "    print(f\"Original shape: {df.shape}\")\n",
    "    print(f\"Cleaned shape: {cleaned.shape}\")\n",
    "    print(f\"Valid rows: {validation_report['n_valid']}\")\n",
    "    print(f\"Features after engineering: {engineered.shape[1]}\")\n",
    "    \n",
    "    return cleaned, engineered, quality_report\n",
    "'''\n",
    "\n",
    "print(\"DATA CLEANING WITH SCITEX:\")\n",
    "print(data_cleaning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Advanced Merging and Joining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced merging operations\n",
    "advanced_merging = '''\n",
    "import scitex as stx\n",
    "\n",
    "def advanced_merge_operations():\n",
    "    \"\"\"Demonstrate enhanced merging and joining capabilities.\"\"\"\n",
    "    # Create sample datasets\n",
    "    subjects = stx.pd.DataFrame({\n",
    "        'subject_id': ['S001', 'S002', 'S003', 'S004'],\n",
    "        'age': [25, 30, 28, 35],\n",
    "        'group': ['Control', 'Treatment', 'Control', 'Treatment']\n",
    "    })\n",
    "    \n",
    "    measurements = stx.pd.DataFrame({\n",
    "        'subject_id': ['S001', 'S001', 'S002', 'S003', 'S003', 'S005'],\n",
    "        'timepoint': [1, 2, 1, 1, 2, 1],\n",
    "        'value': [10.5, 12.3, 11.8, 9.7, 10.2, 15.3]\n",
    "    })\n",
    "    \n",
    "    demographics = stx.pd.DataFrame({\n",
    "        'id': ['S001', 'S002', 'S004'],  # Note: different column name\n",
    "        'gender': ['M', 'F', 'M'],\n",
    "        'education': ['Bachelor', 'Master', 'PhD']\n",
    "    })\n",
    "    \n",
    "    # Smart merge with automatic key detection\n",
    "    merged = stx.pd.merge_smart(\n",
    "        [subjects, measurements, demographics],\n",
    "        keys='auto',  # Automatically detect common keys\n",
    "        how='outer',  # Preserve all data\n",
    "        indicator=True,  # Track merge source\n",
    "        validate='many_to_many',  # Validate merge type\n",
    "        fuzzy_match=True,  # Handle slight key differences\n",
    "        fuzzy_threshold=0.9\n",
    "    )\n",
    "    \n",
    "    # Merge with conflict resolution\n",
    "    merged_conflicts = stx.pd.merge_with_conflicts(\n",
    "        subjects,\n",
    "        demographics.rename(columns={'id': 'subject_id'}),\n",
    "        on='subject_id',\n",
    "        conflict_resolution={\n",
    "            'prefer_left': ['age'],  # Keep left DataFrame values\n",
    "            'prefer_right': ['gender'],  # Keep right DataFrame values\n",
    "            'combine': ['group', 'education']  # Combine into single column\n",
    "        },\n",
    "        track_conflicts=True\n",
    "    )\n",
    "    \n",
    "    # Time-aware merge for longitudinal data\n",
    "    time_series1 = stx.pd.DataFrame({\n",
    "        'subject_id': ['S001', 'S001', 'S002'],\n",
    "        'date': ['2024-01-01', '2024-01-03', '2024-01-02'],\n",
    "        'measure1': [10, 12, 11]\n",
    "    })\n",
    "    time_series1['date'] = stx.pd.to_datetime(time_series1['date'])\n",
    "    \n",
    "    time_series2 = stx.pd.DataFrame({\n",
    "        'subject_id': ['S001', 'S001', 'S002'],\n",
    "        'date': ['2024-01-01', '2024-01-04', '2024-01-02'],\n",
    "        'measure2': [20, 25, 22]\n",
    "    })\n",
    "    time_series2['date'] = stx.pd.to_datetime(time_series2['date'])\n",
    "    \n",
    "    time_merged = stx.pd.merge_asof_enhanced(\n",
    "        time_series1,\n",
    "        time_series2,\n",
    "        on='date',\n",
    "        by='subject_id',\n",
    "        tolerance='1D',  # Allow 1 day tolerance\n",
    "        direction='nearest',  # or 'backward', 'forward'\n",
    "        interpolate_missing=True,\n",
    "        add_time_diff=True  # Add column showing time difference\n",
    "    )\n",
    "    \n",
    "    # Multi-level merge for hierarchical data\n",
    "    hierarchical = stx.pd.merge_hierarchical(\n",
    "        {'level1': subjects, 'level2': measurements, 'level3': demographics},\n",
    "        hierarchy=['subject_id', 'timepoint'],\n",
    "        propagate_down=True,  # Propagate parent values to children\n",
    "        aggregate_up={'value': 'mean'},  # Aggregate child values to parent\n",
    "        maintain_structure=True\n",
    "    )\n",
    "    \n",
    "    # Merge with data quality checks\n",
    "    quality_merge = stx.pd.merge_with_quality(\n",
    "        subjects,\n",
    "        measurements,\n",
    "        on='subject_id',\n",
    "        quality_checks=[\n",
    "            'no_duplicate_keys',\n",
    "            'no_null_keys',\n",
    "            'consistent_types',\n",
    "            'value_ranges'\n",
    "        ],\n",
    "        report_issues=True,\n",
    "        clean_before_merge=True\n",
    "    )\n",
    "    \n",
    "    # Save merge report\n",
    "    merge_report = stx.pd.generate_merge_report(\n",
    "        original_dfs=[subjects, measurements, demographics],\n",
    "        merged_df=merged,\n",
    "        report_path='./reports/merge_analysis.html'\n",
    "    )\n",
    "    \n",
    "    return merged, time_merged, hierarchical\n",
    "'''\n",
    "\n",
    "print(\"ADVANCED MERGING WITH SCITEX:\")\n",
    "print(advanced_merging)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4: Statistical Operations and Hypothesis Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical operations with pandas\n",
    "statistical_pandas = '''\n",
    "import scitex as stx\n",
    "import numpy as np\n",
    "\n",
    "def pandas_statistical_operations():\n",
    "    \"\"\"Enhanced statistical operations with pandas.\"\"\"\n",
    "    # Generate experimental data\n",
    "    np.random.seed(42)\n",
    "    n_subjects = 100\n",
    "    \n",
    "    data = stx.pd.DataFrame({\n",
    "        'subject_id': [f'S{i:03d}' for i in range(n_subjects)],\n",
    "        'group': np.random.choice(['Control', 'Drug_A', 'Drug_B'], n_subjects),\n",
    "        'baseline': np.random.normal(100, 15, n_subjects),\n",
    "        'week_1': np.random.normal(105, 18, n_subjects),\n",
    "        'week_2': np.random.normal(110, 20, n_subjects),\n",
    "        'week_4': np.random.normal(115, 22, n_subjects),\n",
    "        'age': np.random.randint(20, 60, n_subjects),\n",
    "        'gender': np.random.choice(['M', 'F'], n_subjects)\n",
    "    })\n",
    "    \n",
    "    # Add group effects\n",
    "    data.loc[data['group'] == 'Drug_A', ['week_1', 'week_2', 'week_4']] += 10\n",
    "    data.loc[data['group'] == 'Drug_B', ['week_1', 'week_2', 'week_4']] += 15\n",
    "    \n",
    "    # Comprehensive statistical summary\n",
    "    stats_summary = stx.pd.statistical_summary(\n",
    "        data,\n",
    "        value_cols=['baseline', 'week_1', 'week_2', 'week_4'],\n",
    "        group_cols=['group', 'gender'],\n",
    "        statistics=[\n",
    "            'mean', 'std', 'sem', 'median', 'iqr',\n",
    "            'ci95', 'skew', 'kurtosis', 'shapiro_p'\n",
    "        ],\n",
    "        add_change_scores=True,  # Calculate change from baseline\n",
    "        add_effect_sizes=True    # Cohen's d, Hedge's g\n",
    "    )\n",
    "    \n",
    "    # Multiple hypothesis testing\n",
    "    hypothesis_results = stx.pd.hypothesis_testing(\n",
    "        data,\n",
    "        hypotheses=[\n",
    "            # Between-group comparisons\n",
    "            {'type': 'between', 'groups': ['Control', 'Drug_A'], \n",
    "             'variable': 'week_4', 'test': 'auto'},\n",
    "            \n",
    "            # Within-subject comparisons\n",
    "            {'type': 'within', 'timepoints': ['baseline', 'week_4'],\n",
    "             'by_group': True, 'test': 'paired_t'},\n",
    "            \n",
    "            # Correlation analysis\n",
    "            {'type': 'correlation', 'x': 'age', 'y': 'week_4',\n",
    "             'method': 'pearson', 'by_group': True},\n",
    "            \n",
    "            # ANOVA\n",
    "            {'type': 'anova', 'factor': 'group', 'dependent': 'week_4',\n",
    "             'covariates': ['age', 'baseline'], 'post_hoc': 'tukey'}\n",
    "        ],\n",
    "        alpha=0.05,\n",
    "        correction='fdr_bh',  # False discovery rate correction\n",
    "        report_format='comprehensive'\n",
    "    )\n",
    "    \n",
    "    # Longitudinal analysis\n",
    "    long_format = stx.pd.melt_enhanced(\n",
    "        data,\n",
    "        id_vars=['subject_id', 'group', 'age', 'gender'],\n",
    "        value_vars=['baseline', 'week_1', 'week_2', 'week_4'],\n",
    "        var_name='timepoint',\n",
    "        value_name='measurement',\n",
    "        add_numeric_time=True,  # Convert timepoint to numeric\n",
    "        time_mapping={'baseline': 0, 'week_1': 1, 'week_2': 2, 'week_4': 4}\n",
    "    )\n",
    "    \n",
    "    # Mixed effects model preparation\n",
    "    model_data = stx.pd.prepare_for_mixed_model(\n",
    "        long_format,\n",
    "        fixed_effects=['timepoint', 'group', 'age'],\n",
    "        random_effects=['subject_id'],\n",
    "        interactions=[('timepoint', 'group')],\n",
    "        center_predictors=True,\n",
    "        scale_predictors=True,\n",
    "        create_dummy_coding=True\n",
    "    )\n",
    "    \n",
    "    # Effect size calculations\n",
    "    effect_sizes = stx.pd.calculate_effect_sizes(\n",
    "        data,\n",
    "        group_col='group',\n",
    "        value_cols=['week_4'],\n",
    "        baseline_col='baseline',\n",
    "        effect_types=['cohens_d', 'hedges_g', 'glass_delta', 'odds_ratio'],\n",
    "        confidence_level=0.95,\n",
    "        bootstrap_ci=True,\n",
    "        n_bootstrap=1000\n",
    "    )\n",
    "    \n",
    "    # Power analysis for future studies\n",
    "    power_analysis = stx.pd.power_analysis_from_data(\n",
    "        data,\n",
    "        primary_outcome='week_4',\n",
    "        group_col='group',\n",
    "        target_power=0.80,\n",
    "        alpha=0.05,\n",
    "        include_dropouts=True,\n",
    "        dropout_rate=0.15\n",
    "    )\n",
    "    \n",
    "    # Generate statistical report\n",
    "    report = stx.pd.generate_statistical_report(\n",
    "        data=data,\n",
    "        stats_summary=stats_summary,\n",
    "        hypothesis_results=hypothesis_results,\n",
    "        effect_sizes=effect_sizes,\n",
    "        output_format='latex',  # or 'html', 'markdown'\n",
    "        include_figures=True,\n",
    "        save_path='./reports/statistical_analysis.tex'\n",
    "    )\n",
    "    \n",
    "    return stats_summary, hypothesis_results, effect_sizes\n",
    "'''\n",
    "\n",
    "print(\"STATISTICAL OPERATIONS WITH PANDAS:\")\n",
    "print(statistical_pandas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 5: Time Series and Panel Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series operations\n",
    "time_series_pandas = '''\n",
    "import scitex as stx\n",
    "import numpy as np\n",
    "\n",
    "def time_series_panel_operations():\n",
    "    \"\"\"Advanced time series and panel data operations.\"\"\"\n",
    "    # Generate time series data\n",
    "    dates = stx.pd.date_range('2024-01-01', periods=365, freq='D')\n",
    "    n_series = 5\n",
    "    \n",
    "    # Multi-variate time series\n",
    "    data = stx.pd.DataFrame({\n",
    "        'date': np.tile(dates, n_series),\n",
    "        'series_id': np.repeat([f'Series_{i}' for i in range(n_series)], len(dates)),\n",
    "        'value': np.concatenate([\n",
    "            100 + 10*np.sin(2*np.pi*np.arange(len(dates))/365) + \n",
    "            5*np.random.randn(len(dates)) + 20*i\n",
    "            for i in range(n_series)\n",
    "        ]),\n",
    "        'temperature': np.tile(20 + 10*np.sin(2*np.pi*np.arange(len(dates))/365) + \n",
    "                              2*np.random.randn(len(dates)), n_series),\n",
    "        'event': np.random.choice([0, 1], size=len(dates)*n_series, p=[0.95, 0.05])\n",
    "    })\n",
    "    \n",
    "    # Convert to panel data structure\n",
    "    panel = stx.pd.to_panel(\n",
    "        data,\n",
    "        index=['series_id', 'date'],\n",
    "        verify_integrity=True,\n",
    "        sort_index=True\n",
    "    )\n",
    "    \n",
    "    # Time series decomposition for each series\n",
    "    decomposition = stx.pd.time_series_decompose(\n",
    "        panel,\n",
    "        value_col='value',\n",
    "        method='stl',  # or 'x11', 'seats'\n",
    "        period='auto',  # Automatically detect seasonality\n",
    "        robust=True,\n",
    "        by_group='series_id'\n",
    "    )\n",
    "    \n",
    "    # Advanced rolling operations\n",
    "    rolling_stats = stx.pd.rolling_enhanced(\n",
    "        panel,\n",
    "        window='30D',  # 30-day window\n",
    "        operations={\n",
    "            'value': ['mean', 'std', 'skew', 'zscore'],\n",
    "            'temperature': ['mean', 'correlation:value']\n",
    "        },\n",
    "        min_periods=15,\n",
    "        center=True,\n",
    "        by_group='series_id'\n",
    "    )\n",
    "    \n",
    "    # Lag and lead features\n",
    "    lagged = stx.pd.create_lags_and_leads(\n",
    "        panel,\n",
    "        columns=['value', 'temperature'],\n",
    "        lags=[1, 7, 30],  # Previous day, week, month\n",
    "        leads=[1, 7],     # Next day, week\n",
    "        by_group='series_id',\n",
    "        fill_method='interpolate'\n",
    "    )\n",
    "    \n",
    "    # Change point detection\n",
    "    change_points = stx.pd.detect_change_points(\n",
    "        panel,\n",
    "        value_col='value',\n",
    "        method='pelt',  # or 'binary_segmentation', 'window_sliding'\n",
    "        penalty='bic',\n",
    "        min_segment_length=30,\n",
    "        by_group='series_id'\n",
    "    )\n",
    "    \n",
    "    # Anomaly detection\n",
    "    anomalies = stx.pd.detect_anomalies(\n",
    "        panel,\n",
    "        value_col='value',\n",
    "        methods=['isolation_forest', 'local_outlier_factor', 'seasonal_decompose'],\n",
    "        contamination='auto',  # Automatically estimate contamination\n",
    "        ensemble_method='voting',  # Combine multiple methods\n",
    "        by_group='series_id'\n",
    "    )\n",
    "    \n",
    "    # Cross-correlation analysis\n",
    "    cross_corr = stx.pd.cross_correlation(\n",
    "        panel,\n",
    "        x='value',\n",
    "        y='temperature',\n",
    "        max_lag=30,\n",
    "        method='pearson',\n",
    "        by_group='series_id',\n",
    "        plot=True\n",
    "    )\n",
    "    \n",
    "    # Granger causality testing\n",
    "    causality = stx.pd.granger_causality(\n",
    "        panel,\n",
    "        cause='temperature',\n",
    "        effect='value',\n",
    "        max_lag=10,\n",
    "        by_group='series_id',\n",
    "        include_instantaneous=True\n",
    "    )\n",
    "    \n",
    "    # Forecasting preparation\n",
    "    forecast_data = stx.pd.prepare_for_forecasting(\n",
    "        panel,\n",
    "        target='value',\n",
    "        features=['temperature', 'event'],\n",
    "        horizon=30,  # 30-day forecast\n",
    "        train_size=0.8,\n",
    "        gap=0,  # No gap between train and test\n",
    "        include_calendar_features=True,\n",
    "        include_fourier_features=True,\n",
    "        by_group='series_id'\n",
    "    )\n",
    "    \n",
    "    # Create time series visualization\n",
    "    fig = stx.pd.plot_time_series_panel(\n",
    "        panel,\n",
    "        value_col='value',\n",
    "        group_col='series_id',\n",
    "        show_decomposition=True,\n",
    "        show_anomalies=anomalies,\n",
    "        show_change_points=change_points,\n",
    "        facet_scales='free_y',\n",
    "        figsize=(15, 10)\n",
    "    )\n",
    "    \n",
    "    stx.io.save(fig, './figures/time_series_analysis.png',\n",
    "                symlink_from_cwd=True)\n",
    "    \n",
    "    return panel, decomposition, anomalies, forecast_data\n",
    "'''\n",
    "\n",
    "print(\"TIME SERIES AND PANEL DATA OPERATIONS:\")\n",
    "print(time_series_pandas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 6: DataFrame Export and Reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced export and reporting\n",
    "export_reporting = '''\n",
    "import scitex as stx\n",
    "\n",
    "def dataframe_export_reporting():\n",
    "    \"\"\"Advanced DataFrame export and reporting capabilities.\"\"\"\n",
    "    # Create sample analysis results\n",
    "    results = stx.pd.DataFrame({\n",
    "        'Model': ['Linear', 'Random Forest', 'XGBoost', 'Neural Net'],\n",
    "        'Accuracy': [0.85, 0.92, 0.94, 0.93],\n",
    "        'Precision': [0.83, 0.91, 0.93, 0.92],\n",
    "        'Recall': [0.87, 0.93, 0.95, 0.94],\n",
    "        'F1_Score': [0.85, 0.92, 0.94, 0.93],\n",
    "        'Training_Time': [0.5, 12.3, 8.7, 45.2],\n",
    "        'Parameters': [10, 500, 300, 1000]\n",
    "    })\n",
    "    \n",
    "    # Export to multiple formats with formatting\n",
    "    stx.pd.export_formatted(\n",
    "        results,\n",
    "        base_path='./results/model_comparison',\n",
    "        formats=['excel', 'latex', 'html', 'markdown'],\n",
    "        \n",
    "        # Excel-specific options\n",
    "        excel_options={\n",
    "            'sheet_name': 'Model Results',\n",
    "            'index': False,\n",
    "            'freeze_panes': (1, 1),\n",
    "            'conditional_format': {\n",
    "                'Accuracy': {'type': 'data_bar', 'color': 'green'},\n",
    "                'Training_Time': {'type': 'color_scale', 'palette': 'RdYlGn_r'}\n",
    "            },\n",
    "            'column_widths': 'auto',\n",
    "            'add_chart': True\n",
    "        },\n",
    "        \n",
    "        # LaTeX-specific options\n",
    "        latex_options={\n",
    "            'caption': 'Model Performance Comparison',\n",
    "            'label': 'tab:model_comparison',\n",
    "            'column_format': 'l|rrrr|rr',\n",
    "            'bold_max': True,  # Bold best values\n",
    "            'escape': False,\n",
    "            'position': 'htbp'\n",
    "        },\n",
    "        \n",
    "        # HTML-specific options\n",
    "        html_options={\n",
    "            'table_id': 'model-comparison',\n",
    "            'classes': 'table table-striped table-hover',\n",
    "            'include_css': True,\n",
    "            'sortable': True,\n",
    "            'searchable': True\n",
    "        },\n",
    "        \n",
    "        # Markdown-specific options\n",
    "        markdown_options={\n",
    "            'tablefmt': 'github',\n",
    "            'floatfmt': '.3f',\n",
    "            'numalign': 'right'\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Create comprehensive report\n",
    "    report = stx.pd.create_analysis_report(\n",
    "        title='Model Comparison Analysis',\n",
    "        sections=[\n",
    "            {\n",
    "                'title': 'Executive Summary',\n",
    "                'type': 'text',\n",
    "                'content': 'XGBoost showed the best overall performance...'\n",
    "            },\n",
    "            {\n",
    "                'title': 'Performance Metrics',\n",
    "                'type': 'dataframe',\n",
    "                'data': results,\n",
    "                'styling': {\n",
    "                    'highlight_max': ['Accuracy', 'F1_Score'],\n",
    "                    'highlight_min': ['Training_Time'],\n",
    "                    'precision': 3\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                'title': 'Performance Visualization',\n",
    "                'type': 'plot',\n",
    "                'plot_type': 'radar',\n",
    "                'data': results[['Model', 'Accuracy', 'Precision', 'Recall', 'F1_Score']]\n",
    "            },\n",
    "            {\n",
    "                'title': 'Statistical Comparison',\n",
    "                'type': 'statistics',\n",
    "                'tests': [\n",
    "                    {'type': 'friedman', 'metrics': ['Accuracy', 'F1_Score']},\n",
    "                    {'type': 'nemenyi', 'metric': 'F1_Score'}\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        output_format='html',  # or 'pdf', 'docx'\n",
    "        template='academic',   # or 'business', 'technical'\n",
    "        include_code=True,\n",
    "        include_data=True,\n",
    "        save_path='./reports/model_comparison_report.html'\n",
    "    )\n",
    "    \n",
    "    # Create interactive dashboard\n",
    "    dashboard = stx.pd.create_dashboard(\n",
    "        data={'results': results},\n",
    "        layout=[\n",
    "            {'type': 'metric_cards', 'metrics': ['Best Accuracy', 'Fastest Model']},\n",
    "            {'type': 'comparison_table', 'data': 'results'},\n",
    "            {'type': 'bar_chart', 'x': 'Model', 'y': ['Accuracy', 'F1_Score']},\n",
    "            {'type': 'scatter', 'x': 'Training_Time', 'y': 'Accuracy', 'size': 'Parameters'}\n",
    "        ],\n",
    "        filters=['Model'],\n",
    "        export_options=['png', 'pdf', 'pptx'],\n",
    "        save_path='./dashboards/model_comparison.html'\n",
    "    )\n",
    "    \n",
    "    # Generate presentation slides\n",
    "    slides = stx.pd.create_presentation(\n",
    "        title='Model Performance Analysis',\n",
    "        data=results,\n",
    "        slide_templates=[\n",
    "            'title_slide',\n",
    "            'overview_table',\n",
    "            'comparison_chart',\n",
    "            'winner_announcement',\n",
    "            'recommendations'\n",
    "        ],\n",
    "        theme='corporate',\n",
    "        save_path='./presentations/model_comparison.pptx'\n",
    "    )\n",
    "    \n",
    "    return report, dashboard\n",
    "'''\n",
    "\n",
    "print(\"DATAFRAME EXPORT AND REPORTING:\")\n",
    "print(export_reporting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The SciTeX PD Server provides comprehensive enhancements to pandas:\n",
    "\n",
    "### 1. **Smart Data Handling**\n",
    "   - Automatic type inference and optimization\n",
    "   - Intelligent missing value handling\n",
    "   - Outlier detection and treatment\n",
    "   - Data validation with rules\n",
    "\n",
    "### 2. **Enhanced Operations**\n",
    "   - Smart merging with conflict resolution\n",
    "   - Time-aware joins\n",
    "   - Hierarchical data support\n",
    "   - Advanced groupby operations\n",
    "\n",
    "### 3. **Statistical Integration**\n",
    "   - Built-in hypothesis testing\n",
    "   - Effect size calculations\n",
    "   - Power analysis\n",
    "   - Multiple comparison corrections\n",
    "\n",
    "### 4. **Time Series Features**\n",
    "   - Panel data structures\n",
    "   - Decomposition and seasonality\n",
    "   - Anomaly detection\n",
    "   - Forecasting preparation\n",
    "\n",
    "### 5. **Export and Reporting**\n",
    "   - Multi-format export with styling\n",
    "   - Automated report generation\n",
    "   - Interactive dashboards\n",
    "   - Presentation creation\n",
    "\n",
    "### 6. **Data Quality**\n",
    "   - Comprehensive cleaning pipelines\n",
    "   - Validation and quality reports\n",
    "   - Feature engineering\n",
    "   - Data profiling\n",
    "\n",
    "## Key Benefits\n",
    "\n",
    "- **Reduced Code**: Common operations require fewer lines\n",
    "- **Better Defaults**: Intelligent parameter selection\n",
    "- **Integrated Workflow**: Statistical analysis built-in\n",
    "- **Production Ready**: Validation and quality checks\n",
    "- **Reproducible**: All operations tracked and logged"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}