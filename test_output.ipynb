{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef3c8156",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">An Exception was encountered at '<a href=\"#papermill-error-cell\">In [8]</a>'.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa987c98",
   "metadata": {
    "papermill": {
     "duration": 0.006797,
     "end_time": "2025-07-04T08:31:24.719934",
     "exception": false,
     "start_time": "2025-07-04T08:31:24.713137",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# SciTeX I/O Operations Tutorial\n",
    "\n",
    "This comprehensive notebook demonstrates the SciTeX I/O module capabilities, combining features from basic operations, advanced functionality, and complete workflow examples.\n",
    "\n",
    "## Features Covered\n",
    "\n",
    "### Basic I/O Operations\n",
    "* Unified save/load interface with automatic format detection\n",
    "* Symlink creation and management\n",
    "* Basic file operations\n",
    "\n",
    "### Advanced I/O Features\n",
    "* Compression support (gzip, bz2, xz)\n",
    "* HDF5 operations\n",
    "* Configuration file management\n",
    "* Performance comparisons across formats\n",
    "\n",
    "### Complete Workflows\n",
    "* Caching mechanisms\n",
    "* Batch operations\n",
    "* Experiment pipeline integration\n",
    "* Real-world data processing examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29a48cda",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-04T08:31:24.734739Z",
     "iopub.status.busy": "2025-07-04T08:31:24.734053Z",
     "iopub.status.idle": "2025-07-04T08:31:24.745061Z",
     "shell.execute_reply": "2025-07-04T08:31:24.742673Z"
    },
    "papermill": {
     "duration": 0.02124,
     "end_time": "2025-07-04T08:31:24.747462",
     "exception": false,
     "start_time": "2025-07-04T08:31:24.726222",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Detect notebook name for output directory\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Get notebook name (for papermill compatibility)\n",
    "notebook_name = \"01_scitex_io\"\n",
    "if 'PAPERMILL_NOTEBOOK_NAME' in os.environ:\n",
    "    notebook_name = Path(os.environ['PAPERMILL_NOTEBOOK_NAME']).stem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6512ce3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-04T08:31:24.766715Z",
     "iopub.status.busy": "2025-07-04T08:31:24.765714Z",
     "iopub.status.idle": "2025-07-04T08:31:24.779937Z",
     "shell.execute_reply": "2025-07-04T08:31:24.777752Z"
    },
    "papermill": {
     "duration": 0.027325,
     "end_time": "2025-07-04T08:31:24.782518",
     "exception": false,
     "start_time": "2025-07-04T08:31:24.755193",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SciTeX I/O Tutorial - Ready to begin!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "import scitex\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# Set up example data directory\n",
    "data_dir = Path(\"./io_examples\")\n",
    "# Ensure output directory exists\n",
    "import os\n",
    "output_dir = Path(\"io_examples\")\n",
    "actual_dir = Path(\"01_scitex_io_out/io_examples\")\n",
    "\n",
    "if actual_dir.exists() and not output_dir.exists():\n",
    "    # Create symlink for backward compatibility\n",
    "    os.symlink(str(actual_dir), str(output_dir))\n",
    "    print(f\"Created symlink: {output_dir} -> {actual_dir}\")\n",
    "\n",
    "data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"SciTeX I/O Tutorial - Ready to begin!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8112a293",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-04T08:31:24.800989Z",
     "iopub.status.busy": "2025-07-04T08:31:24.799906Z",
     "iopub.status.idle": "2025-07-04T08:31:24.813133Z",
     "shell.execute_reply": "2025-07-04T08:31:24.810567Z"
    },
    "papermill": {
     "duration": 0.026951,
     "end_time": "2025-07-04T08:31:24.817616",
     "exception": false,
     "start_time": "2025-07-04T08:31:24.790665",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Path compatibility helper\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def ensure_output_dir(subdir: str, notebook_name: str = \"01_scitex_io\"):\n",
    "    \"\"\"Ensure output directory exists with backward compatibility.\"\"\"\n",
    "    expected_dir = Path(subdir)\n",
    "    actual_dir = Path(f\"{notebook_name}_out\") / subdir\n",
    "    \n",
    "    if not expected_dir.exists() and actual_dir.exists():\n",
    "        # Create symlink for backward compatibility\n",
    "        try:\n",
    "            os.symlink(str(actual_dir.resolve()), str(expected_dir))\n",
    "            print(f\"Created symlink: {expected_dir} -> {actual_dir}\")\n",
    "        except (OSError, FileExistsError):\n",
    "            pass\n",
    "    \n",
    "    return expected_dir\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810b7c8f",
   "metadata": {
    "papermill": {
     "duration": 0.011231,
     "end_time": "2025-07-04T08:31:24.839010",
     "exception": false,
     "start_time": "2025-07-04T08:31:24.827779",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Part 1: Basic I/O Operations\n",
    "\n",
    "### 1.1 Unified Save/Load Interface\n",
    "\n",
    "SciTeX provides a unified interface that automatically detects file formats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8869ac51",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-04T08:31:24.862696Z",
     "iopub.status.busy": "2025-07-04T08:31:24.861665Z",
     "iopub.status.idle": "2025-07-04T08:31:24.877404Z",
     "shell.execute_reply": "2025-07-04T08:31:24.875048Z"
    },
    "papermill": {
     "duration": 0.02966,
     "end_time": "2025-07-04T08:31:24.880213",
     "exception": false,
     "start_time": "2025-07-04T08:31:24.850553",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data created:\n",
      "- Array shape: (100, 50)\n",
      "- DataFrame shape: (1000, 3)\n",
      "- Metadata keys: ['experiment', 'date', 'parameters']\n"
     ]
    }
   ],
   "source": [
    "# Create sample data\n",
    "sample_data = {\n",
    "    'array': np.random.randn(100, 50),\n",
    "    'dataframe': pd.DataFrame({\n",
    "        'x': np.random.randn(1000),\n",
    "        'y': np.random.randn(1000),\n",
    "        'category': np.random.choice(['A', 'B', 'C'], 1000)\n",
    "    }),\n",
    "    'metadata': {\n",
    "        'experiment': 'demo',\n",
    "        'date': '2024-01-01',\n",
    "        'parameters': {'alpha': 0.05, 'beta': 0.1}\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"Sample data created:\")\n",
    "print(f\"- Array shape: {sample_data['array'].shape}\")\n",
    "print(f\"- DataFrame shape: {sample_data['dataframe'].shape}\")\n",
    "print(f\"- Metadata keys: {list(sample_data['metadata'].keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a03b6d56",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-04T08:31:24.895345Z",
     "iopub.status.busy": "2025-07-04T08:31:24.894257Z",
     "iopub.status.idle": "2025-07-04T08:31:25.223029Z",
     "shell.execute_reply": "2025-07-04T08:31:25.221151Z"
    },
    "papermill": {
     "duration": 0.338952,
     "end_time": "2025-07-04T08:31:25.225630",
     "exception": false,
     "start_time": "2025-07-04T08:31:24.886678",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m\n",
      "Saved to: /home/ywatanabe/proj/SciTeX-Code/test_output_out/io_examples/sample_data.pkl (57.5 KiB)\u001b[0m\n",
      "✓ Saved data in PKL format\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Error occurred while saving: Object of type ndarray is not JSON serializable\n",
      "Debug: Initial script_path = /tmp/ipykernel_1374938/3137666053.py\n",
      "Debug: Final spath = /home/ywatanabe/proj/SciTeX-Code/test_output_out/io_examples/sample_data.json\n",
      "Debug: specified_path type = <class 'str'>\n",
      "Debug: specified_path = io_examples/sample_data.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved data in JSON format\n",
      "\u001b[93m\n",
      "Saved to: /home/ywatanabe/proj/SciTeX-Code/test_output_out/io_examples/sample_array.npy (39.2 KiB)\u001b[0m\n",
      "✓ Saved data in NPY format\n",
      "\u001b[93m\n",
      "Saved to: /home/ywatanabe/proj/SciTeX-Code/test_output_out/io_examples/sample_dataframe.csv (40.3 KiB)\u001b[0m\n",
      "✓ Saved data in CSV format\n"
     ]
    }
   ],
   "source": [
    "# Save data in multiple formats - automatic format detection\n",
    "formats_to_test = ['pkl', 'json', 'npy', 'csv']\n",
    "\n",
    "for fmt in formats_to_test:\n",
    "    try:\n",
    "        if fmt == 'npy':\n",
    "            # For .npy, save just the array\n",
    "            scitex.io.save(sample_data['array'], data_dir / f\"sample_array.{fmt}\")\n",
    "        elif fmt == 'csv':\n",
    "            # For .csv, save just the dataframe\n",
    "            scitex.io.save(sample_data['dataframe'], data_dir / f\"sample_dataframe.{fmt}\")\n",
    "        else:\n",
    "            # For pkl and json, save the full dictionary\n",
    "            scitex.io.save(sample_data, data_dir / f\"sample_data.{fmt}\")\n",
    "        print(f\"✓ Saved data in {fmt.upper()} format\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Failed to save in {fmt.upper()} format: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "076821fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-04T08:31:25.242132Z",
     "iopub.status.busy": "2025-07-04T08:31:25.241000Z",
     "iopub.status.idle": "2025-07-04T08:31:25.253966Z",
     "shell.execute_reply": "2025-07-04T08:31:25.251377Z"
    },
    "papermill": {
     "duration": 0.025766,
     "end_time": "2025-07-04T08:31:25.257720",
     "exception": false,
     "start_time": "2025-07-04T08:31:25.231954",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load data back - automatic format detection\n",
    "loaded_data = {}\n",
    "\n",
    "# Load pickle data (full dictionary)\n",
    "if (data_dir / \"sample_data.pkl\").exists():\n",
    "    loaded_data['from_pkl'] = scitex.io.load(data_dir / \"sample_data.pkl\")\n",
    "    print(\"✓ Loaded data from pickle\")\n",
    "\n",
    "# Load numpy array\n",
    "if (data_dir / \"sample_array.npy\").exists():\n",
    "    loaded_data['from_npy'] = scitex.io.load(data_dir / \"sample_array.npy\")\n",
    "    print(f\"✓ Loaded array from npy: shape {loaded_data['from_npy'].shape}\")\n",
    "\n",
    "# Load CSV dataframe\n",
    "if (data_dir / \"sample_dataframe.csv\").exists():\n",
    "    loaded_data['from_csv'] = scitex.io.load(data_dir / \"sample_dataframe.csv\")\n",
    "    print(f\"✓ Loaded dataframe from csv: shape {loaded_data['from_csv'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edaa18a",
   "metadata": {
    "papermill": {
     "duration": 0.007207,
     "end_time": "2025-07-04T08:31:25.274212",
     "exception": false,
     "start_time": "2025-07-04T08:31:25.267005",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 1.2 Symlink Creation and Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f6462a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-04T08:31:25.289335Z",
     "iopub.status.busy": "2025-07-04T08:31:25.288173Z",
     "iopub.status.idle": "2025-07-04T08:31:25.300395Z",
     "shell.execute_reply": "2025-07-04T08:31:25.298131Z"
    },
    "papermill": {
     "duration": 0.022616,
     "end_time": "2025-07-04T08:31:25.303080",
     "exception": false,
     "start_time": "2025-07-04T08:31:25.280464",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original file not found for symlink creation\n"
     ]
    }
   ],
   "source": [
    "# Create symlinks for easy access\n",
    "symlink_dir = data_dir / \"symlinks\"\n",
    "symlink_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Create symlinks to our saved files\n",
    "original_file = data_dir / \"sample_data.pkl\"\n",
    "if original_file.exists():\n",
    "    symlink_path = symlink_dir / \"latest_data.pkl\"\n",
    "    \n",
    "    # Remove existing symlink if it exists\n",
    "    if symlink_path.is_symlink():\n",
    "        symlink_path.unlink()\n",
    "    \n",
    "    # Create new symlink\n",
    "    symlink_path.symlink_to(original_file.resolve())\n",
    "    print(f\"✓ Created symlink: {symlink_path} -> {original_file}\")\n",
    "    \n",
    "    # Verify symlink works\n",
    "    symlink_data = scitex.io.load(symlink_path)\n",
    "    print(f\"✓ Successfully loaded data through symlink\")\n",
    "    print(f\"  Array shape: {symlink_data['array'].shape}\")\n",
    "else:\n",
    "    print(\"Original file not found for symlink creation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e04f569",
   "metadata": {
    "papermill": {
     "duration": 0.008184,
     "end_time": "2025-07-04T08:31:25.319781",
     "exception": false,
     "start_time": "2025-07-04T08:31:25.311597",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Part 2: Advanced I/O Features\n",
    "\n",
    "### 2.1 Compression Support"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8607add3",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span id=\"papermill-error-cell\" style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">Execution using papermill encountered an exception here and stopped:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "470a308f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-04T08:31:25.336316Z",
     "iopub.status.busy": "2025-07-04T08:31:25.335661Z",
     "iopub.status.idle": "2025-07-04T08:31:26.194921Z",
     "shell.execute_reply": "2025-07-04T08:31:26.192233Z"
    },
    "papermill": {
     "duration": 0.870864,
     "end_time": "2025-07-04T08:31:26.197789",
     "exception": true,
     "start_time": "2025-07-04T08:31:25.326925",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m\n",
      "Saved to: /home/ywatanabe/proj/SciTeX-Code/test_output_out/io_examples/large_data.pkl (7.7 MiB)\u001b[0m\n",
      "Warning: Could not find io_examples/large_data.pkl\n",
      "✓ Saved with gzip compression\n",
      "✓ Saved with bz2 compression\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ywatanabe/proj/SciTeX-Code/src/scitex/io/_save.py:415: UserWarning: Unsupported file format. /home/ywatanabe/proj/SciTeX-Code/test_output_out/io_examples/large_data.pkl.gzip was not saved.\n",
      "  warnings.warn(f\"Unsupported file format. {spath} was not saved.\")\n",
      "/home/ywatanabe/proj/SciTeX-Code/src/scitex/io/_save.py:415: UserWarning: Unsupported file format. /home/ywatanabe/proj/SciTeX-Code/test_output_out/io_examples/large_data.pkl.bz2 was not saved.\n",
      "  warnings.warn(f\"Unsupported file format. {spath} was not saved.\")\n",
      "/home/ywatanabe/proj/SciTeX-Code/src/scitex/io/_save.py:415: UserWarning: Unsupported file format. /home/ywatanabe/proj/SciTeX-Code/test_output_out/io_examples/large_data.pkl.xz was not saved.\n",
      "  warnings.warn(f\"Unsupported file format. {spath} was not saved.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved with xz compression\n",
      "\n",
      "File size comparison:\n",
      "uncompressed: 0.00 MB\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mZeroDivisionError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 49\u001b[39m\n\u001b[32m     47\u001b[39m size_mb = size / (\u001b[32m1024\u001b[39m * \u001b[32m1024\u001b[39m)\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m format_name != \u001b[33m'\u001b[39m\u001b[33muncompressed\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m     compression_ratio = \u001b[43mfile_sizes\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43muncompressed\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\n\u001b[32m     50\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mformat_name\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m12\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize_mb\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m MB (compression ratio: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcompression_ratio\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mx)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mZeroDivisionError\u001b[39m: division by zero"
     ]
    }
   ],
   "source": [
    "# Test compression formats\n",
    "compression_formats = ['gzip', 'bz2', 'xz']\n",
    "large_data = {\n",
    "    'large_array': np.random.randn(1000, 1000),\n",
    "    'text_data': 'This is a test string that will be repeated many times. ' * 1000\n",
    "}\n",
    "\n",
    "file_sizes = {}\n",
    "\n",
    "# Save uncompressed\n",
    "uncompressed_file = data_dir / \"large_data.pkl\"\n",
    "scitex.io.save(large_data, uncompressed_file)\n",
    "# Wait for file to be written and get actual path\n",
    "import time\n",
    "time.sleep(0.1)  # Brief pause to ensure file is written\n",
    "# The file is saved to notebook_out directory\n",
    "actual_path = Path(f'{Path().name}_out') / uncompressed_file\n",
    "if actual_path.exists():\n",
    "    file_sizes['uncompressed'] = actual_path.stat().st_size\n",
    "else:\n",
    "    # Fallback: check if it's in current directory\n",
    "    if uncompressed_file.exists():\n",
    "        file_sizes['uncompressed'] = uncompressed_file.stat().st_size\n",
    "    else:\n",
    "        print(f'Warning: Could not find {uncompressed_file}')\n",
    "        file_sizes['uncompressed'] = 0\n",
    "\n",
    "# Save with compression\n",
    "for compression in compression_formats:\n",
    "    try:\n",
    "        compressed_file = data_dir / f\"large_data.pkl.{compression}\"\n",
    "        scitex.io.save(large_data, compressed_file, compression=compression)\n",
    "        actual_path = Path(f'{Path().name}_out') / compressed_file\n",
    "        if actual_path.exists():\n",
    "            file_sizes[compression] = actual_path.stat().st_size\n",
    "        elif compressed_file.exists():\n",
    "            file_sizes[compression] = compressed_file.stat().st_size\n",
    "        else:\n",
    "            file_sizes[compression] = 0\n",
    "        print(f\"✓ Saved with {compression} compression\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Failed to save with {compression}: {e}\")\n",
    "\n",
    "# Compare file sizes\n",
    "print(\"\\nFile size comparison:\")\n",
    "for format_name, size in file_sizes.items():\n",
    "    size_mb = size / (1024 * 1024)\n",
    "    if format_name != 'uncompressed':\n",
    "        compression_ratio = file_sizes['uncompressed'] / size\n",
    "        print(f\"{format_name:12}: {size_mb:.2f} MB (compression ratio: {compression_ratio:.1f}x)\")\n",
    "    else:\n",
    "        print(f\"{format_name:12}: {size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b50895",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 2.2 HDF5 Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73226a73",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# HDF5 operations for hierarchical data\n",
    "try:\n",
    "    import h5py\n",
    "    \n",
    "    # Create hierarchical data structure\n",
    "    hdf5_data = {\n",
    "        'experiment_1': {\n",
    "            'raw_data': np.random.randn(500, 100),\n",
    "            'processed_data': np.random.randn(500, 50),\n",
    "            'metadata': {\n",
    "                'sampling_rate': 1000,\n",
    "                'channels': 100\n",
    "            }\n",
    "        },\n",
    "        'experiment_2': {\n",
    "            'raw_data': np.random.randn(300, 100),\n",
    "            'processed_data': np.random.randn(300, 50),\n",
    "            'metadata': {\n",
    "                'sampling_rate': 500,\n",
    "                'channels': 100\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save as HDF5\n",
    "    hdf5_file = data_dir / \"experiments.h5\"\n",
    "    scitex.io.save(hdf5_data, hdf5_file)\n",
    "    print(f\"✓ Saved hierarchical data to HDF5: {hdf5_file}\")\n",
    "    \n",
    "    # Load HDF5 data\n",
    "    loaded_hdf5 = scitex.io.load(hdf5_file)\n",
    "    print(f\"✓ Loaded HDF5 data with {len(loaded_hdf5)} experiments\")\n",
    "    \n",
    "    for exp_name, exp_data in loaded_hdf5.items():\n",
    "        print(f\"  {exp_name}: raw_data shape {exp_data['raw_data'].shape}\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"h5py not available - skipping HDF5 examples\")\n",
    "except Exception as e:\n",
    "    print(f\"HDF5 operations failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0a731a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 2.3 Performance Comparison Across Formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cff454",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Performance benchmark for different formats\n",
    "benchmark_data = {\n",
    "    'numeric_array': np.random.randn(1000, 100),\n",
    "    'dataframe': pd.DataFrame({\n",
    "        'col_' + str(i): np.random.randn(5000) \n",
    "        for i in range(20)\n",
    "    }),\n",
    "    'mixed_data': {\n",
    "        'numbers': list(range(10000)),\n",
    "        'strings': [f'item_{i}' for i in range(1000)],\n",
    "        'nested': {'a': [1, 2, 3], 'b': {'c': 4, 'd': 5}}\n",
    "    }\n",
    "}\n",
    "\n",
    "formats_to_benchmark = ['pkl', 'json', 'h5']\n",
    "benchmark_results = {}\n",
    "\n",
    "for fmt in formats_to_benchmark:\n",
    "    try:\n",
    "        test_file = data_dir / f\"benchmark.{fmt}\"\n",
    "        \n",
    "        # Time save operation\n",
    "        start_time = time.time()\n",
    "        if fmt == 'json':\n",
    "            # JSON can't handle numpy arrays directly\n",
    "            json_safe_data = {\n",
    "                'numeric_array': benchmark_data['numeric_array'].tolist(),\n",
    "                'mixed_data': benchmark_data['mixed_data']\n",
    "            }\n",
    "            scitex.io.save(json_safe_data, test_file)\n",
    "        else:\n",
    "            scitex.io.save(benchmark_data, test_file)\n",
    "        save_time = time.time() - start_time\n",
    "        \n",
    "        # Time load operation\n",
    "        start_time = time.time()\n",
    "        loaded = scitex.io.load(test_file)\n",
    "        load_time = time.time() - start_time\n",
    "        \n",
    "        # Get file size\n",
    "        file_size = test_file.stat().st_size / (1024 * 1024)  # MB\n",
    "        \n",
    "        benchmark_results[fmt] = {\n",
    "            'save_time': save_time,\n",
    "            'load_time': load_time,\n",
    "            'file_size_mb': file_size\n",
    "        }\n",
    "        \n",
    "        print(f\"✓ {fmt.upper()}: Save {save_time:.3f}s, Load {load_time:.3f}s, Size {file_size:.2f}MB\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ {fmt.upper()} benchmark failed: {e}\")\n",
    "\n",
    "# Visualize benchmark results\n",
    "if benchmark_results:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    formats = list(benchmark_results.keys())\n",
    "    save_times = [benchmark_results[fmt]['save_time'] for fmt in formats]\n",
    "    load_times = [benchmark_results[fmt]['load_time'] for fmt in formats]\n",
    "    file_sizes = [benchmark_results[fmt]['file_size_mb'] for fmt in formats]\n",
    "    \n",
    "    axes[0].bar(formats, save_times)\n",
    "    axes[0].set_title('Save Time (seconds)')\n",
    "    axes[0].set_ylabel('Time (s)')\n",
    "    \n",
    "    axes[1].bar(formats, load_times)\n",
    "    axes[1].set_title('Load Time (seconds)')\n",
    "    axes[1].set_ylabel('Time (s)')\n",
    "    \n",
    "    axes[2].bar(formats, file_sizes)\n",
    "    axes[2].set_title('File Size (MB)')\n",
    "    axes[2].set_ylabel('Size (MB)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01947856",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Part 3: Complete Workflows and Caching\n",
    "\n",
    "### 3.1 Caching Mechanisms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280df44a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Demonstrate caching for expensive operations\n",
    "cache_dir = data_dir / \"cache\"\n",
    "cache_dir.mkdir(exist_ok=True)\n",
    "\n",
    "@scitex.io.cache_result(cache_dir / \"expensive_computation.pkl\")\n",
    "def expensive_computation(n_samples=10000, n_features=100):\n",
    "    \"\"\"Simulate an expensive computation that we want to cache.\"\"\"\n",
    "    print(f\"Performing expensive computation with {n_samples} samples...\")\n",
    "    time.sleep(1)  # Simulate computation time\n",
    "    \n",
    "    # Generate some \"computed\" result\n",
    "    data = np.random.randn(n_samples, n_features)\n",
    "    features = np.mean(data, axis=0)\n",
    "    correlations = np.corrcoef(data.T)\n",
    "    \n",
    "    return {\n",
    "        'raw_data': data,\n",
    "        'features': features,\n",
    "        'correlations': correlations,\n",
    "        'metadata': {\n",
    "            'n_samples': n_samples,\n",
    "            'n_features': n_features,\n",
    "            'computed_at': time.time()\n",
    "        }\n",
    "    }\n",
    "\n",
    "# First call - will compute and cache\n",
    "print(\"First call (will compute):\")\n",
    "start_time = time.time()\n",
    "result1 = expensive_computation(5000, 50)\n",
    "first_call_time = time.time() - start_time\n",
    "print(f\"First call took {first_call_time:.2f} seconds\")\n",
    "\n",
    "# Second call - will load from cache\n",
    "print(\"\\nSecond call (will load from cache):\")\n",
    "start_time = time.time()\n",
    "result2 = expensive_computation(5000, 50)\n",
    "second_call_time = time.time() - start_time\n",
    "print(f\"Second call took {second_call_time:.2f} seconds\")\n",
    "\n",
    "print(f\"\\nSpeedup from caching: {first_call_time/second_call_time:.1f}x\")\n",
    "print(f\"Results identical: {np.array_equal(result1['features'], result2['features'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b01f8bc",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 3.2 Batch Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6152136c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Batch file operations\n",
    "batch_dir = data_dir / \"batch_processing\"\n",
    "batch_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Create multiple data files for batch processing\n",
    "batch_files = []\n",
    "for i in range(5):\n",
    "    batch_data = {\n",
    "        'id': i,\n",
    "        'data': np.random.randn(100, 10),\n",
    "        'labels': np.random.choice(['A', 'B', 'C'], 100),\n",
    "        'timestamp': time.time() + i\n",
    "    }\n",
    "    \n",
    "    filename = batch_dir / f\"batch_data_{i:03d}.pkl\"\n",
    "    scitex.io.save(batch_data, filename)\n",
    "    batch_files.append(filename)\n",
    "\n",
    "print(f\"Created {len(batch_files)} batch files\")\n",
    "\n",
    "# Batch loading with pattern matching\n",
    "pattern = batch_dir / \"batch_data_*.pkl\"\n",
    "all_batch_files = list(batch_dir.glob(\"batch_data_*.pkl\"))\n",
    "print(f\"Found {len(all_batch_files)} files matching pattern\")\n",
    "\n",
    "# Load and combine all batch files\n",
    "combined_data = []\n",
    "for file_path in sorted(all_batch_files):\n",
    "    data = scitex.io.load(file_path)\n",
    "    combined_data.append(data)\n",
    "\n",
    "print(f\"Loaded {len(combined_data)} batch files\")\n",
    "print(f\"Total data points: {sum(len(d['data']) for d in combined_data)}\")\n",
    "\n",
    "# Combine all data into single arrays\n",
    "all_data = np.vstack([d['data'] for d in combined_data])\n",
    "all_labels = np.hstack([d['labels'] for d in combined_data])\n",
    "\n",
    "print(f\"Combined data shape: {all_data.shape}\")\n",
    "print(f\"Label distribution: {dict(zip(*np.unique(all_labels, return_counts=True)))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8dd699e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 3.3 Experiment Pipeline Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e18a11",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Complete experiment pipeline with I/O\n",
    "class ExperimentPipeline:\n",
    "    def __init__(self, experiment_name, output_dir):\n",
    "        self.experiment_name = experiment_name\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.output_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Create subdirectories\n",
    "        (self.output_dir / \"raw\").mkdir(exist_ok=True)\n",
    "        (self.output_dir / \"processed\").mkdir(exist_ok=True)\n",
    "        (self.output_dir / \"results\").mkdir(exist_ok=True)\n",
    "        \n",
    "    def generate_data(self, n_samples=1000, noise_level=0.1):\n",
    "        \"\"\"Generate synthetic experimental data.\"\"\"\n",
    "        print(f\"Generating data for {self.experiment_name}...\")\n",
    "        \n",
    "        # Simulate different experimental conditions\n",
    "        conditions = ['control', 'treatment_A', 'treatment_B']\n",
    "        raw_data = {}\n",
    "        \n",
    "        for condition in conditions:\n",
    "            # Different signal patterns for each condition\n",
    "            if condition == 'control':\n",
    "                signal = np.sin(np.linspace(0, 4*np.pi, n_samples))\n",
    "            elif condition == 'treatment_A':\n",
    "                signal = np.sin(np.linspace(0, 4*np.pi, n_samples)) * 1.5\n",
    "            else:  # treatment_B\n",
    "                signal = np.sin(np.linspace(0, 6*np.pi, n_samples)) * 0.8\n",
    "            \n",
    "            # Add noise\n",
    "            noisy_signal = signal + np.random.normal(0, noise_level, n_samples)\n",
    "            \n",
    "            raw_data[condition] = {\n",
    "                'signal': noisy_signal,\n",
    "                'time': np.linspace(0, 10, n_samples),\n",
    "                'metadata': {\n",
    "                    'condition': condition,\n",
    "                    'n_samples': n_samples,\n",
    "                    'noise_level': noise_level\n",
    "                }\n",
    "            }\n",
    "        \n",
    "        # Save raw data\n",
    "        raw_file = self.output_dir / \"raw\" / \"raw_data.pkl\"\n",
    "        scitex.io.save(raw_data, raw_file)\n",
    "        print(f\"✓ Raw data saved to {raw_file}\")\n",
    "        \n",
    "        return raw_data\n",
    "    \n",
    "    def process_data(self, raw_data=None):\n",
    "        \"\"\"Process the raw experimental data.\"\"\"\n",
    "        if raw_data is None:\n",
    "            # Load from file\n",
    "            raw_file = self.output_dir / \"raw\" / \"raw_data.pkl\"\n",
    "            raw_data = scitex.io.load(raw_file)\n",
    "        \n",
    "        print(\"Processing experimental data...\")\n",
    "        processed_data = {}\n",
    "        \n",
    "        for condition, data in raw_data.items():\n",
    "            signal = data['signal']\n",
    "            time = data['time']\n",
    "            \n",
    "            # Apply processing steps\n",
    "            # 1. Smoothing\n",
    "            from scipy import ndimage\n",
    "            smoothed = ndimage.gaussian_filter1d(signal, sigma=2)\n",
    "            \n",
    "            # 2. Feature extraction\n",
    "            features = {\n",
    "                'mean': np.mean(smoothed),\n",
    "                'std': np.std(smoothed),\n",
    "                'max': np.max(smoothed),\n",
    "                'min': np.min(smoothed),\n",
    "                'peak_to_peak': np.ptp(smoothed)\n",
    "            }\n",
    "            \n",
    "            # 3. Spectral analysis\n",
    "            fft = np.fft.fft(smoothed)\n",
    "            freqs = np.fft.fftfreq(len(smoothed), d=time[1]-time[0])\n",
    "            power_spectrum = np.abs(fft)**2\n",
    "            \n",
    "            processed_data[condition] = {\n",
    "                'original_signal': signal,\n",
    "                'smoothed_signal': smoothed,\n",
    "                'features': features,\n",
    "                'power_spectrum': power_spectrum[:len(power_spectrum)//2],\n",
    "                'frequencies': freqs[:len(freqs)//2],\n",
    "                'time': time,\n",
    "                'metadata': data['metadata']\n",
    "            }\n",
    "        \n",
    "        # Save processed data\n",
    "        processed_file = self.output_dir / \"processed\" / \"processed_data.pkl\"\n",
    "        scitex.io.save(processed_data, processed_file)\n",
    "        print(f\"✓ Processed data saved to {processed_file}\")\n",
    "        \n",
    "        return processed_data\n",
    "    \n",
    "    def analyze_results(self, processed_data=None):\n",
    "        \"\"\"Analyze processed data and generate results.\"\"\"\n",
    "        if processed_data is None:\n",
    "            processed_file = self.output_dir / \"processed\" / \"processed_data.pkl\"\n",
    "            processed_data = scitex.io.load(processed_file)\n",
    "        \n",
    "        print(\"Analyzing results...\")\n",
    "        \n",
    "        # Statistical analysis\n",
    "        results = {\n",
    "            'summary_statistics': {},\n",
    "            'comparisons': {},\n",
    "            'figures': {}\n",
    "        }\n",
    "        \n",
    "        # Extract features for all conditions\n",
    "        all_features = {}\n",
    "        for condition, data in processed_data.items():\n",
    "            all_features[condition] = data['features']\n",
    "            results['summary_statistics'][condition] = data['features']\n",
    "        \n",
    "        # Generate comparison plots\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "        fig.suptitle(f'Experiment Results: {self.experiment_name}')\n",
    "        \n",
    "        # Plot 1: Original signals\n",
    "        for condition, data in processed_data.items():\n",
    "            axes[0, 0].plot(data['time'], data['smoothed_signal'], label=condition)\n",
    "        axes[0, 0].set_title('Processed Signals')\n",
    "        axes[0, 0].set_xlabel('Time')\n",
    "        axes[0, 0].set_ylabel('Amplitude')\n",
    "        axes[0, 0].legend()\n",
    "        \n",
    "        # Plot 2: Feature comparison\n",
    "        feature_names = list(all_features['control'].keys())\n",
    "        x_pos = np.arange(len(feature_names))\n",
    "        width = 0.25\n",
    "        \n",
    "        for i, condition in enumerate(all_features.keys()):\n",
    "            values = [all_features[condition][feat] for feat in feature_names]\n",
    "            axes[0, 1].bar(x_pos + i*width, values, width, label=condition)\n",
    "        \n",
    "        axes[0, 1].set_title('Feature Comparison')\n",
    "        axes[0, 1].set_xlabel('Features')\n",
    "        axes[0, 1].set_ylabel('Value')\n",
    "        axes[0, 1].set_xticks(x_pos + width)\n",
    "        axes[0, 1].set_xticklabels(feature_names, rotation=45)\n",
    "        axes[0, 1].legend()\n",
    "        \n",
    "        # Plot 3: Power spectra\n",
    "        for condition, data in processed_data.items():\n",
    "            axes[1, 0].semilogy(data['frequencies'], data['power_spectrum'], label=condition)\n",
    "        axes[1, 0].set_title('Power Spectra')\n",
    "        axes[1, 0].set_xlabel('Frequency (Hz)')\n",
    "        axes[1, 0].set_ylabel('Power')\n",
    "        axes[1, 0].legend()\n",
    "        \n",
    "        # Plot 4: Summary statistics\n",
    "        conditions = list(all_features.keys())\n",
    "        means = [all_features[cond]['mean'] for cond in conditions]\n",
    "        stds = [all_features[cond]['std'] for cond in conditions]\n",
    "        \n",
    "        axes[1, 1].bar(conditions, means, yerr=stds, capsize=5)\n",
    "        axes[1, 1].set_title('Mean ± Std by Condition')\n",
    "        axes[1, 1].set_ylabel('Signal Mean')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save figure\n",
    "        figure_file = self.output_dir / \"results\" / \"analysis_summary.png\"\n",
    "        plt.savefig(figure_file, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        results['figures']['summary_plot'] = str(figure_file)\n",
    "        \n",
    "        # Save results\n",
    "        results_file = self.output_dir / \"results\" / \"analysis_results.pkl\"\n",
    "        scitex.io.save(results, results_file)\n",
    "        print(f\"✓ Analysis results saved to {results_file}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def run_complete_pipeline(self, n_samples=1000, noise_level=0.1):\n",
    "        \"\"\"Run the complete experiment pipeline.\"\"\"\n",
    "        print(f\"\\n=== Running Complete Pipeline: {self.experiment_name} ===\")\n",
    "        \n",
    "        # Step 1: Generate data\n",
    "        raw_data = self.generate_data(n_samples, noise_level)\n",
    "        \n",
    "        # Step 2: Process data\n",
    "        processed_data = self.process_data(raw_data)\n",
    "        \n",
    "        # Step 3: Analyze results\n",
    "        results = self.analyze_results(processed_data)\n",
    "        \n",
    "        print(f\"\\n=== Pipeline Complete ===\")\n",
    "        print(f\"Output directory: {self.output_dir}\")\n",
    "        print(f\"Files created:\")\n",
    "        for file in self.output_dir.rglob(\"*\"):\n",
    "            if file.is_file():\n",
    "                print(f\"  {file.relative_to(self.output_dir)}\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Run the complete pipeline\n",
    "pipeline = ExperimentPipeline(\n",
    "    experiment_name=\"SciTeX_IO_Demo\",\n",
    "    output_dir=data_dir / \"experiment_pipeline\"\n",
    ")\n",
    "\n",
    "final_results = pipeline.run_complete_pipeline(n_samples=500, noise_level=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fe9068",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Part 4: Configuration Management and Advanced Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6e3905",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configuration file management\n",
    "config_dir = data_dir / \"configs\"\n",
    "config_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Create experiment configurations\n",
    "configs = {\n",
    "    'default': {\n",
    "        'data_params': {\n",
    "            'n_samples': 1000,\n",
    "            'noise_level': 0.1,\n",
    "            'sampling_rate': 100\n",
    "        },\n",
    "        'processing_params': {\n",
    "            'smoothing_sigma': 2.0,\n",
    "            'filter_cutoff': 0.5\n",
    "        },\n",
    "        'analysis_params': {\n",
    "            'significance_level': 0.05,\n",
    "            'bootstrap_iterations': 1000\n",
    "        }\n",
    "    },\n",
    "    'high_resolution': {\n",
    "        'data_params': {\n",
    "            'n_samples': 5000,\n",
    "            'noise_level': 0.05,\n",
    "            'sampling_rate': 1000\n",
    "        },\n",
    "        'processing_params': {\n",
    "            'smoothing_sigma': 1.0,\n",
    "            'filter_cutoff': 0.1\n",
    "        },\n",
    "        'analysis_params': {\n",
    "            'significance_level': 0.01,\n",
    "            'bootstrap_iterations': 5000\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save configurations in different formats\n",
    "for config_name, config_data in configs.items():\n",
    "    # Save as JSON (human-readable)\n",
    "    json_file = config_dir / f\"{config_name}_config.json\"\n",
    "    scitex.io.save(config_data, json_file)\n",
    "    \n",
    "    # Save as YAML (if available)\n",
    "    try:\n",
    "        yaml_file = config_dir / f\"{config_name}_config.yaml\"\n",
    "        scitex.io.save(config_data, yaml_file)\n",
    "        print(f\"✓ Saved {config_name} config in JSON and YAML formats\")\n",
    "    except Exception:\n",
    "        print(f\"✓ Saved {config_name} config in JSON format (YAML not available)\")\n",
    "\n",
    "# Load and use configuration\n",
    "loaded_config = scitex.io.load(config_dir / \"high_resolution_config.json\")\n",
    "print(f\"\\nLoaded configuration:\")\n",
    "for section, params in loaded_config.items():\n",
    "    print(f\"  {section}:\")\n",
    "    for key, value in params.items():\n",
    "        print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245fd5c6",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Summary and Best Practices\n",
    "\n",
    "This tutorial demonstrated the comprehensive I/O capabilities of the SciTeX library:\n",
    "\n",
    "### Key Features Covered:\n",
    "1. **Unified Interface**: Automatic format detection for save/load operations\n",
    "2. **Multiple Formats**: Support for pickle, JSON, HDF5, CSV, NumPy, and compressed formats\n",
    "3. **Performance Optimization**: Caching, compression, and format-specific optimizations\n",
    "4. **Batch Operations**: Efficient handling of multiple files\n",
    "5. **Complete Workflows**: Integration with experimental pipelines\n",
    "6. **Configuration Management**: Flexible configuration file handling\n",
    "\n",
    "### Best Practices:\n",
    "- Use **pickle** for complex Python objects and mixed data types\n",
    "- Use **HDF5** for large, hierarchical datasets\n",
    "- Use **JSON/YAML** for human-readable configuration files\n",
    "- Apply **compression** for large files when storage space is limited\n",
    "- Implement **caching** for expensive computations\n",
    "- Organize data with **clear directory structures**\n",
    "- Use **symlinks** for easy access to frequently used files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dee6fa",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cleanup - remove example files (optional)\n",
    "import shutil\n",
    "\n",
    "cleanup = input(\"Clean up example files? (y/n): \").lower().startswith('y')\n",
    "if cleanup:\n",
    "    shutil.rmtree(data_dir)\n",
    "    print(\"✓ Example files cleaned up\")\n",
    "else:\n",
    "    print(f\"Example files preserved in: {data_dir}\")\n",
    "    print(f\"Total size: {sum(f.stat().st_size for f in data_dir.rglob('*') if f.is_file()) / (1024*1024):.1f} MB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 11.219111,
   "end_time": "2025-07-04T08:31:29.033258",
   "environment_variables": {},
   "exception": true,
   "input_path": "examples/01_scitex_io.ipynb",
   "output_path": "./test_output.ipynb",
   "parameters": {},
   "start_time": "2025-07-04T08:31:17.814147",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}